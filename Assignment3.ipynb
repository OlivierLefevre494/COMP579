{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Value-based methods with deep neural network [50 points]\n",
    "Implement Q-learning and Exptected SARSA for both Acrobot-v1 1 and ALE/Assault-ram-v52\n",
    "environments from the Gym suite using the following guidelines:\n",
    "• Use a Neural Network approximation for Q, that is, if x is a vector representing the state\n",
    "and a is the action vector, use Q value(x) = M LP (x; θ), where θ are the parameters of the\n",
    "Value function you need to learn, Q ∈ Rm where m denotes the number of discrete actions.\n",
    "• Model configuration: Initialize the parameters for the value function uniformly between\n",
    "−0.001 and 0.001, we recommend using either a 2 or 3-layer Neural Network for the Value\n",
    "function, with a hidden dimension of 256.\n",
    "• Use an ϵ- greedy policy with three choices of ϵ and step-size parameters 1/4, 1/8, 1/16. and\n",
    "run 50 learning trials with different initializations for the Value function, each having 1000\n",
    "episodes, for each configuration. That means 3(ϵ’s) * 3 (step-sizes) * 50 runs * 1000 episodes.\n",
    "• Repeat the Previous step using a replay buffer (with transitions randomly sampled) and do\n",
    "gradient updates using a mini-batch of transitions. The capacity of the replay buffer is 1M.\n",
    "• Plot training curves with the mean across seeds as lines and the standard deviation as a shaded\n",
    "region. (Performance on the Y-axis, and the episode on the X-axis). Generate 18 graphs\n",
    "covering all configurations per environment. Present separate plots for each environment,\n",
    "with distinct graphs for settings with and without a replay buffer. Use green for Q-Learning\n",
    "and red for Expected SARSA, differentiating hyperparameters with different line styles (e.g.,\n",
    "solid, dashed).\n",
    "• Implement all the methods using any automatic differentiation package, such as Py-\n",
    "torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Q-learning and Exptected SARSA for both Acrobot-v1 1 and ALE/Assault-ram-v52 environments from the Gym suite using the following guidelines:\n",
    "\n",
    "# • Use a Neural Network approximation for Q, that is, if x is a vector representing the state and a is the action vector, use Q value(x) = M LP (x; θ), where θ are the parameters of the Value function you need to learn, Q ∈ Rm where m denotes the number of discrete actions.\n",
    "\n",
    "# • Model configuration: Initialize the parameters for the value function uniformly between −0.001 and 0.001, we recommend using either a 2 or 3-layer Neural Network for the Value function, with a hidden dimension of 256.\n",
    "\n",
    "# • Use an ϵ- greedy policy with three choices of ϵ and step-size parameters 1/4, 1/8, 1/16. and run 50 learning trials with different initializations for the Value function, each having 1000 episodes, for each configuration. That means 3(ϵ’s) * 3 (step-sizes) * 50 runs * 1000 episodes.\n",
    "\n",
    "# • Repeat the Previous step using a replay buffer (with transitions randomly sampled) and do gradient updates using a mini-batch of transitions. The capacity of the replay buffer is 1M.\n",
    "\n",
    "# • Plot training curves with the mean across seeds as lines and the standard deviation as a shaded region. (Performance on the Y-axis, and the episode on the X-axis). Generate 18 graphs covering all configurations per environment. Present separate plots for each environment, with distinct graphs for settings with and without a replay buffer. Use green for Q-Learning and red for Expected SARSA, differentiating hyperparameters with different line styles (e.g., solid, dashed).\n",
    "\n",
    "# • Implement all the methods using any automatic differentiation package, such as Py-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# env_name = 'Acrobot-v1'\n",
    "env_name = 'ALE/Assault-ram-v5'\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import math as math\n",
    "\n",
    "# Hyperparameters\n",
    "EPISODES = 1000\n",
    "MAX_STEPS = 1000\n",
    "GAMMA = 0.99\n",
    "HIDDEN_DIM = 256\n",
    "SEEDS = 50\n",
    "\n",
    "# Environment\n",
    "env = gym.make(env_name)\n",
    "env._max_episode_steps = MAX_STEPS\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Neural Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, INPUT_DIM, OUTPUT_DIM):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc3 = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "        # torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        # torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        # torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        # self.fc1.bias.data.fill_(0.01)\n",
    "        # self.fc2.bias.data.fill_(0.01)\n",
    "        # self.fc3.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Q-Learning\n",
    "def q_learning(lr, batch_size, replay_buffer_size, epsilon):\n",
    "    q_network = QNetwork(state_dim, action_dim)\n",
    "    # q_network_target = QNetwork()\n",
    "    # q_network_target.load_state_dict(q_network.state_dict())\n",
    "    q_network.to(device)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "    rewards = []\n",
    "    for episode in range(EPISODES):\n",
    "        print(episode)\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    action = q_network(state_tensor).argmax().item()\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "                state_batch = torch.tensor(np.array(state_batch), dtype=torch.float32).to(device)\n",
    "                action_batch = torch.tensor(np.array(action_batch), dtype=torch.long).to(device)\n",
    "                reward_batch = torch.tensor(np.array(reward_batch), dtype=torch.float32).to(device)\n",
    "                next_state_batch = torch.tensor(np.array(next_state_batch), dtype=torch.float32).to(device)\n",
    "                done_batch = torch.tensor(np.array(done_batch), dtype=torch.float32).to(device)\n",
    "                q_values = q_network(state_batch)\n",
    "                # next_q_values = q_network_target(next_state_batch)\n",
    "                next_q_values = q_network(next_state_batch)\n",
    "                target_q_values = q_values.clone()\n",
    "                # for i in range(batch_size):\n",
    "                #     target_q_values[i][action_batch[i]] = reward_batch[i] + GAMMA * next_q_values[i].max() * (1 - done_batch[i])\n",
    "                target_q_values[range(batch_size), action_batch] = reward_batch + GAMMA * next_q_values.max(dim=1).values * (1 - done_batch)\n",
    "                loss = nn.MSELoss()(q_values, target_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            state = next_state\n",
    "        rewards.append(total_reward)\n",
    "        # if episode % 100 == 0:\n",
    "        #     q_network_target.load_state_dict(q_network.state_dict())\n",
    "    return rewards\n",
    "\n",
    "# Expected SARSA\n",
    "def expected_sarsa(lr, batch_size, replay_buffer_size, epsilon):\n",
    "    q_network = QNetwork()\n",
    "    # q_network_target = QNetwork()\n",
    "    # q_network_target.load_state_dict(q_network.state_dict())\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "    rewards = []\n",
    "    for episode in range(EPISODES):\n",
    "        print(episode)\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                    action = q_network(state_tensor).argmax().item()\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "                state_batch = torch.tensor(np.array(state_batch), dtype=torch.float32).to(device)\n",
    "                action_batch = torch.tensor(np.array(action_batch), dtype=torch.long).to(device)\n",
    "                reward_batch = torch.tensor(np.array(reward_batch), dtype=torch.float32).to(device)\n",
    "                next_state_batch = torch.tensor(np.array(next_state_batch), dtype=torch.float32).to(device)\n",
    "                done_batch = torch.tensor(np.array(done_batch), dtype=torch.float32).to(device)\n",
    "                q_values = q_network(state_batch)\n",
    "                # next_q_values = q_network_target(next_state_batch)\n",
    "                next_q_values = q_network(next_state_batch)\n",
    "                target_q_values = q_values.clone()\n",
    "                # for i in range(batch_size):\n",
    "                #     target_q_values[i][action_batch[i]] = reward_batch[i] + GAMMA * next_q_values[i].mean() * (1 - done_batch[i])\n",
    "                target_q_values[range(batch_size), action_batch] = reward_batch + GAMMA * next_q_values.mean(dim=1) * (1 - done_batch)\n",
    "                loss = nn.MSELoss()(q_values, target_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            state = next_state\n",
    "        rewards.append(total_reward)\n",
    "        # if episode % 100 == 0:\n",
    "        #     q_network_target.load_state_dict(q_network.state_dict())\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def run_experiment(lr, batch_size, replay_buffer_size, epsilon):\n",
    "    random_seeds = np.random.randint(0, 1000, size=SEEDS)\n",
    "    q_learning_rewards = np.zeros((SEEDS, EPISODES))\n",
    "    expected_sarsa_rewards = np.zeros((SEEDS, EPISODES))\n",
    "    for i, seed in enumerate(random_seeds):\n",
    "        print(f'Run {i+1}/{SEEDS}')\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        q_learning_rewards[i] = q_learning(lr, batch_size, replay_buffer_size, epsilon)\n",
    "        expected_sarsa_rewards[i] = expected_sarsa(lr, batch_size, replay_buffer_size, epsilon)\n",
    "    q_learning_rewards_mean = q_learning_rewards.mean(axis=0)\n",
    "    q_learning_rewards_std = q_learning_rewards.std(axis=0)\n",
    "    expected_sarsa_rewards_mean = expected_sarsa_rewards.mean(axis=0)\n",
    "    expected_sarsa_rewards_std = expected_sarsa_rewards.std(axis=0)\n",
    "    plt.plot(q_learning_rewards_mean, label='Q-Learning', color='green')\n",
    "    plt.fill_between(range(EPISODES), q_learning_rewards_mean - q_learning_rewards_std, q_learning_rewards_mean + q_learning_rewards_std, color='green', alpha=0.2)\n",
    "    plt.plot(expected_sarsa_rewards_mean, label='Expected SARSA', color='red')\n",
    "    plt.fill_between(range(EPISODES), expected_sarsa_rewards_mean - expected_sarsa_rewards_std, expected_sarsa_rewards_mean + expected_sarsa_rewards_std, color='red', alpha=0.2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.savefig('plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEEDS = 1\n",
    "# EPISODES = 100\n",
    "# run_experiment(0.001, 64, 1000000, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/50\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m lrs:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m replay_buffer \u001b[38;5;129;01min\u001b[39;00m replay_buffers:\n\u001b[0;32m----> 7\u001b[0m         \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m         os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmv plot.png plots/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env_name, epsilon, lr, replay_buffer[\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[1], line 160\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(lr, batch_size, replay_buffer_size, epsilon)\u001b[0m\n\u001b[1;32m    158\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m    159\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m--> 160\u001b[0m     q_learning_rewards[i] \u001b[38;5;241m=\u001b[39m \u001b[43mq_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     expected_sarsa_rewards[i] \u001b[38;5;241m=\u001b[39m expected_sarsa(lr, batch_size, replay_buffer_size, epsilon)\n\u001b[1;32m    162\u001b[0m q_learning_rewards_mean \u001b[38;5;241m=\u001b[39m q_learning_rewards\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m, in \u001b[0;36mq_learning\u001b[0;34m(lr, batch_size, replay_buffer_size, epsilon)\u001b[0m\n\u001b[1;32m     92\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(q_values, target_q_values)\n\u001b[1;32m     93\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     96\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilons = [0.25, 0.125, 0.0625]\n",
    "lrs = [1/4, 1/8, 1/16]\n",
    "replay_buffers = [(1, 1), (32, 1000000)]\n",
    "for epsilon in epsilons:\n",
    "    for lr in lrs:\n",
    "        for replay_buffer in replay_buffers:\n",
    "            run_experiment(lr, replay_buffer[0], replay_buffer[1], epsilon)\n",
    "            time.sleep(1)\n",
    "            os.system('mv plot.png plots/{}_{}_{}_{}.png'.format(env_name, epsilon, lr, replay_buffer[0]))\n",
    "            time.sleep(1)\n",
    "            os.system('rm plot.png')\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoltzmanPolicy(preds, temp):\n",
    "    probs = torch.softmax(preds / temp, dim=1)\n",
    "    a = torch.distributions.Categorical(probs).sample().item()\n",
    "    return a, probs[0][a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001 32 1000000 0.25\n",
      "1\n",
      "Run 1/1\n",
      "Q-Learning\n",
      "initializing layer Linear(in_features=128, out_features=256, bias=True)\n",
      "initializing layer Linear(in_features=256, out_features=256, bias=True)\n",
      "initializing layer Linear(in_features=256, out_features=7, bias=True)\n",
      "initializing layer Linear(in_features=128, out_features=256, bias=True)\n",
      "initializing layer Linear(in_features=256, out_features=256, bias=True)\n",
      "initializing layer Linear(in_features=256, out_features=7, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 126.0 0.003928137011826038\n",
      "10 210.0 1.8146321773529053\n",
      "20 399.0 1.664935827255249\n",
      "30 399.0 1.6783517599105835\n",
      "40 252.0 0.11752704530954361\n",
      "50 420.0 2.9209423065185547\n",
      "60 357.0 0.08160822093486786\n",
      "70 252.0 0.0679195374250412\n",
      "80 315.0 1.0093916654586792\n",
      "90 315.0 0.06897712498903275\n",
      "100 378.0 0.13388080894947052\n",
      "110 294.0 0.16688521206378937\n",
      "120 378.0 0.21346697211265564\n"
     ]
    }
   ],
   "source": [
    "epsilons = [0.25, 0.1, 0.01]\n",
    "# lrs = [1/4, 1/8, 1/16]\n",
    "lrs = [0.0001, 0.001, 0.01] # 0.01\n",
    "replay_buffers = [(32, 1000000), (1, 1)]\n",
    "for i in range(1, 11):\n",
    "    for replay_buffer in replay_buffers:\n",
    "        for lr in lrs:\n",
    "            for epsilon in epsilons:\n",
    "                print(lr, replay_buffer[0], replay_buffer[1], epsilon)\n",
    "                run_experiment(lr, replay_buffer[0], replay_buffer[1], epsilon, i)\n"
   ]
=======
   "outputs": [],
   "source": []
>>>>>>> 72294918dafe066ca00ce360b230a0ca917f2f78
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(episodes, lr, gamma, T, decay, decay_rate):\n",
    "    rewards = []\n",
    "    q_network = QNetwork(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    temp = T\n",
    "    for i in range(episodes):\n",
    "        if decay:\n",
    "            temp = T * math.exp(-decay_rate * i)\n",
    "        total_reward = 0\n",
    "        print(\"EPSIODE#\", i)\n",
    "        ep = []\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done and step < 500:\n",
    "            action, prob = BoltzmanPolicy(q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)), temp)\n",
    "            next_state, reward, done, truncated, _= env.step(action)\n",
    "            total_reward += reward\n",
    "            ep.append((state, action, reward, prob))\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        grad = 0\n",
    "        for t in range(len(ep)):\n",
    "            G = sum([gamma**(i-t-1) * ep[i][2] for i in range(t, len(ep))])\n",
    "            # print(G)\n",
    "            grad += (gamma**t) * G * torch.log(max(prob,torch.tensor(1e-8)))\n",
    "        optimizer.zero_grad()\n",
    "        grad.backward()\n",
    "        optimizer.step()\n",
    "        rewards.append(total_reward)\n",
    "        print(\"MEAN POLICY LOSS\", grad)\n",
    "        print(\"REWARD\", total_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2C(episodes, gamma, policy_lr,value_lr, T, decay, decay_rate):\n",
    "    policy_network = QNetwork(state_dim, action_dim)\n",
    "    value_network = QNetwork(state_dim, 1)\n",
    "    optimizer_actor = optim.Adam(policy_network.parameters(), lr=0.0005)\n",
    "    optimizer_value = optim.Adam(value_network.parameters(), lr=0.004)\n",
    "    rewards = []\n",
    "    temp = T\n",
    "    end_temp = 0.1\n",
    "    for i in range(episodes):\n",
    "        if decay:\n",
    "            # linear scheduler\n",
    "            torch.optim.lr_scheduler.LinearLR(optimizer_actor, decay_rate)\n",
    "\n",
    "        total_reward = 0\n",
    "        print(\"EPSIODE#\", i)\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "        while not done  and step < 1000:\n",
    "            with torch.no_grad():\n",
    "                action, prob = BoltzmanPolicy(policy_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))[0], temp)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                advantage = reward - value_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)).detach()\n",
    "            else:\n",
    "                advantage = reward + gamma * value_network(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)).detach() - value_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)).detach()\n",
    "            preds = policy_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))[0]\n",
    "            policy_loss -= torch.log(torch.exp(preds[action]/T)/torch.sum(torch.exp(preds/T))) * advantage\n",
    "            value_loss +=  0.5*(reward + gamma * value_network(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)).detach() - value_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)))**2\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        mean_policy_loss = policy_loss / step\n",
    "        mean_value_loss = value_loss / step\n",
    "        optimizer_actor.zero_grad()\n",
    "        optimizer_value.zero_grad()\n",
    "        mean_policy_loss.backward()\n",
    "        mean_value_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "        optimizer_value.step()\n",
    "        rewards.append(total_reward)\n",
    "        print(\"MEAN POLICY LOSS\", mean_policy_loss)\n",
    "        print(\"MEAN VALUE LOSS\", mean_value_loss)\n",
    "        print(\"REWARD\", total_reward)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment2(lr, T, decay, decay_rate, gamma, episodes):\n",
    "    random_seeds = np.random.randint(0, 50, size=SEEDS)\n",
    "    a2c_learning_rewards = np.zeros((SEEDS, EPISODES))\n",
    "    reinforce_rewards = np.zeros((SEEDS, EPISODES))\n",
    "    for i, seed in enumerate(random_seeds):\n",
    "        print(f'Run {i+1}/{SEEDS}')\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        # a2c_learning_rewards[i] = A2C(episodes=episodes, gamma=gamma, policy_lr=lr, value_lr=lr, T=T, decay=decay, decay_rate=decay_rate)\n",
    "        reinforce_rewards[i] = reinforce(episodes=episodes, lr=lr, gamma=gamma, T=T, decay=decay, decay_rate=decay_rate)\n",
    "    # a2c_learning_rewards_mean = a2c_learning_rewards.mean(axis=0)\n",
    "    # a2c_learning_rewards_std = a2c_learning_rewards.std(axis=0)\n",
    "    reinforce_rewards_mean = reinforce_rewards.mean(axis=0)\n",
    "    reinforce_rewards_std = reinforce_rewards.std(axis=0)/math.sqrt(SEEDS)\n",
    "    # plt.plot(a2c_learning_rewards_mean, label='Q-Learning', color='green')\n",
    "    # plt.fill_between(range(EPISODES), a2c_learning_rewards_mean - a2c_learning_rewards_std, a2c_learning_rewards_mean + a2c_learning_rewards_std, color='green', alpha=0.2)\n",
    "    plt.plot(reinforce_rewards_mean, label='Reinforce', color='red')\n",
    "    plt.fill_between(range(EPISODES), reinforce_rewards_mean - reinforce_rewards_std, reinforce_rewards_mean + reinforce_rewards_std, color='red', alpha=0.2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.savefig('plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "EPSIODE# 0\n",
      "MEAN POLICY LOSS tensor(10240.8916, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 1\n",
      "MEAN POLICY LOSS tensor(11701.2432, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 2\n",
      "MEAN POLICY LOSS tensor(12206.8984, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 3\n",
      "MEAN POLICY LOSS tensor(12330.0195, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 4\n",
      "MEAN POLICY LOSS tensor(6612.4019, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 5\n",
      "MEAN POLICY LOSS tensor(9637.4785, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 6\n",
      "MEAN POLICY LOSS tensor(8714.1357, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 7\n",
      "MEAN POLICY LOSS tensor(15208.6562, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 8\n",
      "MEAN POLICY LOSS tensor(12019.6689, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 9\n",
      "MEAN POLICY LOSS tensor(4662.3589, grad_fn=<AddBackward0>)\n",
      "REWARD -335.0\n",
      "EPSIODE# 10\n",
      "MEAN POLICY LOSS tensor(3940.3855, grad_fn=<AddBackward0>)\n",
      "REWARD -394.0\n",
      "EPSIODE# 11\n",
      "MEAN POLICY LOSS tensor(7764.4946, grad_fn=<AddBackward0>)\n",
      "REWARD -328.0\n",
      "EPSIODE# 12\n",
      "MEAN POLICY LOSS tensor(15913.2432, grad_fn=<AddBackward0>)\n",
      "REWARD -471.0\n",
      "EPSIODE# 13\n",
      "MEAN POLICY LOSS tensor(2736.1235, grad_fn=<AddBackward0>)\n",
      "REWARD -406.0\n",
      "EPSIODE# 14\n",
      "MEAN POLICY LOSS tensor(9802.7627, grad_fn=<AddBackward0>)\n",
      "REWARD -417.0\n",
      "EPSIODE# 15\n",
      "MEAN POLICY LOSS tensor(15259.6025, grad_fn=<AddBackward0>)\n",
      "REWARD -358.0\n",
      "EPSIODE# 16\n",
      "MEAN POLICY LOSS tensor(8510.9180, grad_fn=<AddBackward0>)\n",
      "REWARD -348.0\n",
      "EPSIODE# 17\n",
      "MEAN POLICY LOSS tensor(6306.7441, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 18\n",
      "MEAN POLICY LOSS tensor(33.3814, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 19\n",
      "MEAN POLICY LOSS tensor(13220.5859, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 20\n",
      "MEAN POLICY LOSS tensor(7388.2373, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 21\n",
      "MEAN POLICY LOSS tensor(1450.0962, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 22\n",
      "MEAN POLICY LOSS tensor(418.5184, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 23\n",
      "MEAN POLICY LOSS tensor(2232.4495, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 24\n",
      "MEAN POLICY LOSS tensor(576.3314, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 25\n",
      "MEAN POLICY LOSS tensor(910.3336, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 26\n",
      "MEAN POLICY LOSS tensor(5210.9058, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 27\n",
      "MEAN POLICY LOSS tensor(313.2014, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 28\n",
      "MEAN POLICY LOSS tensor(3682.2356, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 29\n",
      "MEAN POLICY LOSS tensor(3280.2551, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 30\n",
      "MEAN POLICY LOSS tensor(23033.3691, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 31\n",
      "MEAN POLICY LOSS tensor(64.3593, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 32\n",
      "MEAN POLICY LOSS tensor(1196.6042, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 33\n",
      "MEAN POLICY LOSS tensor(2548.6233, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 34\n",
      "MEAN POLICY LOSS tensor(15468.1250, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 35\n",
      "MEAN POLICY LOSS tensor(145.4844, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 36\n",
      "MEAN POLICY LOSS tensor(2020.5880, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 37\n",
      "MEAN POLICY LOSS tensor(88.8434, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 38\n",
      "MEAN POLICY LOSS tensor(2946.2620, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 39\n",
      "MEAN POLICY LOSS tensor(1692.6947, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 40\n",
      "MEAN POLICY LOSS tensor(2248.6943, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 41\n",
      "MEAN POLICY LOSS tensor(100.7549, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 42\n",
      "MEAN POLICY LOSS tensor(25736.0371, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 43\n",
      "MEAN POLICY LOSS tensor(24541.8320, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 44\n",
      "MEAN POLICY LOSS tensor(25016.0781, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 45\n",
      "MEAN POLICY LOSS tensor(289.0739, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 46\n",
      "MEAN POLICY LOSS tensor(963.1816, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 47\n",
      "MEAN POLICY LOSS tensor(1905.0615, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 48\n",
      "MEAN POLICY LOSS tensor(878.4469, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 49\n",
      "MEAN POLICY LOSS tensor(1475.8175, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 50\n",
      "MEAN POLICY LOSS tensor(3401.6765, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 51\n",
      "MEAN POLICY LOSS tensor(2437.7085, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 52\n",
      "MEAN POLICY LOSS tensor(17855.6211, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 53\n",
      "MEAN POLICY LOSS tensor(282.2379, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 54\n",
      "MEAN POLICY LOSS tensor(74.4998, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 55\n",
      "MEAN POLICY LOSS tensor(802.1722, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 56\n",
      "MEAN POLICY LOSS tensor(16123.4326, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 57\n",
      "MEAN POLICY LOSS tensor(326.1680, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 58\n",
      "MEAN POLICY LOSS tensor(4600.0938, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 59\n",
      "MEAN POLICY LOSS tensor(34485.3320, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 60\n",
      "MEAN POLICY LOSS tensor(329.1868, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 61\n",
      "MEAN POLICY LOSS tensor(4136.4116, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 62\n",
      "MEAN POLICY LOSS tensor(501.7317, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 63\n",
      "MEAN POLICY LOSS tensor(17172.9629, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 64\n",
      "MEAN POLICY LOSS tensor(15024.3711, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 65\n",
      "MEAN POLICY LOSS tensor(387.3408, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 66\n",
      "MEAN POLICY LOSS tensor(4856.4243, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 67\n",
      "MEAN POLICY LOSS tensor(14.7176, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 68\n",
      "MEAN POLICY LOSS tensor(5071.3682, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 69\n",
      "MEAN POLICY LOSS tensor(86.9217, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 70\n",
      "MEAN POLICY LOSS tensor(3099.4600, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 71\n",
      "MEAN POLICY LOSS tensor(969.9377, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 72\n",
      "MEAN POLICY LOSS tensor(9.2317, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 73\n",
      "MEAN POLICY LOSS tensor(363.8812, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 74\n",
      "MEAN POLICY LOSS tensor(14838.3643, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 75\n",
      "MEAN POLICY LOSS tensor(5734.9253, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 76\n",
      "MEAN POLICY LOSS tensor(5683.5000, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 77\n",
      "MEAN POLICY LOSS tensor(15490.3379, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 78\n",
      "MEAN POLICY LOSS tensor(320.6601, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 79\n",
      "MEAN POLICY LOSS tensor(5142.8232, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 80\n",
      "MEAN POLICY LOSS tensor(12815.0918, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 81\n",
      "MEAN POLICY LOSS tensor(4038.0740, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 82\n",
      "MEAN POLICY LOSS tensor(1623.9039, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 83\n",
      "MEAN POLICY LOSS tensor(15032.0791, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 84\n",
      "MEAN POLICY LOSS tensor(818.2808, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 85\n",
      "MEAN POLICY LOSS tensor(14872.2402, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 86\n",
      "MEAN POLICY LOSS tensor(5167.5083, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 87\n",
      "MEAN POLICY LOSS tensor(6483.6694, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 88\n",
      "MEAN POLICY LOSS tensor(15771.1455, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 89\n",
      "MEAN POLICY LOSS tensor(15212.2207, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 90\n",
      "MEAN POLICY LOSS tensor(250.3863, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 91\n",
      "MEAN POLICY LOSS tensor(2428.8845, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 92\n",
      "MEAN POLICY LOSS tensor(12649.3174, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 93\n",
      "MEAN POLICY LOSS tensor(219.9151, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 94\n",
      "MEAN POLICY LOSS tensor(6942.2900, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 95\n",
      "MEAN POLICY LOSS tensor(2967.9192, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 96\n",
      "MEAN POLICY LOSS tensor(2435.8164, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 97\n",
      "MEAN POLICY LOSS tensor(1495.3569, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 98\n",
      "MEAN POLICY LOSS tensor(182.8693, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 99\n",
      "MEAN POLICY LOSS tensor(2467.3037, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 100\n",
      "MEAN POLICY LOSS tensor(1192.9209, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 101\n",
      "MEAN POLICY LOSS tensor(1506.3314, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 102\n",
      "MEAN POLICY LOSS tensor(45.7054, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 103\n",
      "MEAN POLICY LOSS tensor(1113.8878, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 104\n",
      "MEAN POLICY LOSS tensor(2094.6130, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 105\n",
      "MEAN POLICY LOSS tensor(454.8633, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 106\n",
      "MEAN POLICY LOSS tensor(2215.0186, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 107\n",
      "MEAN POLICY LOSS tensor(116.4761, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 108\n",
      "MEAN POLICY LOSS tensor(6023.7266, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 109\n",
      "MEAN POLICY LOSS tensor(987.7154, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 110\n",
      "MEAN POLICY LOSS tensor(972.9564, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 111\n",
      "MEAN POLICY LOSS tensor(44.0624, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 112\n",
      "MEAN POLICY LOSS tensor(178.8843, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 113\n",
      "MEAN POLICY LOSS tensor(59.3125, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 114\n",
      "MEAN POLICY LOSS tensor(612.9009, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 115\n",
      "MEAN POLICY LOSS tensor(341.4569, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 116\n",
      "MEAN POLICY LOSS tensor(260.4120, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 117\n",
      "MEAN POLICY LOSS tensor(151.6242, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 118\n",
      "MEAN POLICY LOSS tensor(275.6833, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 119\n",
      "MEAN POLICY LOSS tensor(209.7094, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 120\n",
      "MEAN POLICY LOSS tensor(529.1055, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 121\n",
      "MEAN POLICY LOSS tensor(802.0110, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 122\n",
      "MEAN POLICY LOSS tensor(261.1660, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 123\n",
      "MEAN POLICY LOSS tensor(269.2862, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 124\n",
      "MEAN POLICY LOSS tensor(1120.9949, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 125\n",
      "MEAN POLICY LOSS tensor(85.1608, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 126\n",
      "MEAN POLICY LOSS tensor(876.7443, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 127\n",
      "MEAN POLICY LOSS tensor(4.5630, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 128\n",
      "MEAN POLICY LOSS tensor(174.1026, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 129\n",
      "MEAN POLICY LOSS tensor(513.0776, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 130\n",
      "MEAN POLICY LOSS tensor(202.2935, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 131\n",
      "MEAN POLICY LOSS tensor(1516.7577, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 132\n",
      "MEAN POLICY LOSS tensor(632.4158, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 133\n",
      "MEAN POLICY LOSS tensor(155.2375, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 134\n",
      "MEAN POLICY LOSS tensor(771.0518, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 135\n",
      "MEAN POLICY LOSS tensor(1261.0222, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 136\n",
      "MEAN POLICY LOSS tensor(163.6323, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 137\n",
      "MEAN POLICY LOSS tensor(120.6468, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 138\n",
      "MEAN POLICY LOSS tensor(36.0176, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 139\n",
      "MEAN POLICY LOSS tensor(0.0578, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 140\n",
      "MEAN POLICY LOSS tensor(5.2799, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 141\n",
      "MEAN POLICY LOSS tensor(124.5463, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 142\n",
      "MEAN POLICY LOSS tensor(26.4629, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 143\n",
      "MEAN POLICY LOSS tensor(9.8268, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 144\n",
      "MEAN POLICY LOSS tensor(15.2684, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 145\n",
      "MEAN POLICY LOSS tensor(17.8046, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 146\n",
      "MEAN POLICY LOSS tensor(7.5351, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 147\n",
      "MEAN POLICY LOSS tensor(20.5693, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 148\n",
      "MEAN POLICY LOSS tensor(26.3475, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 149\n",
      "MEAN POLICY LOSS tensor(19.8629, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 150\n",
      "MEAN POLICY LOSS tensor(27.8728, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 151\n",
      "MEAN POLICY LOSS tensor(6.6982, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 152\n",
      "MEAN POLICY LOSS tensor(33.2665, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 153\n",
      "MEAN POLICY LOSS tensor(20.1399, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 154\n",
      "MEAN POLICY LOSS tensor(16.7641, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 155\n",
      "MEAN POLICY LOSS tensor(35.1841, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 156\n",
      "MEAN POLICY LOSS tensor(32.3566, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 157\n",
      "MEAN POLICY LOSS tensor(40.2375, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 158\n",
      "MEAN POLICY LOSS tensor(21.3019, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 159\n",
      "MEAN POLICY LOSS tensor(6.1699, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 160\n",
      "MEAN POLICY LOSS tensor(5.5264, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 161\n",
      "MEAN POLICY LOSS tensor(37.3424, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 162\n",
      "MEAN POLICY LOSS tensor(28.4331, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 163\n",
      "MEAN POLICY LOSS tensor(10.8515, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 164\n",
      "MEAN POLICY LOSS tensor(15.3495, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 165\n",
      "MEAN POLICY LOSS tensor(19.4555, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 166\n",
      "MEAN POLICY LOSS tensor(8.2829, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 167\n",
      "MEAN POLICY LOSS tensor(9.8349, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 168\n",
      "MEAN POLICY LOSS tensor(8.4560, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 169\n",
      "MEAN POLICY LOSS tensor(17.3365, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 170\n",
      "MEAN POLICY LOSS tensor(52.0475, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 171\n",
      "MEAN POLICY LOSS tensor(13.5835, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 172\n",
      "MEAN POLICY LOSS tensor(5.6265, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 173\n",
      "MEAN POLICY LOSS tensor(11.3112, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 174\n",
      "MEAN POLICY LOSS tensor(19.6838, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 175\n",
      "MEAN POLICY LOSS tensor(29.5068, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 176\n",
      "MEAN POLICY LOSS tensor(14.8728, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 177\n",
      "MEAN POLICY LOSS tensor(13.9287, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 178\n",
      "MEAN POLICY LOSS tensor(13.3386, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 179\n",
      "MEAN POLICY LOSS tensor(21.2446, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 180\n",
      "MEAN POLICY LOSS tensor(8.2667, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 181\n",
      "MEAN POLICY LOSS tensor(27.3705, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 182\n",
      "MEAN POLICY LOSS tensor(36.4843, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 183\n",
      "MEAN POLICY LOSS tensor(10.7311, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 184\n",
      "MEAN POLICY LOSS tensor(5.1676, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 185\n",
      "MEAN POLICY LOSS tensor(12.2053, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 186\n",
      "MEAN POLICY LOSS tensor(9.8789, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 187\n",
      "MEAN POLICY LOSS tensor(4.2847, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 188\n",
      "MEAN POLICY LOSS tensor(7.4124, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 189\n",
      "MEAN POLICY LOSS tensor(5.0108, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 190\n",
      "MEAN POLICY LOSS tensor(8.9903, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 191\n",
      "MEAN POLICY LOSS tensor(11.4803, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 192\n",
      "MEAN POLICY LOSS tensor(22.1366, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 193\n",
      "MEAN POLICY LOSS tensor(25.0695, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 194\n",
      "MEAN POLICY LOSS tensor(14.6602, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 195\n",
      "MEAN POLICY LOSS tensor(13.6484, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 196\n",
      "MEAN POLICY LOSS tensor(9.5460, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 197\n",
      "MEAN POLICY LOSS tensor(14.5687, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 198\n",
      "MEAN POLICY LOSS tensor(8.2494, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 199\n",
      "MEAN POLICY LOSS tensor(3.4139, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 200\n",
      "MEAN POLICY LOSS tensor(8.4109, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 201\n",
      "MEAN POLICY LOSS tensor(8.8004, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 202\n",
      "MEAN POLICY LOSS tensor(10.5800, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 203\n",
      "MEAN POLICY LOSS tensor(16.1750, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 204\n",
      "MEAN POLICY LOSS tensor(12.3234, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 205\n",
      "MEAN POLICY LOSS tensor(3.9745, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 206\n",
      "MEAN POLICY LOSS tensor(4.8060, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 207\n",
      "MEAN POLICY LOSS tensor(6.6033, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 208\n",
      "MEAN POLICY LOSS tensor(10.9175, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 209\n",
      "MEAN POLICY LOSS tensor(17.1465, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 210\n",
      "MEAN POLICY LOSS tensor(6.6184, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 211\n",
      "MEAN POLICY LOSS tensor(6.4083, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 212\n",
      "MEAN POLICY LOSS tensor(5.4130, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 213\n",
      "MEAN POLICY LOSS tensor(16.3934, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 214\n",
      "MEAN POLICY LOSS tensor(5.2787, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 215\n",
      "MEAN POLICY LOSS tensor(14.1001, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 216\n",
      "MEAN POLICY LOSS tensor(3.9861, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 217\n",
      "MEAN POLICY LOSS tensor(5.3065, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 218\n",
      "MEAN POLICY LOSS tensor(19.3384, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 219\n",
      "MEAN POLICY LOSS tensor(2.1534, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 220\n",
      "MEAN POLICY LOSS tensor(4.1898, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 221\n",
      "MEAN POLICY LOSS tensor(4.6208, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 222\n",
      "MEAN POLICY LOSS tensor(6.7364, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 223\n",
      "MEAN POLICY LOSS tensor(10.5568, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 224\n",
      "MEAN POLICY LOSS tensor(6.2370, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 225\n",
      "MEAN POLICY LOSS tensor(11.4120, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 226\n",
      "MEAN POLICY LOSS tensor(12.1387, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 227\n",
      "MEAN POLICY LOSS tensor(10.2911, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 228\n",
      "MEAN POLICY LOSS tensor(12.6952, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 229\n",
      "MEAN POLICY LOSS tensor(7.5154, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 230\n",
      "MEAN POLICY LOSS tensor(4.8719, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 231\n",
      "MEAN POLICY LOSS tensor(9.7608, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 232\n",
      "MEAN POLICY LOSS tensor(14.1036, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 233\n",
      "MEAN POLICY LOSS tensor(14.4048, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 234\n",
      "MEAN POLICY LOSS tensor(4.9610, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 235\n",
      "MEAN POLICY LOSS tensor(7.1375, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 236\n",
      "MEAN POLICY LOSS tensor(7.1989, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 237\n",
      "MEAN POLICY LOSS tensor(10.6425, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 238\n",
      "MEAN POLICY LOSS tensor(2.6092, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 239\n",
      "MEAN POLICY LOSS tensor(20.6047, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 240\n",
      "MEAN POLICY LOSS tensor(4.4692, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 241\n",
      "MEAN POLICY LOSS tensor(11.0061, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 242\n",
      "MEAN POLICY LOSS tensor(3.9525, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 243\n",
      "MEAN POLICY LOSS tensor(13.3119, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 244\n",
      "MEAN POLICY LOSS tensor(2.2344, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 245\n",
      "MEAN POLICY LOSS tensor(9.8858, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 246\n",
      "MEAN POLICY LOSS tensor(4.4750, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 247\n",
      "MEAN POLICY LOSS tensor(3.8496, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 248\n",
      "MEAN POLICY LOSS tensor(5.1433, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 249\n",
      "MEAN POLICY LOSS tensor(4.1088, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 250\n",
      "MEAN POLICY LOSS tensor(9.1020, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 251\n",
      "MEAN POLICY LOSS tensor(5.3146, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 252\n",
      "MEAN POLICY LOSS tensor(6.7029, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 253\n",
      "MEAN POLICY LOSS tensor(8.8988, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 254\n",
      "MEAN POLICY LOSS tensor(6.5628, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 255\n",
      "MEAN POLICY LOSS tensor(4.9599, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 256\n",
      "MEAN POLICY LOSS tensor(5.3493, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 257\n",
      "MEAN POLICY LOSS tensor(5.1179, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 258\n",
      "MEAN POLICY LOSS tensor(17.9234, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 259\n",
      "MEAN POLICY LOSS tensor(5.5507, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 260\n",
      "MEAN POLICY LOSS tensor(6.5003, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 261\n",
      "MEAN POLICY LOSS tensor(8.7946, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 262\n",
      "MEAN POLICY LOSS tensor(16.2491, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 263\n",
      "MEAN POLICY LOSS tensor(1.7820, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 264\n",
      "MEAN POLICY LOSS tensor(3.2809, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 265\n",
      "MEAN POLICY LOSS tensor(5.1271, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 266\n",
      "MEAN POLICY LOSS tensor(6.6878, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 267\n",
      "MEAN POLICY LOSS tensor(4.8488, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 268\n",
      "MEAN POLICY LOSS tensor(7.8170, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 269\n",
      "MEAN POLICY LOSS tensor(4.1759, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 270\n",
      "MEAN POLICY LOSS tensor(4.7296, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 271\n",
      "MEAN POLICY LOSS tensor(10.9117, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 272\n",
      "MEAN POLICY LOSS tensor(7.7429, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 273\n",
      "MEAN POLICY LOSS tensor(9.5541, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 274\n",
      "MEAN POLICY LOSS tensor(21.4208, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 275\n",
      "MEAN POLICY LOSS tensor(8.3061, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 276\n",
      "MEAN POLICY LOSS tensor(5.1746, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 277\n",
      "MEAN POLICY LOSS tensor(1.7137, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 278\n",
      "MEAN POLICY LOSS tensor(7.3603, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 279\n",
      "MEAN POLICY LOSS tensor(5.8070, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 280\n",
      "MEAN POLICY LOSS tensor(2.8753, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 281\n",
      "MEAN POLICY LOSS tensor(2.6925, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 282\n",
      "MEAN POLICY LOSS tensor(5.1074, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 283\n",
      "MEAN POLICY LOSS tensor(6.2162, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 284\n",
      "MEAN POLICY LOSS tensor(6.1583, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 285\n",
      "MEAN POLICY LOSS tensor(3.6974, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 286\n",
      "MEAN POLICY LOSS tensor(6.7607, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 287\n",
      "MEAN POLICY LOSS tensor(2.4102, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 288\n",
      "MEAN POLICY LOSS tensor(4.8037, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 289\n",
      "MEAN POLICY LOSS tensor(0.6304, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 290\n",
      "MEAN POLICY LOSS tensor(2.5490, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 291\n",
      "MEAN POLICY LOSS tensor(3.3098, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 292\n",
      "MEAN POLICY LOSS tensor(1.7577, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 293\n",
      "MEAN POLICY LOSS tensor(5.4859, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 294\n",
      "MEAN POLICY LOSS tensor(2.8059, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 295\n",
      "MEAN POLICY LOSS tensor(3.5933, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 296\n",
      "MEAN POLICY LOSS tensor(1.1290, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 297\n",
      "MEAN POLICY LOSS tensor(3.9063, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 298\n",
      "MEAN POLICY LOSS tensor(5.8059, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 299\n",
      "MEAN POLICY LOSS tensor(2.6647, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 300\n",
      "MEAN POLICY LOSS tensor(6.9951, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 301\n",
      "MEAN POLICY LOSS tensor(9.8627, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 302\n",
      "MEAN POLICY LOSS tensor(4.5560, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 303\n",
      "MEAN POLICY LOSS tensor(8.7356, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 304\n",
      "MEAN POLICY LOSS tensor(4.4235, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 305\n",
      "MEAN POLICY LOSS tensor(3.1004, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 306\n",
      "MEAN POLICY LOSS tensor(2.9141, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 307\n",
      "MEAN POLICY LOSS tensor(2.3119, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 308\n",
      "MEAN POLICY LOSS tensor(4.6532, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 309\n",
      "MEAN POLICY LOSS tensor(2.6069, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 310\n",
      "MEAN POLICY LOSS tensor(3.6025, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 311\n",
      "MEAN POLICY LOSS tensor(3.2404, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 312\n",
      "MEAN POLICY LOSS tensor(2.8406, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 313\n",
      "MEAN POLICY LOSS tensor(4.9483, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 314\n",
      "MEAN POLICY LOSS tensor(6.3389, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 315\n",
      "MEAN POLICY LOSS tensor(2.5444, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 316\n",
      "MEAN POLICY LOSS tensor(5.1260, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 317\n",
      "MEAN POLICY LOSS tensor(4.2534, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 318\n",
      "MEAN POLICY LOSS tensor(5.9864, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 319\n",
      "MEAN POLICY LOSS tensor(2.7515, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 320\n",
      "MEAN POLICY LOSS tensor(3.6349, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 321\n",
      "MEAN POLICY LOSS tensor(8.6146, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 322\n",
      "MEAN POLICY LOSS tensor(8.1162, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 323\n",
      "MEAN POLICY LOSS tensor(3.9699, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 324\n",
      "MEAN POLICY LOSS tensor(2.8244, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 325\n",
      "MEAN POLICY LOSS tensor(4.3217, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 326\n",
      "MEAN POLICY LOSS tensor(2.4669, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 327\n",
      "MEAN POLICY LOSS tensor(4.2765, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 328\n",
      "MEAN POLICY LOSS tensor(6.9558, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 329\n",
      "MEAN POLICY LOSS tensor(7.8077, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 330\n",
      "MEAN POLICY LOSS tensor(3.9525, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 331\n",
      "MEAN POLICY LOSS tensor(1.7623, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 332\n",
      "MEAN POLICY LOSS tensor(3.1686, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 333\n",
      "MEAN POLICY LOSS tensor(5.2579, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 334\n",
      "MEAN POLICY LOSS tensor(5.2405, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 335\n",
      "MEAN POLICY LOSS tensor(4.7539, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 336\n",
      "MEAN POLICY LOSS tensor(2.3119, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 337\n",
      "MEAN POLICY LOSS tensor(4.9333, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 338\n",
      "MEAN POLICY LOSS tensor(5.7908, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 339\n",
      "MEAN POLICY LOSS tensor(1.9093, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 340\n",
      "MEAN POLICY LOSS tensor(1.5061, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 341\n",
      "MEAN POLICY LOSS tensor(2.5352, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 342\n",
      "MEAN POLICY LOSS tensor(5.6855, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 343\n",
      "MEAN POLICY LOSS tensor(2.7261, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 344\n",
      "MEAN POLICY LOSS tensor(6.7943, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 345\n",
      "MEAN POLICY LOSS tensor(1.0653, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 346\n",
      "MEAN POLICY LOSS tensor(2.6971, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 347\n",
      "MEAN POLICY LOSS tensor(4.5595, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 348\n",
      "MEAN POLICY LOSS tensor(5.5125, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 349\n",
      "MEAN POLICY LOSS tensor(8.1116, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 350\n",
      "MEAN POLICY LOSS tensor(1.1949, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 351\n",
      "MEAN POLICY LOSS tensor(3.0437, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 352\n",
      "MEAN POLICY LOSS tensor(5.1271, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 353\n",
      "MEAN POLICY LOSS tensor(2.3778, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 354\n",
      "MEAN POLICY LOSS tensor(2.1973, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 355\n",
      "MEAN POLICY LOSS tensor(9.6120, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 356\n",
      "MEAN POLICY LOSS tensor(3.2820, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 357\n",
      "MEAN POLICY LOSS tensor(1.0411, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 358\n",
      "MEAN POLICY LOSS tensor(3.8253, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 359\n",
      "MEAN POLICY LOSS tensor(0.9890, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 360\n",
      "MEAN POLICY LOSS tensor(3.6164, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 361\n",
      "MEAN POLICY LOSS tensor(4.9286, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 362\n",
      "MEAN POLICY LOSS tensor(1.6484, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 363\n",
      "MEAN POLICY LOSS tensor(2.4114, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 364\n",
      "MEAN POLICY LOSS tensor(2.0377, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 365\n",
      "MEAN POLICY LOSS tensor(3.5875, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 366\n",
      "MEAN POLICY LOSS tensor(20.7525, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 367\n",
      "MEAN POLICY LOSS tensor(2.4796, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 368\n",
      "MEAN POLICY LOSS tensor(2.5097, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 369\n",
      "MEAN POLICY LOSS tensor(4.3830, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 370\n",
      "MEAN POLICY LOSS tensor(2.1406, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 371\n",
      "MEAN POLICY LOSS tensor(5.1109, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 372\n",
      "MEAN POLICY LOSS tensor(1.0387, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 373\n",
      "MEAN POLICY LOSS tensor(6.5593, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 374\n",
      "MEAN POLICY LOSS tensor(3.3283, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 375\n",
      "MEAN POLICY LOSS tensor(3.8021, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 376\n",
      "MEAN POLICY LOSS tensor(3.9907, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 377\n",
      "MEAN POLICY LOSS tensor(3.0934, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 378\n",
      "MEAN POLICY LOSS tensor(1.6507, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 379\n",
      "MEAN POLICY LOSS tensor(0.8629, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 380\n",
      "MEAN POLICY LOSS tensor(7.6416, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 381\n",
      "MEAN POLICY LOSS tensor(2.6080, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 382\n",
      "MEAN POLICY LOSS tensor(3.1154, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 383\n",
      "MEAN POLICY LOSS tensor(2.4345, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 384\n",
      "MEAN POLICY LOSS tensor(2.3720, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 385\n",
      "MEAN POLICY LOSS tensor(2.8464, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 386\n",
      "MEAN POLICY LOSS tensor(1.2851, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 387\n",
      "MEAN POLICY LOSS tensor(3.0066, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 388\n",
      "MEAN POLICY LOSS tensor(2.7469, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 389\n",
      "MEAN POLICY LOSS tensor(5.0646, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 390\n",
      "MEAN POLICY LOSS tensor(2.0678, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 391\n",
      "MEAN POLICY LOSS tensor(1.3384, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 392\n",
      "MEAN POLICY LOSS tensor(2.3350, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 393\n",
      "MEAN POLICY LOSS tensor(1.2099, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 394\n",
      "MEAN POLICY LOSS tensor(1.7010, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 395\n",
      "MEAN POLICY LOSS tensor(1.3465, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 396\n",
      "MEAN POLICY LOSS tensor(5.5449, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 397\n",
      "MEAN POLICY LOSS tensor(5.3609, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 398\n",
      "MEAN POLICY LOSS tensor(1.2562, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 399\n",
      "MEAN POLICY LOSS tensor(1.4876, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 400\n",
      "MEAN POLICY LOSS tensor(1.7161, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 401\n",
      "MEAN POLICY LOSS tensor(3.5771, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 402\n",
      "MEAN POLICY LOSS tensor(3.1987, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 403\n",
      "MEAN POLICY LOSS tensor(2.3234, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 404\n",
      "MEAN POLICY LOSS tensor(2.5363, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 405\n",
      "MEAN POLICY LOSS tensor(1.9474, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 406\n",
      "MEAN POLICY LOSS tensor(2.4218, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 407\n",
      "MEAN POLICY LOSS tensor(1.1382, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 408\n",
      "MEAN POLICY LOSS tensor(3.1362, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 409\n",
      "MEAN POLICY LOSS tensor(1.6848, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 410\n",
      "MEAN POLICY LOSS tensor(1.6033, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 411\n",
      "MEAN POLICY LOSS tensor(3.5065, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 412\n",
      "MEAN POLICY LOSS tensor(3.2647, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 413\n",
      "MEAN POLICY LOSS tensor(4.4773, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 414\n",
      "MEAN POLICY LOSS tensor(1.9185, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 415\n",
      "MEAN POLICY LOSS tensor(3.3561, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 416\n",
      "MEAN POLICY LOSS tensor(3.0957, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 417\n",
      "MEAN POLICY LOSS tensor(2.3986, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 418\n",
      "MEAN POLICY LOSS tensor(1.2065, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 419\n",
      "MEAN POLICY LOSS tensor(3.6245, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 420\n",
      "MEAN POLICY LOSS tensor(2.6636, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 421\n",
      "MEAN POLICY LOSS tensor(2.5548, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 422\n",
      "MEAN POLICY LOSS tensor(1.3245, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 423\n",
      "MEAN POLICY LOSS tensor(2.9442, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 424\n",
      "MEAN POLICY LOSS tensor(3.7402, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 425\n",
      "MEAN POLICY LOSS tensor(1.4644, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 426\n",
      "MEAN POLICY LOSS tensor(2.9430, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 427\n",
      "MEAN POLICY LOSS tensor(2.4727, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 428\n",
      "MEAN POLICY LOSS tensor(1.6704, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 429\n",
      "MEAN POLICY LOSS tensor(1.7496, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 430\n",
      "MEAN POLICY LOSS tensor(2.1915, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 431\n",
      "MEAN POLICY LOSS tensor(1.0457, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 432\n",
      "MEAN POLICY LOSS tensor(2.1314, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 433\n",
      "MEAN POLICY LOSS tensor(1.3141, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 434\n",
      "MEAN POLICY LOSS tensor(3.3595, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 435\n",
      "MEAN POLICY LOSS tensor(1.6860, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 436\n",
      "MEAN POLICY LOSS tensor(1.9555, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 437\n",
      "MEAN POLICY LOSS tensor(2.4657, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 438\n",
      "MEAN POLICY LOSS tensor(3.0656, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 439\n",
      "MEAN POLICY LOSS tensor(3.3144, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 440\n",
      "MEAN POLICY LOSS tensor(3.0587, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 441\n",
      "MEAN POLICY LOSS tensor(2.7388, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 442\n",
      "MEAN POLICY LOSS tensor(1.1186, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 443\n",
      "MEAN POLICY LOSS tensor(1.0723, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 444\n",
      "MEAN POLICY LOSS tensor(1.9035, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 445\n",
      "MEAN POLICY LOSS tensor(2.4831, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 446\n",
      "MEAN POLICY LOSS tensor(0.7611, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 447\n",
      "MEAN POLICY LOSS tensor(3.2311, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 448\n",
      "MEAN POLICY LOSS tensor(1.0943, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 449\n",
      "MEAN POLICY LOSS tensor(1.5420, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 450\n",
      "MEAN POLICY LOSS tensor(3.2010, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 451\n",
      "MEAN POLICY LOSS tensor(4.1446, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 452\n",
      "MEAN POLICY LOSS tensor(6.7561, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 453\n",
      "MEAN POLICY LOSS tensor(2.4241, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 454\n",
      "MEAN POLICY LOSS tensor(4.0266, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 455\n",
      "MEAN POLICY LOSS tensor(0.8965, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 456\n",
      "MEAN POLICY LOSS tensor(2.6879, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 457\n",
      "MEAN POLICY LOSS tensor(1.4286, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 458\n",
      "MEAN POLICY LOSS tensor(1.0179, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 459\n",
      "MEAN POLICY LOSS tensor(3.0032, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 460\n",
      "MEAN POLICY LOSS tensor(0.9022, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 461\n",
      "MEAN POLICY LOSS tensor(2.4843, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 462\n",
      "MEAN POLICY LOSS tensor(3.5238, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 463\n",
      "MEAN POLICY LOSS tensor(4.1828, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 464\n",
      "MEAN POLICY LOSS tensor(0.8768, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 465\n",
      "MEAN POLICY LOSS tensor(2.3813, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 466\n",
      "MEAN POLICY LOSS tensor(1.3788, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 467\n",
      "MEAN POLICY LOSS tensor(1.0607, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 468\n",
      "MEAN POLICY LOSS tensor(0.9890, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 469\n",
      "MEAN POLICY LOSS tensor(1.3187, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 470\n",
      "MEAN POLICY LOSS tensor(3.3410, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 471\n",
      "MEAN POLICY LOSS tensor(2.0435, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 472\n",
      "MEAN POLICY LOSS tensor(1.1926, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 473\n",
      "MEAN POLICY LOSS tensor(0.9555, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 474\n",
      "MEAN POLICY LOSS tensor(0.8849, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 475\n",
      "MEAN POLICY LOSS tensor(1.5107, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 476\n",
      "MEAN POLICY LOSS tensor(1.5038, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 477\n",
      "MEAN POLICY LOSS tensor(3.1883, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 478\n",
      "MEAN POLICY LOSS tensor(2.2251, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 479\n",
      "MEAN POLICY LOSS tensor(1.6600, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 480\n",
      "MEAN POLICY LOSS tensor(0.9902, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 481\n",
      "MEAN POLICY LOSS tensor(1.0480, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 482\n",
      "MEAN POLICY LOSS tensor(3.6257, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 483\n",
      "MEAN POLICY LOSS tensor(1.6669, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 484\n",
      "MEAN POLICY LOSS tensor(2.0134, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 485\n",
      "MEAN POLICY LOSS tensor(0.9450, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 486\n",
      "MEAN POLICY LOSS tensor(0.6235, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 487\n",
      "MEAN POLICY LOSS tensor(1.7589, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 488\n",
      "MEAN POLICY LOSS tensor(1.8688, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 489\n",
      "MEAN POLICY LOSS tensor(1.1509, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 490\n",
      "MEAN POLICY LOSS tensor(1.4563, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 491\n",
      "MEAN POLICY LOSS tensor(3.0714, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 492\n",
      "MEAN POLICY LOSS tensor(1.6750, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 493\n",
      "MEAN POLICY LOSS tensor(1.3395, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 494\n",
      "MEAN POLICY LOSS tensor(1.9579, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 495\n",
      "MEAN POLICY LOSS tensor(3.7801, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 496\n",
      "MEAN POLICY LOSS tensor(1.6299, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 497\n",
      "MEAN POLICY LOSS tensor(2.5224, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 498\n",
      "MEAN POLICY LOSS tensor(3.2566, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 499\n",
      "MEAN POLICY LOSS tensor(1.2701, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 500\n",
      "MEAN POLICY LOSS tensor(2.8371, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 501\n",
      "MEAN POLICY LOSS tensor(2.8510, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 502\n",
      "MEAN POLICY LOSS tensor(1.0295, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 503\n",
      "MEAN POLICY LOSS tensor(3.3861, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 504\n",
      "MEAN POLICY LOSS tensor(3.6754, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 505\n",
      "MEAN POLICY LOSS tensor(1.8433, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 506\n",
      "MEAN POLICY LOSS tensor(3.8021, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 507\n",
      "MEAN POLICY LOSS tensor(2.6474, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 508\n",
      "MEAN POLICY LOSS tensor(0.9601, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 509\n",
      "MEAN POLICY LOSS tensor(1.9243, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 510\n",
      "MEAN POLICY LOSS tensor(2.2679, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 511\n",
      "MEAN POLICY LOSS tensor(1.0295, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 512\n",
      "MEAN POLICY LOSS tensor(3.0240, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 513\n",
      "MEAN POLICY LOSS tensor(1.2076, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 514\n",
      "MEAN POLICY LOSS tensor(2.0828, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 515\n",
      "MEAN POLICY LOSS tensor(3.6037, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 516\n",
      "MEAN POLICY LOSS tensor(1.8641, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 517\n",
      "MEAN POLICY LOSS tensor(1.3893, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 518\n",
      "MEAN POLICY LOSS tensor(1.9914, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 519\n",
      "MEAN POLICY LOSS tensor(1.2435, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 520\n",
      "MEAN POLICY LOSS tensor(2.5895, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 521\n",
      "MEAN POLICY LOSS tensor(0.6501, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 522\n",
      "MEAN POLICY LOSS tensor(0.6639, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 523\n",
      "MEAN POLICY LOSS tensor(0.7993, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 524\n",
      "MEAN POLICY LOSS tensor(2.8927, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 525\n",
      "MEAN POLICY LOSS tensor(3.1883, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 526\n",
      "MEAN POLICY LOSS tensor(1.2354, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 527\n",
      "MEAN POLICY LOSS tensor(1.5223, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 528\n",
      "MEAN POLICY LOSS tensor(3.2253, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 529\n",
      "MEAN POLICY LOSS tensor(0.8143, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 530\n",
      "MEAN POLICY LOSS tensor(5.7677, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 531\n",
      "MEAN POLICY LOSS tensor(3.5944, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 532\n",
      "MEAN POLICY LOSS tensor(1.2516, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 533\n",
      "MEAN POLICY LOSS tensor(0.8479, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 534\n",
      "MEAN POLICY LOSS tensor(1.6952, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 535\n",
      "MEAN POLICY LOSS tensor(1.4841, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 536\n",
      "MEAN POLICY LOSS tensor(2.2922, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 537\n",
      "MEAN POLICY LOSS tensor(1.2493, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 538\n",
      "MEAN POLICY LOSS tensor(0.7079, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 539\n",
      "MEAN POLICY LOSS tensor(1.5223, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 540\n",
      "MEAN POLICY LOSS tensor(1.1382, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 541\n",
      "MEAN POLICY LOSS tensor(1.7635, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 542\n",
      "MEAN POLICY LOSS tensor(0.5136, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 543\n",
      "MEAN POLICY LOSS tensor(1.5269, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 544\n",
      "MEAN POLICY LOSS tensor(1.3256, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 545\n",
      "MEAN POLICY LOSS tensor(0.4615, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 546\n",
      "MEAN POLICY LOSS tensor(1.3349, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 547\n",
      "MEAN POLICY LOSS tensor(0.6570, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 548\n",
      "MEAN POLICY LOSS tensor(0.6616, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 549\n",
      "MEAN POLICY LOSS tensor(1.2435, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 550\n",
      "MEAN POLICY LOSS tensor(2.3662, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 551\n",
      "MEAN POLICY LOSS tensor(3.2658, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 552\n",
      "MEAN POLICY LOSS tensor(1.1718, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 553\n",
      "MEAN POLICY LOSS tensor(0.6489, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 554\n",
      "MEAN POLICY LOSS tensor(0.5760, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 555\n",
      "MEAN POLICY LOSS tensor(1.5339, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 556\n",
      "MEAN POLICY LOSS tensor(1.0711, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 557\n",
      "MEAN POLICY LOSS tensor(1.0029, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 558\n",
      "MEAN POLICY LOSS tensor(2.0423, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 559\n",
      "MEAN POLICY LOSS tensor(1.9590, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 560\n",
      "MEAN POLICY LOSS tensor(2.4565, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 561\n",
      "MEAN POLICY LOSS tensor(0.8097, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 562\n",
      "MEAN POLICY LOSS tensor(1.7623, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 563\n",
      "MEAN POLICY LOSS tensor(0.8652, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 564\n",
      "MEAN POLICY LOSS tensor(1.0144, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 565\n",
      "MEAN POLICY LOSS tensor(0.5251, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 566\n",
      "MEAN POLICY LOSS tensor(1.2007, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 567\n",
      "MEAN POLICY LOSS tensor(1.3916, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 568\n",
      "MEAN POLICY LOSS tensor(1.3164, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 569\n",
      "MEAN POLICY LOSS tensor(1.2967, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 570\n",
      "MEAN POLICY LOSS tensor(1.8988, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 571\n",
      "MEAN POLICY LOSS tensor(0.9034, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 572\n",
      "MEAN POLICY LOSS tensor(1.0098, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 573\n",
      "MEAN POLICY LOSS tensor(1.1648, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 574\n",
      "MEAN POLICY LOSS tensor(1.4691, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 575\n",
      "MEAN POLICY LOSS tensor(1.3465, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 576\n",
      "MEAN POLICY LOSS tensor(1.3141, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 577\n",
      "MEAN POLICY LOSS tensor(1.1729, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 578\n",
      "MEAN POLICY LOSS tensor(1.2782, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 579\n",
      "MEAN POLICY LOSS tensor(2.1696, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 580\n",
      "MEAN POLICY LOSS tensor(1.1151, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 581\n",
      "MEAN POLICY LOSS tensor(1.8156, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 582\n",
      "MEAN POLICY LOSS tensor(1.3916, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 583\n",
      "MEAN POLICY LOSS tensor(0.8109, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 584\n",
      "MEAN POLICY LOSS tensor(0.6732, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 585\n",
      "MEAN POLICY LOSS tensor(2.1626, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 586\n",
      "MEAN POLICY LOSS tensor(0.7056, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 587\n",
      "MEAN POLICY LOSS tensor(1.5975, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 588\n",
      "MEAN POLICY LOSS tensor(1.4887, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 589\n",
      "MEAN POLICY LOSS tensor(1.3869, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 590\n",
      "MEAN POLICY LOSS tensor(1.5524, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 591\n",
      "MEAN POLICY LOSS tensor(1.0619, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 592\n",
      "MEAN POLICY LOSS tensor(2.6196, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 593\n",
      "MEAN POLICY LOSS tensor(1.1024, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 594\n",
      "MEAN POLICY LOSS tensor(1.8896, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 595\n",
      "MEAN POLICY LOSS tensor(1.1660, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 596\n",
      "MEAN POLICY LOSS tensor(0.5425, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 597\n",
      "MEAN POLICY LOSS tensor(1.7485, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 598\n",
      "MEAN POLICY LOSS tensor(0.9312, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 599\n",
      "MEAN POLICY LOSS tensor(1.8757, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 600\n",
      "MEAN POLICY LOSS tensor(2.1406, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 601\n",
      "MEAN POLICY LOSS tensor(1.3904, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 602\n",
      "MEAN POLICY LOSS tensor(1.2689, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 603\n",
      "MEAN POLICY LOSS tensor(2.5271, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 604\n",
      "MEAN POLICY LOSS tensor(0.4847, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 605\n",
      "MEAN POLICY LOSS tensor(1.3279, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 606\n",
      "MEAN POLICY LOSS tensor(2.0597, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 607\n",
      "MEAN POLICY LOSS tensor(0.9080, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 608\n",
      "MEAN POLICY LOSS tensor(0.8224, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 609\n",
      "MEAN POLICY LOSS tensor(0.4673, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 610\n",
      "MEAN POLICY LOSS tensor(1.9231, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 611\n",
      "MEAN POLICY LOSS tensor(1.3869, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 612\n",
      "MEAN POLICY LOSS tensor(1.4066, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 613\n",
      "MEAN POLICY LOSS tensor(0.9797, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 614\n",
      "MEAN POLICY LOSS tensor(0.6663, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 615\n",
      "MEAN POLICY LOSS tensor(1.4413, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 616\n",
      "MEAN POLICY LOSS tensor(0.4592, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 617\n",
      "MEAN POLICY LOSS tensor(1.6172, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 618\n",
      "MEAN POLICY LOSS tensor(0.9404, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 619\n",
      "MEAN POLICY LOSS tensor(1.0862, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 620\n",
      "MEAN POLICY LOSS tensor(0.6292, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 621\n",
      "MEAN POLICY LOSS tensor(0.5275, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 622\n",
      "MEAN POLICY LOSS tensor(0.4048, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 623\n",
      "MEAN POLICY LOSS tensor(0.3852, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 624\n",
      "MEAN POLICY LOSS tensor(2.2968, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 625\n",
      "MEAN POLICY LOSS tensor(0.6142, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 626\n",
      "MEAN POLICY LOSS tensor(0.7229, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 627\n",
      "MEAN POLICY LOSS tensor(1.3638, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 628\n",
      "MEAN POLICY LOSS tensor(0.4580, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 629\n",
      "MEAN POLICY LOSS tensor(0.5101, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 630\n",
      "MEAN POLICY LOSS tensor(1.0850, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 631\n",
      "MEAN POLICY LOSS tensor(2.3709, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 632\n",
      "MEAN POLICY LOSS tensor(1.6681, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 633\n",
      "MEAN POLICY LOSS tensor(0.4002, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 634\n",
      "MEAN POLICY LOSS tensor(1.3893, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 635\n",
      "MEAN POLICY LOSS tensor(0.3840, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 636\n",
      "MEAN POLICY LOSS tensor(0.8143, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 637\n",
      "MEAN POLICY LOSS tensor(0.6628, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 638\n",
      "MEAN POLICY LOSS tensor(1.0075, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 639\n",
      "MEAN POLICY LOSS tensor(0.5309, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 640\n",
      "MEAN POLICY LOSS tensor(1.3245, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 641\n",
      "MEAN POLICY LOSS tensor(0.9659, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 642\n",
      "MEAN POLICY LOSS tensor(1.3869, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 643\n",
      "MEAN POLICY LOSS tensor(1.3962, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 644\n",
      "MEAN POLICY LOSS tensor(1.3407, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 645\n",
      "MEAN POLICY LOSS tensor(0.3759, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 646\n",
      "MEAN POLICY LOSS tensor(1.8109, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 647\n",
      "MEAN POLICY LOSS tensor(1.1903, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 648\n",
      "MEAN POLICY LOSS tensor(0.8190, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 649\n",
      "MEAN POLICY LOSS tensor(0.3551, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 650\n",
      "MEAN POLICY LOSS tensor(0.6906, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 651\n",
      "MEAN POLICY LOSS tensor(0.8745, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 652\n",
      "MEAN POLICY LOSS tensor(0.9497, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 653\n",
      "MEAN POLICY LOSS tensor(0.8988, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 654\n",
      "MEAN POLICY LOSS tensor(0.5170, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 655\n",
      "MEAN POLICY LOSS tensor(0.5841, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 656\n",
      "MEAN POLICY LOSS tensor(0.7010, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 657\n",
      "MEAN POLICY LOSS tensor(0.9971, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 658\n",
      "MEAN POLICY LOSS tensor(0.8895, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 659\n",
      "MEAN POLICY LOSS tensor(0.3967, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 660\n",
      "MEAN POLICY LOSS tensor(1.0191, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 661\n",
      "MEAN POLICY LOSS tensor(1.0411, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 662\n",
      "MEAN POLICY LOSS tensor(1.1336, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 663\n",
      "MEAN POLICY LOSS tensor(0.6605, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 664\n",
      "MEAN POLICY LOSS tensor(0.8629, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 665\n",
      "MEAN POLICY LOSS tensor(0.3574, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 666\n",
      "MEAN POLICY LOSS tensor(1.6588, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 667\n",
      "MEAN POLICY LOSS tensor(0.9763, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 668\n",
      "MEAN POLICY LOSS tensor(1.2655, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 669\n",
      "MEAN POLICY LOSS tensor(0.6466, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 670\n",
      "MEAN POLICY LOSS tensor(0.8953, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 671\n",
      "MEAN POLICY LOSS tensor(1.1695, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 672\n",
      "MEAN POLICY LOSS tensor(1.5975, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 673\n",
      "MEAN POLICY LOSS tensor(0.6778, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 674\n",
      "MEAN POLICY LOSS tensor(1.2736, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 675\n",
      "MEAN POLICY LOSS tensor(0.6744, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 676\n",
      "MEAN POLICY LOSS tensor(1.5223, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 677\n",
      "MEAN POLICY LOSS tensor(0.5078, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 678\n",
      "MEAN POLICY LOSS tensor(1.2898, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 679\n",
      "MEAN POLICY LOSS tensor(0.8051, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 680\n",
      "MEAN POLICY LOSS tensor(1.3164, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 681\n",
      "MEAN POLICY LOSS tensor(0.7704, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 682\n",
      "MEAN POLICY LOSS tensor(0.6188, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 683\n",
      "MEAN POLICY LOSS tensor(0.8271, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 684\n",
      "MEAN POLICY LOSS tensor(1.2470, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 685\n",
      "MEAN POLICY LOSS tensor(0.6894, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 686\n",
      "MEAN POLICY LOSS tensor(1.0341, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 687\n",
      "MEAN POLICY LOSS tensor(1.7566, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 688\n",
      "MEAN POLICY LOSS tensor(0.6709, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 689\n",
      "MEAN POLICY LOSS tensor(0.2996, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 690\n",
      "MEAN POLICY LOSS tensor(0.6570, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 691\n",
      "MEAN POLICY LOSS tensor(0.7148, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 692\n",
      "MEAN POLICY LOSS tensor(0.7345, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 693\n",
      "MEAN POLICY LOSS tensor(0.4291, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 694\n",
      "MEAN POLICY LOSS tensor(0.6767, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 695\n",
      "MEAN POLICY LOSS tensor(0.7519, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 696\n",
      "MEAN POLICY LOSS tensor(1.8329, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 697\n",
      "MEAN POLICY LOSS tensor(0.5980, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 698\n",
      "MEAN POLICY LOSS tensor(0.6362, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 699\n",
      "MEAN POLICY LOSS tensor(0.2799, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 700\n",
      "MEAN POLICY LOSS tensor(1.3742, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 701\n",
      "MEAN POLICY LOSS tensor(0.7634, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 702\n",
      "MEAN POLICY LOSS tensor(0.4847, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 703\n",
      "MEAN POLICY LOSS tensor(2.1314, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 704\n",
      "MEAN POLICY LOSS tensor(1.3950, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 705\n",
      "MEAN POLICY LOSS tensor(0.6709, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 706\n",
      "MEAN POLICY LOSS tensor(0.7553, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 707\n",
      "MEAN POLICY LOSS tensor(1.8283, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 708\n",
      "MEAN POLICY LOSS tensor(0.6397, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 709\n",
      "MEAN POLICY LOSS tensor(0.5147, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 710\n",
      "MEAN POLICY LOSS tensor(0.7449, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 711\n",
      "MEAN POLICY LOSS tensor(0.8213, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 712\n",
      "MEAN POLICY LOSS tensor(0.2498, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 713\n",
      "MEAN POLICY LOSS tensor(1.3650, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 714\n",
      "MEAN POLICY LOSS tensor(1.9231, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 715\n",
      "MEAN POLICY LOSS tensor(0.5888, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 716\n",
      "MEAN POLICY LOSS tensor(1.1509, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 717\n",
      "MEAN POLICY LOSS tensor(1.4182, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 718\n",
      "MEAN POLICY LOSS tensor(0.9797, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 719\n",
      "MEAN POLICY LOSS tensor(1.2493, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 720\n",
      "MEAN POLICY LOSS tensor(0.4916, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 721\n",
      "MEAN POLICY LOSS tensor(1.0561, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 722\n",
      "MEAN POLICY LOSS tensor(0.9022, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 723\n",
      "MEAN POLICY LOSS tensor(0.5957, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 724\n",
      "MEAN POLICY LOSS tensor(1.7415, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 725\n",
      "MEAN POLICY LOSS tensor(0.4719, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 726\n",
      "MEAN POLICY LOSS tensor(0.8490, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 727\n",
      "MEAN POLICY LOSS tensor(1.8132, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 728\n",
      "MEAN POLICY LOSS tensor(1.4135, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 729\n",
      "MEAN POLICY LOSS tensor(0.2487, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 730\n",
      "MEAN POLICY LOSS tensor(0.7958, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 731\n",
      "MEAN POLICY LOSS tensor(0.8490, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 732\n",
      "MEAN POLICY LOSS tensor(1.4078, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 733\n",
      "MEAN POLICY LOSS tensor(0.6848, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 734\n",
      "MEAN POLICY LOSS tensor(0.6003, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 735\n",
      "MEAN POLICY LOSS tensor(0.4731, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 736\n",
      "MEAN POLICY LOSS tensor(1.1833, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 737\n",
      "MEAN POLICY LOSS tensor(0.5136, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 738\n",
      "MEAN POLICY LOSS tensor(0.6177, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 739\n",
      "MEAN POLICY LOSS tensor(0.4638, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 740\n",
      "MEAN POLICY LOSS tensor(1.4818, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 741\n",
      "MEAN POLICY LOSS tensor(0.5032, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 742\n",
      "MEAN POLICY LOSS tensor(0.6073, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 743\n",
      "MEAN POLICY LOSS tensor(0.7195, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 744\n",
      "MEAN POLICY LOSS tensor(1.4309, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 745\n",
      "MEAN POLICY LOSS tensor(0.8340, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 746\n",
      "MEAN POLICY LOSS tensor(0.7507, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 747\n",
      "MEAN POLICY LOSS tensor(0.7241, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 748\n",
      "MEAN POLICY LOSS tensor(0.8826, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 749\n",
      "MEAN POLICY LOSS tensor(1.0677, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 750\n",
      "MEAN POLICY LOSS tensor(1.0468, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 751\n",
      "MEAN POLICY LOSS tensor(0.2637, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 752\n",
      "MEAN POLICY LOSS tensor(0.4893, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 753\n",
      "MEAN POLICY LOSS tensor(0.8675, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 754\n",
      "MEAN POLICY LOSS tensor(0.7750, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 755\n",
      "MEAN POLICY LOSS tensor(0.4407, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 756\n",
      "MEAN POLICY LOSS tensor(0.5992, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 757\n",
      "MEAN POLICY LOSS tensor(0.8190, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 758\n",
      "MEAN POLICY LOSS tensor(0.5668, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 759\n",
      "MEAN POLICY LOSS tensor(0.9878, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 760\n",
      "MEAN POLICY LOSS tensor(0.3447, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 761\n",
      "MEAN POLICY LOSS tensor(0.9219, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 762\n",
      "MEAN POLICY LOSS tensor(0.2822, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 763\n",
      "MEAN POLICY LOSS tensor(0.6316, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 764\n",
      "MEAN POLICY LOSS tensor(1.3985, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 765\n",
      "MEAN POLICY LOSS tensor(1.2632, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 766\n",
      "MEAN POLICY LOSS tensor(1.0954, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 767\n",
      "MEAN POLICY LOSS tensor(0.4407, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 768\n",
      "MEAN POLICY LOSS tensor(0.5194, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 769\n",
      "MEAN POLICY LOSS tensor(0.4580, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 770\n",
      "MEAN POLICY LOSS tensor(0.3077, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 771\n",
      "MEAN POLICY LOSS tensor(0.4742, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 772\n",
      "MEAN POLICY LOSS tensor(0.8375, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 773\n",
      "MEAN POLICY LOSS tensor(0.8004, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 774\n",
      "MEAN POLICY LOSS tensor(0.7287, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 775\n",
      "MEAN POLICY LOSS tensor(0.6651, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 776\n",
      "MEAN POLICY LOSS tensor(0.7842, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 777\n",
      "MEAN POLICY LOSS tensor(0.5425, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 778\n",
      "MEAN POLICY LOSS tensor(0.7542, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 779\n",
      "MEAN POLICY LOSS tensor(0.5286, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 780\n",
      "MEAN POLICY LOSS tensor(0.3516, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 781\n",
      "MEAN POLICY LOSS tensor(0.5830, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 782\n",
      "MEAN POLICY LOSS tensor(1.1660, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 783\n",
      "MEAN POLICY LOSS tensor(1.4887, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 784\n",
      "MEAN POLICY LOSS tensor(0.2938, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 785\n",
      "MEAN POLICY LOSS tensor(0.8490, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 786\n",
      "MEAN POLICY LOSS tensor(0.9566, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 787\n",
      "MEAN POLICY LOSS tensor(1.2759, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 788\n",
      "MEAN POLICY LOSS tensor(0.8305, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 789\n",
      "MEAN POLICY LOSS tensor(0.7842, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 790\n",
      "MEAN POLICY LOSS tensor(0.3204, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 791\n",
      "MEAN POLICY LOSS tensor(0.4303, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 792\n",
      "MEAN POLICY LOSS tensor(1.1267, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 793\n",
      "MEAN POLICY LOSS tensor(0.5356, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 794\n",
      "MEAN POLICY LOSS tensor(0.8178, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 795\n",
      "MEAN POLICY LOSS tensor(1.3545, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 796\n",
      "MEAN POLICY LOSS tensor(0.3910, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 797\n",
      "MEAN POLICY LOSS tensor(0.7391, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 798\n",
      "MEAN POLICY LOSS tensor(0.8675, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 799\n",
      "MEAN POLICY LOSS tensor(0.8271, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 800\n",
      "MEAN POLICY LOSS tensor(0.6431, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 801\n",
      "MEAN POLICY LOSS tensor(0.6790, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 802\n",
      "MEAN POLICY LOSS tensor(0.8317, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 803\n",
      "MEAN POLICY LOSS tensor(1.5304, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 804\n",
      "MEAN POLICY LOSS tensor(1.0098, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 805\n",
      "MEAN POLICY LOSS tensor(0.6778, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 806\n",
      "MEAN POLICY LOSS tensor(0.5749, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 807\n",
      "MEAN POLICY LOSS tensor(0.6628, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 808\n",
      "MEAN POLICY LOSS tensor(1.2932, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 809\n",
      "MEAN POLICY LOSS tensor(0.4997, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 810\n",
      "MEAN POLICY LOSS tensor(0.8872, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 811\n",
      "MEAN POLICY LOSS tensor(0.5506, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 812\n",
      "MEAN POLICY LOSS tensor(0.2464, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 813\n",
      "MEAN POLICY LOSS tensor(0.8895, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 814\n",
      "MEAN POLICY LOSS tensor(0.9855, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 815\n",
      "MEAN POLICY LOSS tensor(0.9844, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 816\n",
      "MEAN POLICY LOSS tensor(0.7681, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 817\n",
      "MEAN POLICY LOSS tensor(1.0237, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 818\n",
      "MEAN POLICY LOSS tensor(0.4893, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 819\n",
      "MEAN POLICY LOSS tensor(0.8409, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 820\n",
      "MEAN POLICY LOSS tensor(0.7819, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 821\n",
      "MEAN POLICY LOSS tensor(0.3389, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 822\n",
      "MEAN POLICY LOSS tensor(0.3042, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 823\n",
      "MEAN POLICY LOSS tensor(0.5066, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 824\n",
      "MEAN POLICY LOSS tensor(0.5460, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 825\n",
      "MEAN POLICY LOSS tensor(0.5899, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 826\n",
      "MEAN POLICY LOSS tensor(0.9716, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 827\n",
      "MEAN POLICY LOSS tensor(0.5101, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 828\n",
      "MEAN POLICY LOSS tensor(0.5760, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 829\n",
      "MEAN POLICY LOSS tensor(0.1689, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 830\n",
      "MEAN POLICY LOSS tensor(0.2961, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 831\n",
      "MEAN POLICY LOSS tensor(0.6894, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 832\n",
      "MEAN POLICY LOSS tensor(0.5182, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 833\n",
      "MEAN POLICY LOSS tensor(0.8039, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 834\n",
      "MEAN POLICY LOSS tensor(0.8097, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 835\n",
      "MEAN POLICY LOSS tensor(0.2857, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 836\n",
      "MEAN POLICY LOSS tensor(0.2579, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 837\n",
      "MEAN POLICY LOSS tensor(0.6547, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 838\n",
      "MEAN POLICY LOSS tensor(1.2331, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 839\n",
      "MEAN POLICY LOSS tensor(0.7461, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 840\n",
      "MEAN POLICY LOSS tensor(0.3447, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 841\n",
      "MEAN POLICY LOSS tensor(0.3701, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 842\n",
      "MEAN POLICY LOSS tensor(1.0087, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 843\n",
      "MEAN POLICY LOSS tensor(0.7472, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 844\n",
      "MEAN POLICY LOSS tensor(0.8120, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 845\n",
      "MEAN POLICY LOSS tensor(0.4245, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 846\n",
      "MEAN POLICY LOSS tensor(0.3516, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 847\n",
      "MEAN POLICY LOSS tensor(0.3401, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 848\n",
      "MEAN POLICY LOSS tensor(0.4199, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 849\n",
      "MEAN POLICY LOSS tensor(0.7877, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 850\n",
      "MEAN POLICY LOSS tensor(0.8965, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 851\n",
      "MEAN POLICY LOSS tensor(0.3655, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 852\n",
      "MEAN POLICY LOSS tensor(1.2551, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 853\n",
      "MEAN POLICY LOSS tensor(0.4974, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 854\n",
      "MEAN POLICY LOSS tensor(0.2209, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 855\n",
      "MEAN POLICY LOSS tensor(0.5251, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 856\n",
      "MEAN POLICY LOSS tensor(0.9161, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 857\n",
      "MEAN POLICY LOSS tensor(0.6084, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 858\n",
      "MEAN POLICY LOSS tensor(0.7900, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 859\n",
      "MEAN POLICY LOSS tensor(0.8618, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 860\n",
      "MEAN POLICY LOSS tensor(0.2290, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 861\n",
      "MEAN POLICY LOSS tensor(0.2117, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 862\n",
      "MEAN POLICY LOSS tensor(0.2672, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 863\n",
      "MEAN POLICY LOSS tensor(0.5749, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 864\n",
      "MEAN POLICY LOSS tensor(0.3031, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 865\n",
      "MEAN POLICY LOSS tensor(0.7935, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 866\n",
      "MEAN POLICY LOSS tensor(0.3551, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 867\n",
      "MEAN POLICY LOSS tensor(0.4916, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 868\n",
      "MEAN POLICY LOSS tensor(0.4488, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 869\n",
      "MEAN POLICY LOSS tensor(0.6362, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 870\n",
      "MEAN POLICY LOSS tensor(1.0306, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 871\n",
      "MEAN POLICY LOSS tensor(0.3401, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 872\n",
      "MEAN POLICY LOSS tensor(0.3169, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 873\n",
      "MEAN POLICY LOSS tensor(0.7704, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 874\n",
      "MEAN POLICY LOSS tensor(0.5714, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 875\n",
      "MEAN POLICY LOSS tensor(0.1723, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 876\n",
      "MEAN POLICY LOSS tensor(0.3505, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 877\n",
      "MEAN POLICY LOSS tensor(0.2429, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 878\n",
      "MEAN POLICY LOSS tensor(0.8594, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 879\n",
      "MEAN POLICY LOSS tensor(0.3320, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 880\n",
      "MEAN POLICY LOSS tensor(0.2753, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 881\n",
      "MEAN POLICY LOSS tensor(0.3528, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 882\n",
      "MEAN POLICY LOSS tensor(0.4881, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 883\n",
      "MEAN POLICY LOSS tensor(0.2961, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 884\n",
      "MEAN POLICY LOSS tensor(0.4777, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 885\n",
      "MEAN POLICY LOSS tensor(0.2637, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 886\n",
      "MEAN POLICY LOSS tensor(0.5922, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 887\n",
      "MEAN POLICY LOSS tensor(0.2256, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 888\n",
      "MEAN POLICY LOSS tensor(0.6836, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 889\n",
      "MEAN POLICY LOSS tensor(0.4419, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 890\n",
      "MEAN POLICY LOSS tensor(0.4974, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 891\n",
      "MEAN POLICY LOSS tensor(0.7993, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 892\n",
      "MEAN POLICY LOSS tensor(0.4592, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 893\n",
      "MEAN POLICY LOSS tensor(0.5344, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 894\n",
      "MEAN POLICY LOSS tensor(0.4141, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 895\n",
      "MEAN POLICY LOSS tensor(0.6801, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 896\n",
      "MEAN POLICY LOSS tensor(1.1984, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 897\n",
      "MEAN POLICY LOSS tensor(0.6142, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 898\n",
      "MEAN POLICY LOSS tensor(0.2475, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 899\n",
      "MEAN POLICY LOSS tensor(0.1469, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 900\n",
      "MEAN POLICY LOSS tensor(0.6003, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 901\n",
      "MEAN POLICY LOSS tensor(0.2117, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 902\n",
      "MEAN POLICY LOSS tensor(0.8444, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 903\n",
      "MEAN POLICY LOSS tensor(0.3759, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 904\n",
      "MEAN POLICY LOSS tensor(0.6755, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 905\n",
      "MEAN POLICY LOSS tensor(0.9601, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 906\n",
      "MEAN POLICY LOSS tensor(0.4870, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 907\n",
      "MEAN POLICY LOSS tensor(0.2984, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 908\n",
      "MEAN POLICY LOSS tensor(0.7449, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 909\n",
      "MEAN POLICY LOSS tensor(0.2603, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 910\n",
      "MEAN POLICY LOSS tensor(0.6570, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 911\n",
      "MEAN POLICY LOSS tensor(0.7218, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 912\n",
      "MEAN POLICY LOSS tensor(0.3539, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 913\n",
      "MEAN POLICY LOSS tensor(0.6397, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 914\n",
      "MEAN POLICY LOSS tensor(0.2475, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 915\n",
      "MEAN POLICY LOSS tensor(0.5691, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 916\n",
      "MEAN POLICY LOSS tensor(0.2163, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 917\n",
      "MEAN POLICY LOSS tensor(0.4557, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 918\n",
      "MEAN POLICY LOSS tensor(1.2204, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 919\n",
      "MEAN POLICY LOSS tensor(0.2857, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 920\n",
      "MEAN POLICY LOSS tensor(0.5841, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 921\n",
      "MEAN POLICY LOSS tensor(0.8166, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 922\n",
      "MEAN POLICY LOSS tensor(0.8051, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 923\n",
      "MEAN POLICY LOSS tensor(0.3933, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 924\n",
      "MEAN POLICY LOSS tensor(0.2001, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 925\n",
      "MEAN POLICY LOSS tensor(0.4708, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 926\n",
      "MEAN POLICY LOSS tensor(0.2603, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 927\n",
      "MEAN POLICY LOSS tensor(0.5136, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 928\n",
      "MEAN POLICY LOSS tensor(0.2313, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 929\n",
      "MEAN POLICY LOSS tensor(0.5309, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 930\n",
      "MEAN POLICY LOSS tensor(0.2510, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 931\n",
      "MEAN POLICY LOSS tensor(0.5818, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 932\n",
      "MEAN POLICY LOSS tensor(0.6073, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 933\n",
      "MEAN POLICY LOSS tensor(0.2256, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 934\n",
      "MEAN POLICY LOSS tensor(0.5760, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 935\n",
      "MEAN POLICY LOSS tensor(0.3054, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 936\n",
      "MEAN POLICY LOSS tensor(0.7010, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 937\n",
      "MEAN POLICY LOSS tensor(0.5332, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 938\n",
      "MEAN POLICY LOSS tensor(0.6871, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 939\n",
      "MEAN POLICY LOSS tensor(1.7265, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 940\n",
      "MEAN POLICY LOSS tensor(0.4604, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 941\n",
      "MEAN POLICY LOSS tensor(0.4395, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 942\n",
      "MEAN POLICY LOSS tensor(0.1747, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 943\n",
      "MEAN POLICY LOSS tensor(0.4650, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 944\n",
      "MEAN POLICY LOSS tensor(0.2036, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 945\n",
      "MEAN POLICY LOSS tensor(0.5506, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 946\n",
      "MEAN POLICY LOSS tensor(0.7727, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 947\n",
      "MEAN POLICY LOSS tensor(0.3586, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 948\n",
      "MEAN POLICY LOSS tensor(0.8351, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 949\n",
      "MEAN POLICY LOSS tensor(0.2961, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 950\n",
      "MEAN POLICY LOSS tensor(0.4962, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 951\n",
      "MEAN POLICY LOSS tensor(0.2406, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 952\n",
      "MEAN POLICY LOSS tensor(1.1417, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 953\n",
      "MEAN POLICY LOSS tensor(0.5529, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 954\n",
      "MEAN POLICY LOSS tensor(0.3701, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 955\n",
      "MEAN POLICY LOSS tensor(0.2718, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 956\n",
      "MEAN POLICY LOSS tensor(0.5969, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 957\n",
      "MEAN POLICY LOSS tensor(0.2614, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 958\n",
      "MEAN POLICY LOSS tensor(0.4291, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 959\n",
      "MEAN POLICY LOSS tensor(0.2475, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 960\n",
      "MEAN POLICY LOSS tensor(0.7403, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 961\n",
      "MEAN POLICY LOSS tensor(0.1989, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 962\n",
      "MEAN POLICY LOSS tensor(0.7947, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 963\n",
      "MEAN POLICY LOSS tensor(0.2811, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 964\n",
      "MEAN POLICY LOSS tensor(0.4222, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 965\n",
      "MEAN POLICY LOSS tensor(0.4395, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 966\n",
      "MEAN POLICY LOSS tensor(0.2637, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 967\n",
      "MEAN POLICY LOSS tensor(0.1562, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 968\n",
      "MEAN POLICY LOSS tensor(1.1625, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 969\n",
      "MEAN POLICY LOSS tensor(0.3354, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 970\n",
      "MEAN POLICY LOSS tensor(0.2707, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 971\n",
      "MEAN POLICY LOSS tensor(0.2394, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 972\n",
      "MEAN POLICY LOSS tensor(0.7727, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 973\n",
      "MEAN POLICY LOSS tensor(0.4465, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 974\n",
      "MEAN POLICY LOSS tensor(0.5922, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 975\n",
      "MEAN POLICY LOSS tensor(0.2718, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 976\n",
      "MEAN POLICY LOSS tensor(0.1215, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 977\n",
      "MEAN POLICY LOSS tensor(0.8340, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 978\n",
      "MEAN POLICY LOSS tensor(0.2753, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 979\n",
      "MEAN POLICY LOSS tensor(0.4789, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 980\n",
      "MEAN POLICY LOSS tensor(0.3088, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 981\n",
      "MEAN POLICY LOSS tensor(0.8745, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 982\n",
      "MEAN POLICY LOSS tensor(1.0434, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 983\n",
      "MEAN POLICY LOSS tensor(0.2579, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 984\n",
      "MEAN POLICY LOSS tensor(0.8004, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 985\n",
      "MEAN POLICY LOSS tensor(0.2151, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 986\n",
      "MEAN POLICY LOSS tensor(0.4257, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 987\n",
      "MEAN POLICY LOSS tensor(0.5113, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 988\n",
      "MEAN POLICY LOSS tensor(0.5494, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 989\n",
      "MEAN POLICY LOSS tensor(0.1712, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 990\n",
      "MEAN POLICY LOSS tensor(0.8340, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 991\n",
      "MEAN POLICY LOSS tensor(0.2429, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 992\n",
      "MEAN POLICY LOSS tensor(0.4789, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 993\n",
      "MEAN POLICY LOSS tensor(0.8109, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 994\n",
      "MEAN POLICY LOSS tensor(0.5899, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 995\n",
      "MEAN POLICY LOSS tensor(0.6593, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 996\n",
      "MEAN POLICY LOSS tensor(0.1828, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 997\n",
      "MEAN POLICY LOSS tensor(0.2973, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 998\n",
      "MEAN POLICY LOSS tensor(0.6848, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 999\n",
      "MEAN POLICY LOSS tensor(0.3216, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "Run 2/10\n",
      "EPSIODE# 0\n",
      "MEAN POLICY LOSS tensor(11081.6152, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 1\n",
      "MEAN POLICY LOSS tensor(11613.4111, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 2\n",
      "MEAN POLICY LOSS tensor(12281.4814, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 3\n",
      "MEAN POLICY LOSS tensor(8917.1816, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 4\n",
      "MEAN POLICY LOSS tensor(6475.4663, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 5\n",
      "MEAN POLICY LOSS tensor(8244.1836, grad_fn=<AddBackward0>)\n",
      "REWARD -481.0\n",
      "EPSIODE# 6\n",
      "MEAN POLICY LOSS tensor(7001.9473, grad_fn=<AddBackward0>)\n",
      "REWARD -396.0\n",
      "EPSIODE# 7\n",
      "MEAN POLICY LOSS tensor(16350.2871, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 8\n",
      "MEAN POLICY LOSS tensor(8677.9062, grad_fn=<AddBackward0>)\n",
      "REWARD -210.0\n",
      "EPSIODE# 9\n",
      "MEAN POLICY LOSS tensor(306.6302, grad_fn=<AddBackward0>)\n",
      "REWARD -320.0\n",
      "EPSIODE# 10\n",
      "MEAN POLICY LOSS tensor(3686.9487, grad_fn=<AddBackward0>)\n",
      "REWARD -254.0\n",
      "EPSIODE# 11\n",
      "MEAN POLICY LOSS tensor(2472.7314, grad_fn=<AddBackward0>)\n",
      "REWARD -208.0\n",
      "EPSIODE# 12\n",
      "MEAN POLICY LOSS tensor(8888.7344, grad_fn=<AddBackward0>)\n",
      "REWARD -175.0\n",
      "EPSIODE# 13\n",
      "MEAN POLICY LOSS tensor(9.8047, grad_fn=<AddBackward0>)\n",
      "REWARD -354.0\n",
      "EPSIODE# 14\n",
      "MEAN POLICY LOSS tensor(0.7119, grad_fn=<AddBackward0>)\n",
      "REWARD -267.0\n",
      "EPSIODE# 15\n",
      "MEAN POLICY LOSS tensor(801.3441, grad_fn=<AddBackward0>)\n",
      "REWARD -182.0\n",
      "EPSIODE# 16\n",
      "MEAN POLICY LOSS tensor(177.4064, grad_fn=<AddBackward0>)\n",
      "REWARD -153.0\n",
      "EPSIODE# 17\n",
      "MEAN POLICY LOSS tensor(119.9463, grad_fn=<AddBackward0>)\n",
      "REWARD -203.0\n",
      "EPSIODE# 18\n",
      "MEAN POLICY LOSS tensor(7914.5439, grad_fn=<AddBackward0>)\n",
      "REWARD -137.0\n",
      "EPSIODE# 19\n",
      "MEAN POLICY LOSS tensor(1826.0109, grad_fn=<AddBackward0>)\n",
      "REWARD -141.0\n",
      "EPSIODE# 20\n",
      "MEAN POLICY LOSS tensor(5001.2944, grad_fn=<AddBackward0>)\n",
      "REWARD -177.0\n",
      "EPSIODE# 21\n",
      "MEAN POLICY LOSS tensor(3032.7739, grad_fn=<AddBackward0>)\n",
      "REWARD -193.0\n",
      "EPSIODE# 22\n",
      "MEAN POLICY LOSS tensor(1.8871, grad_fn=<AddBackward0>)\n",
      "REWARD -147.0\n",
      "EPSIODE# 23\n",
      "MEAN POLICY LOSS tensor(173.6107, grad_fn=<AddBackward0>)\n",
      "REWARD -138.0\n",
      "EPSIODE# 24\n",
      "MEAN POLICY LOSS tensor(362.9199, grad_fn=<AddBackward0>)\n",
      "REWARD -264.0\n",
      "EPSIODE# 25\n",
      "MEAN POLICY LOSS tensor(2967.6880, grad_fn=<AddBackward0>)\n",
      "REWARD -194.0\n",
      "EPSIODE# 26\n",
      "MEAN POLICY LOSS tensor(0., grad_fn=<AddBackward0>)\n",
      "REWARD -290.0\n",
      "EPSIODE# 27\n",
      "MEAN POLICY LOSS tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "REWARD -423.0\n",
      "EPSIODE# 28\n",
      "MEAN POLICY LOSS tensor(0., grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 29\n",
      "MEAN POLICY LOSS tensor(1.4433, grad_fn=<AddBackward0>)\n",
      "REWARD -331.0\n",
      "EPSIODE# 30\n",
      "MEAN POLICY LOSS tensor(616.1649, grad_fn=<AddBackward0>)\n",
      "REWARD -311.0\n",
      "EPSIODE# 31\n",
      "MEAN POLICY LOSS tensor(1824.8848, grad_fn=<AddBackward0>)\n",
      "REWARD -217.0\n",
      "EPSIODE# 32\n",
      "MEAN POLICY LOSS tensor(1283.4275, grad_fn=<AddBackward0>)\n",
      "REWARD -278.0\n",
      "EPSIODE# 33\n",
      "MEAN POLICY LOSS tensor(285.5858, grad_fn=<AddBackward0>)\n",
      "REWARD -395.0\n",
      "EPSIODE# 34\n",
      "MEAN POLICY LOSS tensor(355.6790, grad_fn=<AddBackward0>)\n",
      "REWARD -370.0\n",
      "EPSIODE# 35\n",
      "MEAN POLICY LOSS tensor(499.2916, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 36\n",
      "MEAN POLICY LOSS tensor(0.0042, grad_fn=<AddBackward0>)\n",
      "REWARD -354.0\n",
      "EPSIODE# 37\n",
      "MEAN POLICY LOSS tensor(0.0022, grad_fn=<AddBackward0>)\n",
      "REWARD -388.0\n",
      "EPSIODE# 38\n",
      "MEAN POLICY LOSS tensor(2.4516, grad_fn=<AddBackward0>)\n",
      "REWARD -447.0\n",
      "EPSIODE# 39\n",
      "MEAN POLICY LOSS tensor(518.4149, grad_fn=<AddBackward0>)\n",
      "REWARD -386.0\n",
      "EPSIODE# 40\n",
      "MEAN POLICY LOSS tensor(1.3071, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 41\n",
      "MEAN POLICY LOSS tensor(332.6467, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 42\n",
      "MEAN POLICY LOSS tensor(0.5240, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 43\n",
      "MEAN POLICY LOSS tensor(1.7086, grad_fn=<AddBackward0>)\n",
      "REWARD -442.0\n",
      "EPSIODE# 44\n",
      "MEAN POLICY LOSS tensor(1.4274, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 45\n",
      "MEAN POLICY LOSS tensor(24.8700, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 46\n",
      "MEAN POLICY LOSS tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 47\n",
      "MEAN POLICY LOSS tensor(222.5743, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 48\n",
      "MEAN POLICY LOSS tensor(34.6576, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 49\n",
      "MEAN POLICY LOSS tensor(999.8771, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 50\n",
      "MEAN POLICY LOSS tensor(4.4027, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 51\n",
      "MEAN POLICY LOSS tensor(29325.4102, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 52\n",
      "MEAN POLICY LOSS tensor(384.4678, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 53\n",
      "MEAN POLICY LOSS tensor(376.5609, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 54\n",
      "MEAN POLICY LOSS tensor(954.2518, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 55\n",
      "MEAN POLICY LOSS tensor(457.7238, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 56\n",
      "MEAN POLICY LOSS tensor(83.2442, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 57\n",
      "MEAN POLICY LOSS tensor(24210.7656, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 58\n",
      "MEAN POLICY LOSS tensor(793.1212, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 59\n",
      "MEAN POLICY LOSS tensor(1181.2249, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 60\n",
      "MEAN POLICY LOSS tensor(334.7244, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 61\n",
      "MEAN POLICY LOSS tensor(2145.3193, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 62\n",
      "MEAN POLICY LOSS tensor(302.8472, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 63\n",
      "MEAN POLICY LOSS tensor(23.1174, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 64\n",
      "MEAN POLICY LOSS tensor(1906.5156, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 65\n",
      "MEAN POLICY LOSS tensor(1725.5106, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 66\n",
      "MEAN POLICY LOSS tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "REWARD -497.0\n",
      "EPSIODE# 67\n",
      "MEAN POLICY LOSS tensor(436.7214, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 68\n",
      "MEAN POLICY LOSS tensor(1762.6882, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 69\n",
      "MEAN POLICY LOSS tensor(1452.7750, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 70\n",
      "MEAN POLICY LOSS tensor(0.2209, grad_fn=<AddBackward0>)\n",
      "REWARD -357.0\n",
      "EPSIODE# 71\n",
      "MEAN POLICY LOSS tensor(512.0854, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 72\n",
      "MEAN POLICY LOSS tensor(1456.3724, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 73\n",
      "MEAN POLICY LOSS tensor(125.1321, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 74\n",
      "MEAN POLICY LOSS tensor(629.5968, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 75\n",
      "MEAN POLICY LOSS tensor(652.5753, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 76\n",
      "MEAN POLICY LOSS tensor(232.9971, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 77\n",
      "MEAN POLICY LOSS tensor(98.9026, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 78\n",
      "MEAN POLICY LOSS tensor(328.6378, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 79\n",
      "MEAN POLICY LOSS tensor(10.4804, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 80\n",
      "MEAN POLICY LOSS tensor(30839.0176, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 81\n",
      "MEAN POLICY LOSS tensor(720.0560, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 82\n",
      "MEAN POLICY LOSS tensor(11.0501, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 83\n",
      "MEAN POLICY LOSS tensor(4.4426, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 84\n",
      "MEAN POLICY LOSS tensor(911.5767, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 85\n",
      "MEAN POLICY LOSS tensor(410.4471, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 86\n",
      "MEAN POLICY LOSS tensor(1225.3610, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 87\n",
      "MEAN POLICY LOSS tensor(122.8710, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 88\n",
      "MEAN POLICY LOSS tensor(2276.8208, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 89\n",
      "MEAN POLICY LOSS tensor(1653.3339, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 90\n",
      "MEAN POLICY LOSS tensor(1768.1401, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 91\n",
      "MEAN POLICY LOSS tensor(1406.8770, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 92\n",
      "MEAN POLICY LOSS tensor(1568.2062, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 93\n",
      "MEAN POLICY LOSS tensor(19.7441, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 94\n",
      "MEAN POLICY LOSS tensor(1573.2350, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 95\n",
      "MEAN POLICY LOSS tensor(313.7831, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 96\n",
      "MEAN POLICY LOSS tensor(96.9550, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 97\n",
      "MEAN POLICY LOSS tensor(222.2875, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 98\n",
      "MEAN POLICY LOSS tensor(64.6463, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 99\n",
      "MEAN POLICY LOSS tensor(397.4342, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 100\n",
      "MEAN POLICY LOSS tensor(438.1798, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 101\n",
      "MEAN POLICY LOSS tensor(486.4257, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 102\n",
      "MEAN POLICY LOSS tensor(349.2598, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 103\n",
      "MEAN POLICY LOSS tensor(44.1048, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 104\n",
      "MEAN POLICY LOSS tensor(133.5342, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 105\n",
      "MEAN POLICY LOSS tensor(261.4084, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 106\n",
      "MEAN POLICY LOSS tensor(46.4708, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 107\n",
      "MEAN POLICY LOSS tensor(225.8916, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 108\n",
      "MEAN POLICY LOSS tensor(19.4196, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 109\n",
      "MEAN POLICY LOSS tensor(128.9391, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 110\n",
      "MEAN POLICY LOSS tensor(217.3654, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 111\n",
      "MEAN POLICY LOSS tensor(74.7638, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 112\n",
      "MEAN POLICY LOSS tensor(180.0190, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 113\n",
      "MEAN POLICY LOSS tensor(181.4707, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 114\n",
      "MEAN POLICY LOSS tensor(65.3968, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 115\n",
      "MEAN POLICY LOSS tensor(183.1964, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 116\n",
      "MEAN POLICY LOSS tensor(135.3720, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 117\n",
      "MEAN POLICY LOSS tensor(109.2770, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 118\n",
      "MEAN POLICY LOSS tensor(74.8156, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 119\n",
      "MEAN POLICY LOSS tensor(29.0329, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 120\n",
      "MEAN POLICY LOSS tensor(106.7992, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 121\n",
      "MEAN POLICY LOSS tensor(73.9752, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 122\n",
      "MEAN POLICY LOSS tensor(78.5491, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 123\n",
      "MEAN POLICY LOSS tensor(76.0833, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 124\n",
      "MEAN POLICY LOSS tensor(73.7549, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 125\n",
      "MEAN POLICY LOSS tensor(86.0318, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 126\n",
      "MEAN POLICY LOSS tensor(55.2937, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 127\n",
      "MEAN POLICY LOSS tensor(85.8230, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 128\n",
      "MEAN POLICY LOSS tensor(61.0745, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 129\n",
      "MEAN POLICY LOSS tensor(65.1796, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 130\n",
      "MEAN POLICY LOSS tensor(52.0295, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 131\n",
      "MEAN POLICY LOSS tensor(38.6917, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 132\n",
      "MEAN POLICY LOSS tensor(44.9809, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 133\n",
      "MEAN POLICY LOSS tensor(41.5819, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 134\n",
      "MEAN POLICY LOSS tensor(49.6029, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 135\n",
      "MEAN POLICY LOSS tensor(40.9966, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 136\n",
      "MEAN POLICY LOSS tensor(35.2398, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 137\n",
      "MEAN POLICY LOSS tensor(32.9485, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 138\n",
      "MEAN POLICY LOSS tensor(9.7145, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 139\n",
      "MEAN POLICY LOSS tensor(34.8294, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 140\n",
      "MEAN POLICY LOSS tensor(12.6500, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 141\n",
      "MEAN POLICY LOSS tensor(22.3302, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 142\n",
      "MEAN POLICY LOSS tensor(29.2678, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 143\n",
      "MEAN POLICY LOSS tensor(3.2658, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 144\n",
      "MEAN POLICY LOSS tensor(33.3042, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 145\n",
      "MEAN POLICY LOSS tensor(5.0542, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 146\n",
      "MEAN POLICY LOSS tensor(25.8575, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 147\n",
      "MEAN POLICY LOSS tensor(7.4090, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 148\n",
      "MEAN POLICY LOSS tensor(26.0222, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 149\n",
      "MEAN POLICY LOSS tensor(23.6873, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 150\n",
      "MEAN POLICY LOSS tensor(22.7087, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 151\n",
      "MEAN POLICY LOSS tensor(14.1413, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 152\n",
      "MEAN POLICY LOSS tensor(15.4109, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 153\n",
      "MEAN POLICY LOSS tensor(19.4758, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 154\n",
      "MEAN POLICY LOSS tensor(22.6728, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 155\n",
      "MEAN POLICY LOSS tensor(25.8181, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 156\n",
      "MEAN POLICY LOSS tensor(21.3987, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 157\n",
      "MEAN POLICY LOSS tensor(19.0157, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 158\n",
      "MEAN POLICY LOSS tensor(15.1103, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 159\n",
      "MEAN POLICY LOSS tensor(25.9497, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 160\n",
      "MEAN POLICY LOSS tensor(23.5708, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 161\n",
      "MEAN POLICY LOSS tensor(17.7050, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 162\n",
      "MEAN POLICY LOSS tensor(14.7372, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 163\n",
      "MEAN POLICY LOSS tensor(18.4119, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 164\n",
      "MEAN POLICY LOSS tensor(16.3829, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 165\n",
      "MEAN POLICY LOSS tensor(8.1533, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 166\n",
      "MEAN POLICY LOSS tensor(16.8852, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 167\n",
      "MEAN POLICY LOSS tensor(11.3888, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 168\n",
      "MEAN POLICY LOSS tensor(7.6625, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 169\n",
      "MEAN POLICY LOSS tensor(16.4154, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 170\n",
      "MEAN POLICY LOSS tensor(13.5094, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 171\n",
      "MEAN POLICY LOSS tensor(21.1993, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 172\n",
      "MEAN POLICY LOSS tensor(14.0295, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 173\n",
      "MEAN POLICY LOSS tensor(18.8702, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 174\n",
      "MEAN POLICY LOSS tensor(13.2465, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 175\n",
      "MEAN POLICY LOSS tensor(19.0029, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 176\n",
      "MEAN POLICY LOSS tensor(12.9332, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 177\n",
      "MEAN POLICY LOSS tensor(11.9123, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 178\n",
      "MEAN POLICY LOSS tensor(11.9638, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 179\n",
      "MEAN POLICY LOSS tensor(10.5244, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 180\n",
      "MEAN POLICY LOSS tensor(9.6566, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 181\n",
      "MEAN POLICY LOSS tensor(8.4618, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 182\n",
      "MEAN POLICY LOSS tensor(12.6124, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 183\n",
      "MEAN POLICY LOSS tensor(12.3639, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 184\n",
      "MEAN POLICY LOSS tensor(11.7814, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 185\n",
      "MEAN POLICY LOSS tensor(9.8337, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 186\n",
      "MEAN POLICY LOSS tensor(11.4913, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 187\n",
      "MEAN POLICY LOSS tensor(10.8538, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 188\n",
      "MEAN POLICY LOSS tensor(10.6170, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 189\n",
      "MEAN POLICY LOSS tensor(10.7959, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 190\n",
      "MEAN POLICY LOSS tensor(10.0103, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 191\n",
      "MEAN POLICY LOSS tensor(9.5333, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 192\n",
      "MEAN POLICY LOSS tensor(11.3344, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 193\n",
      "MEAN POLICY LOSS tensor(8.9463, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 194\n",
      "MEAN POLICY LOSS tensor(8.8988, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 195\n",
      "MEAN POLICY LOSS tensor(12.6344, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 196\n",
      "MEAN POLICY LOSS tensor(10.3756, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 197\n",
      "MEAN POLICY LOSS tensor(6.2474, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 198\n",
      "MEAN POLICY LOSS tensor(7.2625, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 199\n",
      "MEAN POLICY LOSS tensor(6.2740, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 200\n",
      "MEAN POLICY LOSS tensor(9.4928, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 201\n",
      "MEAN POLICY LOSS tensor(7.7140, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 202\n",
      "MEAN POLICY LOSS tensor(8.8965, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 203\n",
      "MEAN POLICY LOSS tensor(6.4968, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 204\n",
      "MEAN POLICY LOSS tensor(9.1205, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 205\n",
      "MEAN POLICY LOSS tensor(6.6855, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 206\n",
      "MEAN POLICY LOSS tensor(7.2243, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 207\n",
      "MEAN POLICY LOSS tensor(8.0977, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 208\n",
      "MEAN POLICY LOSS tensor(7.7116, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 209\n",
      "MEAN POLICY LOSS tensor(4.8106, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 210\n",
      "MEAN POLICY LOSS tensor(7.3939, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 211\n",
      "MEAN POLICY LOSS tensor(6.7017, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 212\n",
      "MEAN POLICY LOSS tensor(8.4213, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 213\n",
      "MEAN POLICY LOSS tensor(7.7545, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 214\n",
      "MEAN POLICY LOSS tensor(7.1549, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 215\n",
      "MEAN POLICY LOSS tensor(7.0426, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 216\n",
      "MEAN POLICY LOSS tensor(5.9448, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 217\n",
      "MEAN POLICY LOSS tensor(6.0449, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 218\n",
      "MEAN POLICY LOSS tensor(5.3667, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 219\n",
      "MEAN POLICY LOSS tensor(5.8568, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 220\n",
      "MEAN POLICY LOSS tensor(2.5236, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 221\n",
      "MEAN POLICY LOSS tensor(5.7850, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 222\n",
      "MEAN POLICY LOSS tensor(5.1665, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 223\n",
      "MEAN POLICY LOSS tensor(6.3134, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 224\n",
      "MEAN POLICY LOSS tensor(5.0247, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 225\n",
      "MEAN POLICY LOSS tensor(7.4228, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 226\n",
      "MEAN POLICY LOSS tensor(5.7816, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 227\n",
      "MEAN POLICY LOSS tensor(5.9216, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 228\n",
      "MEAN POLICY LOSS tensor(5.4882, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 229\n",
      "MEAN POLICY LOSS tensor(6.0061, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 230\n",
      "MEAN POLICY LOSS tensor(6.9558, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 231\n",
      "MEAN POLICY LOSS tensor(4.6706, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 232\n",
      "MEAN POLICY LOSS tensor(5.5113, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 233\n",
      "MEAN POLICY LOSS tensor(5.1294, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 234\n",
      "MEAN POLICY LOSS tensor(4.8858, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 235\n",
      "MEAN POLICY LOSS tensor(5.6473, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 236\n",
      "MEAN POLICY LOSS tensor(5.0658, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 237\n",
      "MEAN POLICY LOSS tensor(5.5895, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 238\n",
      "MEAN POLICY LOSS tensor(4.9159, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 239\n",
      "MEAN POLICY LOSS tensor(4.7539, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 240\n",
      "MEAN POLICY LOSS tensor(4.8106, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 241\n",
      "MEAN POLICY LOSS tensor(4.9159, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 242\n",
      "MEAN POLICY LOSS tensor(4.5155, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 243\n",
      "MEAN POLICY LOSS tensor(4.7909, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 244\n",
      "MEAN POLICY LOSS tensor(5.1757, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 245\n",
      "MEAN POLICY LOSS tensor(5.6242, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 246\n",
      "MEAN POLICY LOSS tensor(4.2927, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 247\n",
      "MEAN POLICY LOSS tensor(4.8997, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 248\n",
      "MEAN POLICY LOSS tensor(4.5155, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 249\n",
      "MEAN POLICY LOSS tensor(5.1803, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 250\n",
      "MEAN POLICY LOSS tensor(4.3101, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 251\n",
      "MEAN POLICY LOSS tensor(3.9711, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 252\n",
      "MEAN POLICY LOSS tensor(4.4438, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 253\n",
      "MEAN POLICY LOSS tensor(4.4634, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 254\n",
      "MEAN POLICY LOSS tensor(3.7720, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 255\n",
      "MEAN POLICY LOSS tensor(4.0173, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 256\n",
      "MEAN POLICY LOSS tensor(3.8681, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 257\n",
      "MEAN POLICY LOSS tensor(4.3703, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 258\n",
      "MEAN POLICY LOSS tensor(4.6879, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 259\n",
      "MEAN POLICY LOSS tensor(4.4692, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 260\n",
      "MEAN POLICY LOSS tensor(3.7136, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 261\n",
      "MEAN POLICY LOSS tensor(4.1111, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 262\n",
      "MEAN POLICY LOSS tensor(2.9823, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 263\n",
      "MEAN POLICY LOSS tensor(4.1979, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 264\n",
      "MEAN POLICY LOSS tensor(3.3190, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 265\n",
      "MEAN POLICY LOSS tensor(3.8739, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 266\n",
      "MEAN POLICY LOSS tensor(3.0413, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 267\n",
      "MEAN POLICY LOSS tensor(3.6997, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 268\n",
      "MEAN POLICY LOSS tensor(3.3676, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 269\n",
      "MEAN POLICY LOSS tensor(3.5134, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 270\n",
      "MEAN POLICY LOSS tensor(3.6465, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 271\n",
      "MEAN POLICY LOSS tensor(3.1709, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 272\n",
      "MEAN POLICY LOSS tensor(3.4671, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 273\n",
      "MEAN POLICY LOSS tensor(3.9317, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 274\n",
      "MEAN POLICY LOSS tensor(3.0980, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 275\n",
      "MEAN POLICY LOSS tensor(2.9395, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 276\n",
      "MEAN POLICY LOSS tensor(3.7582, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 277\n",
      "MEAN POLICY LOSS tensor(3.2762, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 278\n",
      "MEAN POLICY LOSS tensor(2.7666, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 279\n",
      "MEAN POLICY LOSS tensor(3.3954, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 280\n",
      "MEAN POLICY LOSS tensor(2.8568, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 281\n",
      "MEAN POLICY LOSS tensor(2.7816, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 282\n",
      "MEAN POLICY LOSS tensor(3.0633, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 283\n",
      "MEAN POLICY LOSS tensor(3.0332, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 284\n",
      "MEAN POLICY LOSS tensor(2.6323, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 285\n",
      "MEAN POLICY LOSS tensor(3.0367, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 286\n",
      "MEAN POLICY LOSS tensor(3.1061, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 287\n",
      "MEAN POLICY LOSS tensor(3.2045, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 288\n",
      "MEAN POLICY LOSS tensor(2.4762, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 289\n",
      "MEAN POLICY LOSS tensor(2.7446, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 290\n",
      "MEAN POLICY LOSS tensor(2.6624, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 291\n",
      "MEAN POLICY LOSS tensor(2.4981, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 292\n",
      "MEAN POLICY LOSS tensor(2.4461, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 293\n",
      "MEAN POLICY LOSS tensor(2.6925, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 294\n",
      "MEAN POLICY LOSS tensor(2.9534, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 295\n",
      "MEAN POLICY LOSS tensor(2.2910, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 296\n",
      "MEAN POLICY LOSS tensor(2.7330, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 297\n",
      "MEAN POLICY LOSS tensor(2.5305, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 298\n",
      "MEAN POLICY LOSS tensor(2.4947, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 299\n",
      "MEAN POLICY LOSS tensor(2.4530, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 300\n",
      "MEAN POLICY LOSS tensor(2.5999, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 301\n",
      "MEAN POLICY LOSS tensor(1.8098, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 302\n",
      "MEAN POLICY LOSS tensor(2.4877, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 303\n",
      "MEAN POLICY LOSS tensor(2.7851, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 304\n",
      "MEAN POLICY LOSS tensor(2.5236, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 305\n",
      "MEAN POLICY LOSS tensor(2.7249, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 306\n",
      "MEAN POLICY LOSS tensor(2.7423, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 307\n",
      "MEAN POLICY LOSS tensor(3.5979, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 308\n",
      "MEAN POLICY LOSS tensor(2.1302, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 309\n",
      "MEAN POLICY LOSS tensor(1.8757, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 310\n",
      "MEAN POLICY LOSS tensor(2.3593, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 311\n",
      "MEAN POLICY LOSS tensor(2.1036, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 312\n",
      "MEAN POLICY LOSS tensor(2.3072, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 313\n",
      "MEAN POLICY LOSS tensor(2.3026, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 314\n",
      "MEAN POLICY LOSS tensor(2.6578, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 315\n",
      "MEAN POLICY LOSS tensor(2.1314, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 316\n",
      "MEAN POLICY LOSS tensor(2.7573, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 317\n",
      "MEAN POLICY LOSS tensor(3.3804, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 318\n",
      "MEAN POLICY LOSS tensor(2.1464, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 319\n",
      "MEAN POLICY LOSS tensor(2.6624, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 320\n",
      "MEAN POLICY LOSS tensor(2.6566, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 321\n",
      "MEAN POLICY LOSS tensor(2.1279, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 322\n",
      "MEAN POLICY LOSS tensor(2.2008, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 323\n",
      "MEAN POLICY LOSS tensor(1.8931, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 324\n",
      "MEAN POLICY LOSS tensor(2.1082, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 325\n",
      "MEAN POLICY LOSS tensor(1.8861, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 326\n",
      "MEAN POLICY LOSS tensor(1.8213, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 327\n",
      "MEAN POLICY LOSS tensor(2.1314, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 328\n",
      "MEAN POLICY LOSS tensor(2.2957, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 329\n",
      "MEAN POLICY LOSS tensor(2.2101, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 330\n",
      "MEAN POLICY LOSS tensor(1.8780, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 331\n",
      "MEAN POLICY LOSS tensor(2.0215, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 332\n",
      "MEAN POLICY LOSS tensor(1.9821, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 333\n",
      "MEAN POLICY LOSS tensor(1.9474, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 334\n",
      "MEAN POLICY LOSS tensor(2.0284, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 335\n",
      "MEAN POLICY LOSS tensor(1.9532, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 336\n",
      "MEAN POLICY LOSS tensor(1.6125, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 337\n",
      "MEAN POLICY LOSS tensor(1.7172, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 338\n",
      "MEAN POLICY LOSS tensor(1.8526, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 339\n",
      "MEAN POLICY LOSS tensor(2.1707, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 340\n",
      "MEAN POLICY LOSS tensor(1.8352, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 341\n",
      "MEAN POLICY LOSS tensor(1.6999, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 342\n",
      "MEAN POLICY LOSS tensor(1.3788, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 343\n",
      "MEAN POLICY LOSS tensor(2.0585, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 344\n",
      "MEAN POLICY LOSS tensor(1.7126, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 345\n",
      "MEAN POLICY LOSS tensor(1.9197, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 346\n",
      "MEAN POLICY LOSS tensor(1.7172, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 347\n",
      "MEAN POLICY LOSS tensor(1.6530, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 348\n",
      "MEAN POLICY LOSS tensor(1.7022, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 349\n",
      "MEAN POLICY LOSS tensor(1.0931, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 350\n",
      "MEAN POLICY LOSS tensor(1.7774, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 351\n",
      "MEAN POLICY LOSS tensor(1.5512, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 352\n",
      "MEAN POLICY LOSS tensor(2.1638, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 353\n",
      "MEAN POLICY LOSS tensor(1.9544, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 354\n",
      "MEAN POLICY LOSS tensor(1.7033, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 355\n",
      "MEAN POLICY LOSS tensor(1.9289, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 356\n",
      "MEAN POLICY LOSS tensor(1.7913, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 357\n",
      "MEAN POLICY LOSS tensor(1.6137, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 358\n",
      "MEAN POLICY LOSS tensor(1.6472, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 359\n",
      "MEAN POLICY LOSS tensor(1.7508, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 360\n",
      "MEAN POLICY LOSS tensor(1.7068, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 361\n",
      "MEAN POLICY LOSS tensor(0.9207, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 362\n",
      "MEAN POLICY LOSS tensor(1.8248, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 363\n",
      "MEAN POLICY LOSS tensor(1.5882, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 364\n",
      "MEAN POLICY LOSS tensor(1.7832, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 365\n",
      "MEAN POLICY LOSS tensor(1.3407, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 366\n",
      "MEAN POLICY LOSS tensor(1.4876, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 367\n",
      "MEAN POLICY LOSS tensor(1.5142, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 368\n",
      "MEAN POLICY LOSS tensor(1.7577, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 369\n",
      "MEAN POLICY LOSS tensor(1.5096, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 370\n",
      "MEAN POLICY LOSS tensor(1.5142, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 371\n",
      "MEAN POLICY LOSS tensor(1.5281, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 372\n",
      "MEAN POLICY LOSS tensor(1.4066, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 373\n",
      "MEAN POLICY LOSS tensor(1.5431, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 374\n",
      "MEAN POLICY LOSS tensor(1.3707, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 375\n",
      "MEAN POLICY LOSS tensor(1.4853, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 376\n",
      "MEAN POLICY LOSS tensor(1.4853, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 377\n",
      "MEAN POLICY LOSS tensor(1.2238, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 378\n",
      "MEAN POLICY LOSS tensor(1.5489, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 379\n",
      "MEAN POLICY LOSS tensor(1.3476, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 380\n",
      "MEAN POLICY LOSS tensor(1.6264, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 381\n",
      "MEAN POLICY LOSS tensor(1.4656, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 382\n",
      "MEAN POLICY LOSS tensor(1.5165, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 383\n",
      "MEAN POLICY LOSS tensor(1.5431, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 384\n",
      "MEAN POLICY LOSS tensor(1.5315, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 385\n",
      "MEAN POLICY LOSS tensor(1.4193, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 386\n",
      "MEAN POLICY LOSS tensor(1.2493, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 387\n",
      "MEAN POLICY LOSS tensor(1.6987, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 388\n",
      "MEAN POLICY LOSS tensor(1.3835, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 389\n",
      "MEAN POLICY LOSS tensor(1.3141, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 390\n",
      "MEAN POLICY LOSS tensor(1.4610, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 391\n",
      "MEAN POLICY LOSS tensor(1.3974, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 392\n",
      "MEAN POLICY LOSS tensor(0.9173, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 393\n",
      "MEAN POLICY LOSS tensor(1.1833, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 394\n",
      "MEAN POLICY LOSS tensor(1.5778, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 395\n",
      "MEAN POLICY LOSS tensor(1.1579, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 396\n",
      "MEAN POLICY LOSS tensor(1.4367, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 397\n",
      "MEAN POLICY LOSS tensor(1.1278, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 398\n",
      "MEAN POLICY LOSS tensor(1.4841, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 399\n",
      "MEAN POLICY LOSS tensor(1.1671, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 400\n",
      "MEAN POLICY LOSS tensor(1.3626, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 401\n",
      "MEAN POLICY LOSS tensor(1.2342, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 402\n",
      "MEAN POLICY LOSS tensor(1.1162, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 403\n",
      "MEAN POLICY LOSS tensor(1.4205, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 404\n",
      "MEAN POLICY LOSS tensor(1.3418, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 405\n",
      "MEAN POLICY LOSS tensor(1.0769, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 406\n",
      "MEAN POLICY LOSS tensor(1.3476, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 407\n",
      "MEAN POLICY LOSS tensor(1.1498, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 408\n",
      "MEAN POLICY LOSS tensor(1.0850, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 409\n",
      "MEAN POLICY LOSS tensor(1.2157, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 410\n",
      "MEAN POLICY LOSS tensor(1.0098, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 411\n",
      "MEAN POLICY LOSS tensor(1.5049, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 412\n",
      "MEAN POLICY LOSS tensor(1.3800, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 413\n",
      "MEAN POLICY LOSS tensor(1.1417, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 414\n",
      "MEAN POLICY LOSS tensor(1.1614, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 415\n",
      "MEAN POLICY LOSS tensor(1.0399, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 416\n",
      "MEAN POLICY LOSS tensor(1.2366, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 417\n",
      "MEAN POLICY LOSS tensor(1.0619, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 418\n",
      "MEAN POLICY LOSS tensor(1.1255, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 419\n",
      "MEAN POLICY LOSS tensor(0.9948, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 420\n",
      "MEAN POLICY LOSS tensor(1.2713, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 421\n",
      "MEAN POLICY LOSS tensor(1.1498, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 422\n",
      "MEAN POLICY LOSS tensor(1.3372, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 423\n",
      "MEAN POLICY LOSS tensor(1.2250, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 424\n",
      "MEAN POLICY LOSS tensor(0.9948, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 425\n",
      "MEAN POLICY LOSS tensor(0.9138, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 426\n",
      "MEAN POLICY LOSS tensor(1.4587, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 427\n",
      "MEAN POLICY LOSS tensor(1.0966, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 428\n",
      "MEAN POLICY LOSS tensor(0.9647, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 429\n",
      "MEAN POLICY LOSS tensor(1.0839, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 430\n",
      "MEAN POLICY LOSS tensor(0.9983, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 431\n",
      "MEAN POLICY LOSS tensor(0.8606, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 432\n",
      "MEAN POLICY LOSS tensor(1.0457, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 433\n",
      "MEAN POLICY LOSS tensor(0.9520, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 434\n",
      "MEAN POLICY LOSS tensor(0.9103, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 435\n",
      "MEAN POLICY LOSS tensor(0.8328, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 436\n",
      "MEAN POLICY LOSS tensor(0.8028, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 437\n",
      "MEAN POLICY LOSS tensor(0.9381, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 438\n",
      "MEAN POLICY LOSS tensor(0.9393, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 439\n",
      "MEAN POLICY LOSS tensor(1.0873, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 440\n",
      "MEAN POLICY LOSS tensor(1.0908, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 441\n",
      "MEAN POLICY LOSS tensor(1.0538, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 442\n",
      "MEAN POLICY LOSS tensor(1.3314, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 443\n",
      "MEAN POLICY LOSS tensor(0.7044, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 444\n",
      "MEAN POLICY LOSS tensor(1.0330, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 445\n",
      "MEAN POLICY LOSS tensor(0.9520, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 446\n",
      "MEAN POLICY LOSS tensor(0.9751, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 447\n",
      "MEAN POLICY LOSS tensor(0.7912, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 448\n",
      "MEAN POLICY LOSS tensor(0.8421, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 449\n",
      "MEAN POLICY LOSS tensor(0.9543, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 450\n",
      "MEAN POLICY LOSS tensor(0.8398, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 451\n",
      "MEAN POLICY LOSS tensor(0.8409, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 452\n",
      "MEAN POLICY LOSS tensor(0.9578, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 453\n",
      "MEAN POLICY LOSS tensor(0.9751, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 454\n",
      "MEAN POLICY LOSS tensor(0.9057, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 455\n",
      "MEAN POLICY LOSS tensor(0.7669, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 456\n",
      "MEAN POLICY LOSS tensor(0.9670, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 457\n",
      "MEAN POLICY LOSS tensor(0.8236, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 458\n",
      "MEAN POLICY LOSS tensor(0.8062, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 459\n",
      "MEAN POLICY LOSS tensor(0.7831, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 460\n",
      "MEAN POLICY LOSS tensor(0.9312, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 461\n",
      "MEAN POLICY LOSS tensor(0.8351, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 462\n",
      "MEAN POLICY LOSS tensor(0.9080, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 463\n",
      "MEAN POLICY LOSS tensor(0.7808, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 464\n",
      "MEAN POLICY LOSS tensor(0.6848, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 465\n",
      "MEAN POLICY LOSS tensor(0.7634, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 466\n",
      "MEAN POLICY LOSS tensor(0.7738, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 467\n",
      "MEAN POLICY LOSS tensor(0.7831, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 468\n",
      "MEAN POLICY LOSS tensor(0.9300, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 469\n",
      "MEAN POLICY LOSS tensor(0.8282, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 470\n",
      "MEAN POLICY LOSS tensor(0.8456, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 471\n",
      "MEAN POLICY LOSS tensor(0.7391, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 472\n",
      "MEAN POLICY LOSS tensor(0.7299, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 473\n",
      "MEAN POLICY LOSS tensor(0.8178, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 474\n",
      "MEAN POLICY LOSS tensor(1.1486, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 475\n",
      "MEAN POLICY LOSS tensor(0.9115, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 476\n",
      "MEAN POLICY LOSS tensor(0.7970, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 477\n",
      "MEAN POLICY LOSS tensor(0.7553, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 478\n",
      "MEAN POLICY LOSS tensor(0.6512, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 479\n",
      "MEAN POLICY LOSS tensor(0.8444, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 480\n",
      "MEAN POLICY LOSS tensor(0.7287, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 481\n",
      "MEAN POLICY LOSS tensor(0.6917, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 482\n",
      "MEAN POLICY LOSS tensor(0.6963, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 483\n",
      "MEAN POLICY LOSS tensor(0.7137, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 484\n",
      "MEAN POLICY LOSS tensor(0.7530, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 485\n",
      "MEAN POLICY LOSS tensor(0.7958, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 486\n",
      "MEAN POLICY LOSS tensor(0.8051, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 487\n",
      "MEAN POLICY LOSS tensor(0.8594, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 488\n",
      "MEAN POLICY LOSS tensor(0.7519, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 489\n",
      "MEAN POLICY LOSS tensor(0.6894, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 490\n",
      "MEAN POLICY LOSS tensor(0.7831, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 491\n",
      "MEAN POLICY LOSS tensor(0.6050, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 492\n",
      "MEAN POLICY LOSS tensor(0.9809, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 493\n",
      "MEAN POLICY LOSS tensor(0.7727, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 494\n",
      "MEAN POLICY LOSS tensor(0.7079, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 495\n",
      "MEAN POLICY LOSS tensor(0.7472, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 496\n",
      "MEAN POLICY LOSS tensor(0.6825, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 497\n",
      "MEAN POLICY LOSS tensor(0.7010, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 498\n",
      "MEAN POLICY LOSS tensor(0.6559, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 499\n",
      "MEAN POLICY LOSS tensor(0.6778, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 500\n",
      "MEAN POLICY LOSS tensor(0.6292, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 501\n",
      "MEAN POLICY LOSS tensor(0.5726, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 502\n",
      "MEAN POLICY LOSS tensor(0.9138, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 503\n",
      "MEAN POLICY LOSS tensor(0.6177, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 504\n",
      "MEAN POLICY LOSS tensor(0.5992, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 505\n",
      "MEAN POLICY LOSS tensor(0.6038, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 506\n",
      "MEAN POLICY LOSS tensor(0.6605, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 507\n",
      "MEAN POLICY LOSS tensor(0.8028, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 508\n",
      "MEAN POLICY LOSS tensor(0.6431, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 509\n",
      "MEAN POLICY LOSS tensor(0.5934, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 510\n",
      "MEAN POLICY LOSS tensor(0.5980, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 511\n",
      "MEAN POLICY LOSS tensor(0.6084, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 512\n",
      "MEAN POLICY LOSS tensor(0.7010, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 513\n",
      "MEAN POLICY LOSS tensor(0.6050, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 514\n",
      "MEAN POLICY LOSS tensor(0.5876, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 515\n",
      "MEAN POLICY LOSS tensor(0.5830, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 516\n",
      "MEAN POLICY LOSS tensor(0.6639, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 517\n",
      "MEAN POLICY LOSS tensor(0.6894, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 518\n",
      "MEAN POLICY LOSS tensor(0.6744, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 519\n",
      "MEAN POLICY LOSS tensor(0.5853, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 520\n",
      "MEAN POLICY LOSS tensor(0.5807, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 521\n",
      "MEAN POLICY LOSS tensor(0.5587, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 522\n",
      "MEAN POLICY LOSS tensor(0.6362, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 523\n",
      "MEAN POLICY LOSS tensor(0.5691, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 524\n",
      "MEAN POLICY LOSS tensor(0.5344, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 525\n",
      "MEAN POLICY LOSS tensor(0.5668, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 526\n",
      "MEAN POLICY LOSS tensor(0.7461, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 527\n",
      "MEAN POLICY LOSS tensor(0.5448, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 528\n",
      "MEAN POLICY LOSS tensor(0.5483, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 529\n",
      "MEAN POLICY LOSS tensor(0.6512, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 530\n",
      "MEAN POLICY LOSS tensor(0.5564, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 531\n",
      "MEAN POLICY LOSS tensor(0.5945, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 532\n",
      "MEAN POLICY LOSS tensor(0.5309, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 533\n",
      "MEAN POLICY LOSS tensor(0.6813, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 534\n",
      "MEAN POLICY LOSS tensor(0.5460, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 535\n",
      "MEAN POLICY LOSS tensor(0.5101, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 536\n",
      "MEAN POLICY LOSS tensor(0.5124, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 537\n",
      "MEAN POLICY LOSS tensor(0.4638, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 538\n",
      "MEAN POLICY LOSS tensor(0.5899, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 539\n",
      "MEAN POLICY LOSS tensor(0.5321, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 540\n",
      "MEAN POLICY LOSS tensor(0.5194, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 541\n",
      "MEAN POLICY LOSS tensor(0.6142, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 542\n",
      "MEAN POLICY LOSS tensor(0.7403, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 543\n",
      "MEAN POLICY LOSS tensor(0.5136, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 544\n",
      "MEAN POLICY LOSS tensor(0.5506, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 545\n",
      "MEAN POLICY LOSS tensor(0.5182, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 546\n",
      "MEAN POLICY LOSS tensor(0.5425, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 547\n",
      "MEAN POLICY LOSS tensor(0.5251, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 548\n",
      "MEAN POLICY LOSS tensor(0.6061, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 549\n",
      "MEAN POLICY LOSS tensor(0.5425, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 550\n",
      "MEAN POLICY LOSS tensor(0.5772, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 551\n",
      "MEAN POLICY LOSS tensor(0.4870, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 552\n",
      "MEAN POLICY LOSS tensor(0.6755, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 553\n",
      "MEAN POLICY LOSS tensor(0.4661, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 554\n",
      "MEAN POLICY LOSS tensor(0.4488, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 555\n",
      "MEAN POLICY LOSS tensor(0.5899, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 556\n",
      "MEAN POLICY LOSS tensor(0.6154, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 557\n",
      "MEAN POLICY LOSS tensor(0.5356, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 558\n",
      "MEAN POLICY LOSS tensor(0.5078, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 559\n",
      "MEAN POLICY LOSS tensor(0.6732, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 560\n",
      "MEAN POLICY LOSS tensor(0.7299, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 561\n",
      "MEAN POLICY LOSS tensor(0.7114, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 562\n",
      "MEAN POLICY LOSS tensor(0.4893, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 563\n",
      "MEAN POLICY LOSS tensor(0.6026, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 564\n",
      "MEAN POLICY LOSS tensor(0.5032, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 565\n",
      "MEAN POLICY LOSS tensor(0.6397, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 566\n",
      "MEAN POLICY LOSS tensor(0.4962, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 567\n",
      "MEAN POLICY LOSS tensor(0.4557, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 568\n",
      "MEAN POLICY LOSS tensor(0.6720, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 569\n",
      "MEAN POLICY LOSS tensor(0.5645, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 570\n",
      "MEAN POLICY LOSS tensor(0.4083, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 571\n",
      "MEAN POLICY LOSS tensor(0.5992, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 572\n",
      "MEAN POLICY LOSS tensor(0.5286, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 573\n",
      "MEAN POLICY LOSS tensor(0.4361, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 574\n",
      "MEAN POLICY LOSS tensor(0.5159, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 575\n",
      "MEAN POLICY LOSS tensor(0.5344, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 576\n",
      "MEAN POLICY LOSS tensor(0.4627, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 577\n",
      "MEAN POLICY LOSS tensor(0.4002, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 578\n",
      "MEAN POLICY LOSS tensor(0.6385, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 579\n",
      "MEAN POLICY LOSS tensor(0.4303, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 580\n",
      "MEAN POLICY LOSS tensor(0.6188, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 581\n",
      "MEAN POLICY LOSS tensor(0.4233, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 582\n",
      "MEAN POLICY LOSS tensor(0.6431, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 583\n",
      "MEAN POLICY LOSS tensor(0.4419, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 584\n",
      "MEAN POLICY LOSS tensor(0.5032, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 585\n",
      "MEAN POLICY LOSS tensor(0.5101, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 586\n",
      "MEAN POLICY LOSS tensor(0.6131, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 587\n",
      "MEAN POLICY LOSS tensor(0.4847, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 588\n",
      "MEAN POLICY LOSS tensor(0.3991, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 589\n",
      "MEAN POLICY LOSS tensor(0.5066, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 590\n",
      "MEAN POLICY LOSS tensor(0.4708, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 591\n",
      "MEAN POLICY LOSS tensor(0.4395, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 592\n",
      "MEAN POLICY LOSS tensor(0.3782, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 593\n",
      "MEAN POLICY LOSS tensor(0.4245, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 594\n",
      "MEAN POLICY LOSS tensor(0.4523, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 595\n",
      "MEAN POLICY LOSS tensor(0.4430, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 596\n",
      "MEAN POLICY LOSS tensor(0.4164, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 597\n",
      "MEAN POLICY LOSS tensor(0.5934, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 598\n",
      "MEAN POLICY LOSS tensor(0.4951, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 599\n",
      "MEAN POLICY LOSS tensor(0.5309, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 600\n",
      "MEAN POLICY LOSS tensor(0.4407, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 601\n",
      "MEAN POLICY LOSS tensor(0.3736, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 602\n",
      "MEAN POLICY LOSS tensor(0.4060, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 603\n",
      "MEAN POLICY LOSS tensor(0.4823, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 604\n",
      "MEAN POLICY LOSS tensor(0.4361, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 605\n",
      "MEAN POLICY LOSS tensor(0.4650, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 606\n",
      "MEAN POLICY LOSS tensor(0.3898, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 607\n",
      "MEAN POLICY LOSS tensor(0.3840, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 608\n",
      "MEAN POLICY LOSS tensor(0.4696, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 609\n",
      "MEAN POLICY LOSS tensor(0.3748, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 610\n",
      "MEAN POLICY LOSS tensor(0.3736, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 611\n",
      "MEAN POLICY LOSS tensor(0.3817, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 612\n",
      "MEAN POLICY LOSS tensor(0.4546, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 613\n",
      "MEAN POLICY LOSS tensor(0.5078, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 614\n",
      "MEAN POLICY LOSS tensor(0.3910, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 615\n",
      "MEAN POLICY LOSS tensor(0.3979, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 616\n",
      "MEAN POLICY LOSS tensor(0.4465, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 617\n",
      "MEAN POLICY LOSS tensor(0.3690, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 618\n",
      "MEAN POLICY LOSS tensor(0.3620, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 619\n",
      "MEAN POLICY LOSS tensor(0.3586, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 620\n",
      "MEAN POLICY LOSS tensor(0.3748, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 621\n",
      "MEAN POLICY LOSS tensor(0.4072, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 622\n",
      "MEAN POLICY LOSS tensor(0.4095, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 623\n",
      "MEAN POLICY LOSS tensor(0.3840, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 624\n",
      "MEAN POLICY LOSS tensor(0.4372, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 625\n",
      "MEAN POLICY LOSS tensor(0.2741, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 626\n",
      "MEAN POLICY LOSS tensor(0.3505, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 627\n",
      "MEAN POLICY LOSS tensor(0.3146, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 628\n",
      "MEAN POLICY LOSS tensor(0.3308, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 629\n",
      "MEAN POLICY LOSS tensor(0.5078, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 630\n",
      "MEAN POLICY LOSS tensor(0.5645, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 631\n",
      "MEAN POLICY LOSS tensor(0.3389, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 632\n",
      "MEAN POLICY LOSS tensor(0.2533, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 633\n",
      "MEAN POLICY LOSS tensor(0.4222, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 634\n",
      "MEAN POLICY LOSS tensor(0.5448, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 635\n",
      "MEAN POLICY LOSS tensor(0.3412, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 636\n",
      "MEAN POLICY LOSS tensor(0.3609, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 637\n",
      "MEAN POLICY LOSS tensor(0.3389, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 638\n",
      "MEAN POLICY LOSS tensor(0.3863, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 639\n",
      "MEAN POLICY LOSS tensor(0.5691, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 640\n",
      "MEAN POLICY LOSS tensor(0.5136, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 641\n",
      "MEAN POLICY LOSS tensor(0.4789, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 642\n",
      "MEAN POLICY LOSS tensor(0.4812, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 643\n",
      "MEAN POLICY LOSS tensor(0.4338, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 644\n",
      "MEAN POLICY LOSS tensor(0.3458, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 645\n",
      "MEAN POLICY LOSS tensor(0.3991, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 646\n",
      "MEAN POLICY LOSS tensor(0.4569, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 647\n",
      "MEAN POLICY LOSS tensor(0.3516, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 648\n",
      "MEAN POLICY LOSS tensor(0.4199, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 649\n",
      "MEAN POLICY LOSS tensor(0.3401, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 650\n",
      "MEAN POLICY LOSS tensor(0.3736, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 651\n",
      "MEAN POLICY LOSS tensor(0.2950, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 652\n",
      "MEAN POLICY LOSS tensor(0.5078, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 653\n",
      "MEAN POLICY LOSS tensor(0.3782, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 654\n",
      "MEAN POLICY LOSS tensor(0.3366, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 655\n",
      "MEAN POLICY LOSS tensor(0.2903, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 656\n",
      "MEAN POLICY LOSS tensor(0.4407, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 657\n",
      "MEAN POLICY LOSS tensor(0.3227, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 658\n",
      "MEAN POLICY LOSS tensor(0.3470, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 659\n",
      "MEAN POLICY LOSS tensor(0.2880, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 660\n",
      "MEAN POLICY LOSS tensor(0.4048, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 661\n",
      "MEAN POLICY LOSS tensor(0.3701, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 662\n",
      "MEAN POLICY LOSS tensor(0.4314, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 663\n",
      "MEAN POLICY LOSS tensor(0.3782, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 664\n",
      "MEAN POLICY LOSS tensor(0.2915, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 665\n",
      "MEAN POLICY LOSS tensor(0.3667, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 666\n",
      "MEAN POLICY LOSS tensor(0.2984, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 667\n",
      "MEAN POLICY LOSS tensor(0.3528, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 668\n",
      "MEAN POLICY LOSS tensor(0.3748, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 669\n",
      "MEAN POLICY LOSS tensor(0.4962, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 670\n",
      "MEAN POLICY LOSS tensor(0.3308, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 671\n",
      "MEAN POLICY LOSS tensor(0.3863, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 672\n",
      "MEAN POLICY LOSS tensor(0.4141, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 673\n",
      "MEAN POLICY LOSS tensor(0.2903, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 674\n",
      "MEAN POLICY LOSS tensor(0.3378, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 675\n",
      "MEAN POLICY LOSS tensor(0.4245, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 676\n",
      "MEAN POLICY LOSS tensor(0.3135, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 677\n",
      "MEAN POLICY LOSS tensor(0.3146, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 678\n",
      "MEAN POLICY LOSS tensor(0.4465, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 679\n",
      "MEAN POLICY LOSS tensor(0.3192, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 680\n",
      "MEAN POLICY LOSS tensor(0.2718, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 681\n",
      "MEAN POLICY LOSS tensor(0.3111, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 682\n",
      "MEAN POLICY LOSS tensor(0.4904, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 683\n",
      "MEAN POLICY LOSS tensor(0.2764, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 684\n",
      "MEAN POLICY LOSS tensor(0.3135, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 685\n",
      "MEAN POLICY LOSS tensor(0.2903, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 686\n",
      "MEAN POLICY LOSS tensor(0.3227, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 687\n",
      "MEAN POLICY LOSS tensor(0.3204, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 688\n",
      "MEAN POLICY LOSS tensor(0.2475, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 689\n",
      "MEAN POLICY LOSS tensor(0.2637, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 690\n",
      "MEAN POLICY LOSS tensor(0.4257, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 691\n",
      "MEAN POLICY LOSS tensor(0.2834, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 692\n",
      "MEAN POLICY LOSS tensor(0.3435, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 693\n",
      "MEAN POLICY LOSS tensor(0.2695, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 694\n",
      "MEAN POLICY LOSS tensor(0.3123, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 695\n",
      "MEAN POLICY LOSS tensor(0.3678, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 696\n",
      "MEAN POLICY LOSS tensor(0.2649, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 697\n",
      "MEAN POLICY LOSS tensor(0.2961, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 698\n",
      "MEAN POLICY LOSS tensor(0.2672, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 699\n",
      "MEAN POLICY LOSS tensor(0.2730, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 700\n",
      "MEAN POLICY LOSS tensor(0.3320, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 701\n",
      "MEAN POLICY LOSS tensor(0.2776, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 702\n",
      "MEAN POLICY LOSS tensor(0.4615, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 703\n",
      "MEAN POLICY LOSS tensor(0.2568, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 704\n",
      "MEAN POLICY LOSS tensor(0.3088, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 705\n",
      "MEAN POLICY LOSS tensor(0.3794, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 706\n",
      "MEAN POLICY LOSS tensor(0.2892, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 707\n",
      "MEAN POLICY LOSS tensor(0.3771, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 708\n",
      "MEAN POLICY LOSS tensor(0.2718, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 709\n",
      "MEAN POLICY LOSS tensor(0.2892, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 710\n",
      "MEAN POLICY LOSS tensor(0.2487, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 711\n",
      "MEAN POLICY LOSS tensor(0.3620, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 712\n",
      "MEAN POLICY LOSS tensor(0.2996, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 713\n",
      "MEAN POLICY LOSS tensor(0.4060, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 714\n",
      "MEAN POLICY LOSS tensor(0.2510, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 715\n",
      "MEAN POLICY LOSS tensor(0.2788, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 716\n",
      "MEAN POLICY LOSS tensor(0.2290, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 717\n",
      "MEAN POLICY LOSS tensor(0.2371, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 718\n",
      "MEAN POLICY LOSS tensor(0.3262, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 719\n",
      "MEAN POLICY LOSS tensor(0.3331, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 720\n",
      "MEAN POLICY LOSS tensor(0.4083, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 721\n",
      "MEAN POLICY LOSS tensor(0.2336, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 722\n",
      "MEAN POLICY LOSS tensor(0.2695, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 723\n",
      "MEAN POLICY LOSS tensor(0.2811, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 724\n",
      "MEAN POLICY LOSS tensor(0.4187, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 725\n",
      "MEAN POLICY LOSS tensor(0.2649, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 726\n",
      "MEAN POLICY LOSS tensor(0.2591, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 727\n",
      "MEAN POLICY LOSS tensor(0.2649, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 728\n",
      "MEAN POLICY LOSS tensor(0.2545, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 729\n",
      "MEAN POLICY LOSS tensor(0.3435, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 730\n",
      "MEAN POLICY LOSS tensor(0.3019, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 731\n",
      "MEAN POLICY LOSS tensor(0.3204, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 732\n",
      "MEAN POLICY LOSS tensor(0.3297, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 733\n",
      "MEAN POLICY LOSS tensor(0.3088, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 734\n",
      "MEAN POLICY LOSS tensor(0.2996, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 735\n",
      "MEAN POLICY LOSS tensor(0.2417, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 736\n",
      "MEAN POLICY LOSS tensor(0.1804, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 737\n",
      "MEAN POLICY LOSS tensor(0.2244, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 738\n",
      "MEAN POLICY LOSS tensor(0.2892, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 739\n",
      "MEAN POLICY LOSS tensor(0.4523, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 740\n",
      "MEAN POLICY LOSS tensor(0.2996, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 741\n",
      "MEAN POLICY LOSS tensor(0.2313, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 742\n",
      "MEAN POLICY LOSS tensor(0.2429, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 743\n",
      "MEAN POLICY LOSS tensor(0.2406, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 744\n",
      "MEAN POLICY LOSS tensor(0.2313, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 745\n",
      "MEAN POLICY LOSS tensor(0.2498, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 746\n",
      "MEAN POLICY LOSS tensor(0.3863, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 747\n",
      "MEAN POLICY LOSS tensor(0.3875, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 748\n",
      "MEAN POLICY LOSS tensor(0.4199, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 749\n",
      "MEAN POLICY LOSS tensor(0.2660, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 750\n",
      "MEAN POLICY LOSS tensor(0.3401, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 751\n",
      "MEAN POLICY LOSS tensor(0.2070, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 752\n",
      "MEAN POLICY LOSS tensor(0.2232, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 753\n",
      "MEAN POLICY LOSS tensor(0.2915, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 754\n",
      "MEAN POLICY LOSS tensor(0.2464, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 755\n",
      "MEAN POLICY LOSS tensor(0.3100, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 756\n",
      "MEAN POLICY LOSS tensor(0.2360, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 757\n",
      "MEAN POLICY LOSS tensor(0.2938, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 758\n",
      "MEAN POLICY LOSS tensor(0.2036, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 759\n",
      "MEAN POLICY LOSS tensor(0.2175, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 760\n",
      "MEAN POLICY LOSS tensor(0.2094, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 761\n",
      "MEAN POLICY LOSS tensor(0.3146, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 762\n",
      "MEAN POLICY LOSS tensor(0.2070, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 763\n",
      "MEAN POLICY LOSS tensor(0.2186, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 764\n",
      "MEAN POLICY LOSS tensor(0.1966, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 765\n",
      "MEAN POLICY LOSS tensor(0.2302, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 766\n",
      "MEAN POLICY LOSS tensor(0.2360, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 767\n",
      "MEAN POLICY LOSS tensor(0.2082, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 768\n",
      "MEAN POLICY LOSS tensor(0.2256, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 769\n",
      "MEAN POLICY LOSS tensor(0.2024, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 770\n",
      "MEAN POLICY LOSS tensor(0.3111, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 771\n",
      "MEAN POLICY LOSS tensor(0.1862, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 772\n",
      "MEAN POLICY LOSS tensor(0.4233, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 773\n",
      "MEAN POLICY LOSS tensor(0.2579, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 774\n",
      "MEAN POLICY LOSS tensor(0.2313, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 775\n",
      "MEAN POLICY LOSS tensor(0.2082, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 776\n",
      "MEAN POLICY LOSS tensor(0.2649, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 777\n",
      "MEAN POLICY LOSS tensor(0.2568, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 778\n",
      "MEAN POLICY LOSS tensor(0.1978, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 779\n",
      "MEAN POLICY LOSS tensor(0.2105, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 780\n",
      "MEAN POLICY LOSS tensor(0.2163, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 781\n",
      "MEAN POLICY LOSS tensor(0.2117, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 782\n",
      "MEAN POLICY LOSS tensor(0.3262, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 783\n",
      "MEAN POLICY LOSS tensor(0.3031, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 784\n",
      "MEAN POLICY LOSS tensor(0.2070, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 785\n",
      "MEAN POLICY LOSS tensor(0.1989, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 786\n",
      "MEAN POLICY LOSS tensor(0.2406, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 787\n",
      "MEAN POLICY LOSS tensor(0.2175, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 788\n",
      "MEAN POLICY LOSS tensor(0.2556, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 789\n",
      "MEAN POLICY LOSS tensor(0.2244, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 790\n",
      "MEAN POLICY LOSS tensor(0.1816, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 791\n",
      "MEAN POLICY LOSS tensor(0.2024, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 792\n",
      "MEAN POLICY LOSS tensor(0.1885, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 793\n",
      "MEAN POLICY LOSS tensor(0.2996, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 794\n",
      "MEAN POLICY LOSS tensor(0.2857, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 795\n",
      "MEAN POLICY LOSS tensor(0.1897, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 796\n",
      "MEAN POLICY LOSS tensor(0.1862, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 797\n",
      "MEAN POLICY LOSS tensor(0.2001, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 798\n",
      "MEAN POLICY LOSS tensor(0.3031, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 799\n",
      "MEAN POLICY LOSS tensor(0.3366, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 800\n",
      "MEAN POLICY LOSS tensor(0.1955, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 801\n",
      "MEAN POLICY LOSS tensor(0.2313, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 802\n",
      "MEAN POLICY LOSS tensor(0.1862, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 803\n",
      "MEAN POLICY LOSS tensor(0.2545, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 804\n",
      "MEAN POLICY LOSS tensor(0.2302, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 805\n",
      "MEAN POLICY LOSS tensor(0.2024, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 806\n",
      "MEAN POLICY LOSS tensor(0.2475, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 807\n",
      "MEAN POLICY LOSS tensor(0.2001, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 808\n",
      "MEAN POLICY LOSS tensor(0.2221, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 809\n",
      "MEAN POLICY LOSS tensor(0.1816, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 810\n",
      "MEAN POLICY LOSS tensor(0.1723, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 811\n",
      "MEAN POLICY LOSS tensor(0.3088, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 812\n",
      "MEAN POLICY LOSS tensor(0.1804, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 813\n",
      "MEAN POLICY LOSS tensor(0.1793, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 814\n",
      "MEAN POLICY LOSS tensor(0.2267, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 815\n",
      "MEAN POLICY LOSS tensor(0.2452, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 816\n",
      "MEAN POLICY LOSS tensor(0.3597, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 817\n",
      "MEAN POLICY LOSS tensor(0.2013, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 818\n",
      "MEAN POLICY LOSS tensor(0.2082, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 819\n",
      "MEAN POLICY LOSS tensor(0.2475, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 820\n",
      "MEAN POLICY LOSS tensor(0.2221, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 821\n",
      "MEAN POLICY LOSS tensor(0.2591, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 822\n",
      "MEAN POLICY LOSS tensor(0.1816, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 823\n",
      "MEAN POLICY LOSS tensor(0.1758, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 824\n",
      "MEAN POLICY LOSS tensor(0.1804, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 825\n",
      "MEAN POLICY LOSS tensor(0.1989, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 826\n",
      "MEAN POLICY LOSS tensor(0.2036, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 827\n",
      "MEAN POLICY LOSS tensor(0.2001, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 828\n",
      "MEAN POLICY LOSS tensor(0.1932, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 829\n",
      "MEAN POLICY LOSS tensor(0.1747, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 830\n",
      "MEAN POLICY LOSS tensor(0.2869, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 831\n",
      "MEAN POLICY LOSS tensor(0.2128, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 832\n",
      "MEAN POLICY LOSS tensor(0.2059, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 833\n",
      "MEAN POLICY LOSS tensor(0.1804, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 834\n",
      "MEAN POLICY LOSS tensor(0.1573, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 835\n",
      "MEAN POLICY LOSS tensor(0.2533, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 836\n",
      "MEAN POLICY LOSS tensor(0.2290, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 837\n",
      "MEAN POLICY LOSS tensor(0.1642, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 838\n",
      "MEAN POLICY LOSS tensor(0.2105, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 839\n",
      "MEAN POLICY LOSS tensor(0.1666, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 840\n",
      "MEAN POLICY LOSS tensor(0.1758, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 841\n",
      "MEAN POLICY LOSS tensor(0.2209, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 842\n",
      "MEAN POLICY LOSS tensor(0.1955, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 843\n",
      "MEAN POLICY LOSS tensor(0.2221, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 844\n",
      "MEAN POLICY LOSS tensor(0.2498, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 845\n",
      "MEAN POLICY LOSS tensor(0.2140, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 846\n",
      "MEAN POLICY LOSS tensor(0.2198, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 847\n",
      "MEAN POLICY LOSS tensor(0.2140, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 848\n",
      "MEAN POLICY LOSS tensor(0.2371, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 849\n",
      "MEAN POLICY LOSS tensor(0.1932, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 850\n",
      "MEAN POLICY LOSS tensor(0.1538, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 851\n",
      "MEAN POLICY LOSS tensor(0.2070, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 852\n",
      "MEAN POLICY LOSS tensor(0.2117, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 853\n",
      "MEAN POLICY LOSS tensor(0.1608, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 854\n",
      "MEAN POLICY LOSS tensor(0.1550, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 855\n",
      "MEAN POLICY LOSS tensor(0.1862, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 856\n",
      "MEAN POLICY LOSS tensor(0.1642, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 857\n",
      "MEAN POLICY LOSS tensor(0.1608, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 858\n",
      "MEAN POLICY LOSS tensor(0.2730, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 859\n",
      "MEAN POLICY LOSS tensor(0.2568, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 860\n",
      "MEAN POLICY LOSS tensor(0.1955, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 861\n",
      "MEAN POLICY LOSS tensor(0.1735, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 862\n",
      "MEAN POLICY LOSS tensor(0.1758, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 863\n",
      "MEAN POLICY LOSS tensor(0.1608, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 864\n",
      "MEAN POLICY LOSS tensor(0.1608, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 865\n",
      "MEAN POLICY LOSS tensor(0.1527, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 866\n",
      "MEAN POLICY LOSS tensor(0.2094, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 867\n",
      "MEAN POLICY LOSS tensor(0.1446, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 868\n",
      "MEAN POLICY LOSS tensor(0.1457, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 869\n",
      "MEAN POLICY LOSS tensor(0.1932, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 870\n",
      "MEAN POLICY LOSS tensor(0.1666, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 871\n",
      "MEAN POLICY LOSS tensor(0.1423, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 872\n",
      "MEAN POLICY LOSS tensor(0.1712, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 873\n",
      "MEAN POLICY LOSS tensor(0.2475, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 874\n",
      "MEAN POLICY LOSS tensor(0.1770, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 875\n",
      "MEAN POLICY LOSS tensor(0.1712, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 876\n",
      "MEAN POLICY LOSS tensor(0.1353, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 877\n",
      "MEAN POLICY LOSS tensor(0.1642, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 878\n",
      "MEAN POLICY LOSS tensor(0.1423, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 879\n",
      "MEAN POLICY LOSS tensor(0.1596, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 880\n",
      "MEAN POLICY LOSS tensor(0.2244, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 881\n",
      "MEAN POLICY LOSS tensor(0.2001, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 882\n",
      "MEAN POLICY LOSS tensor(0.2036, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 883\n",
      "MEAN POLICY LOSS tensor(0.1538, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 884\n",
      "MEAN POLICY LOSS tensor(0.1735, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 885\n",
      "MEAN POLICY LOSS tensor(0.2175, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 886\n",
      "MEAN POLICY LOSS tensor(0.2013, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 887\n",
      "MEAN POLICY LOSS tensor(0.1828, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 888\n",
      "MEAN POLICY LOSS tensor(0.1654, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 889\n",
      "MEAN POLICY LOSS tensor(0.1492, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 890\n",
      "MEAN POLICY LOSS tensor(0.2001, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 891\n",
      "MEAN POLICY LOSS tensor(0.1504, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 892\n",
      "MEAN POLICY LOSS tensor(0.1538, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 893\n",
      "MEAN POLICY LOSS tensor(0.1723, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 894\n",
      "MEAN POLICY LOSS tensor(0.2221, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 895\n",
      "MEAN POLICY LOSS tensor(0.1423, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 896\n",
      "MEAN POLICY LOSS tensor(0.1411, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 897\n",
      "MEAN POLICY LOSS tensor(0.1596, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 898\n",
      "MEAN POLICY LOSS tensor(0.1550, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 899\n",
      "MEAN POLICY LOSS tensor(0.1550, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 900\n",
      "MEAN POLICY LOSS tensor(0.1527, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 901\n",
      "MEAN POLICY LOSS tensor(0.2232, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 902\n",
      "MEAN POLICY LOSS tensor(0.2464, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 903\n",
      "MEAN POLICY LOSS tensor(0.1527, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 904\n",
      "MEAN POLICY LOSS tensor(0.1770, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 905\n",
      "MEAN POLICY LOSS tensor(0.1272, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 906\n",
      "MEAN POLICY LOSS tensor(0.1573, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 907\n",
      "MEAN POLICY LOSS tensor(0.1550, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 908\n",
      "MEAN POLICY LOSS tensor(0.1400, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 909\n",
      "MEAN POLICY LOSS tensor(0.1434, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 910\n",
      "MEAN POLICY LOSS tensor(0.2036, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 911\n",
      "MEAN POLICY LOSS tensor(0.2024, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 912\n",
      "MEAN POLICY LOSS tensor(0.1909, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 913\n",
      "MEAN POLICY LOSS tensor(0.1839, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 914\n",
      "MEAN POLICY LOSS tensor(0.1608, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 915\n",
      "MEAN POLICY LOSS tensor(0.1712, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 916\n",
      "MEAN POLICY LOSS tensor(0.2533, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 917\n",
      "MEAN POLICY LOSS tensor(0.1966, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 918\n",
      "MEAN POLICY LOSS tensor(0.1793, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 919\n",
      "MEAN POLICY LOSS tensor(0.1793, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 920\n",
      "MEAN POLICY LOSS tensor(0.1319, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 921\n",
      "MEAN POLICY LOSS tensor(0.1261, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 922\n",
      "MEAN POLICY LOSS tensor(0.1400, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 923\n",
      "MEAN POLICY LOSS tensor(0.1365, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 924\n",
      "MEAN POLICY LOSS tensor(0.1596, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 925\n",
      "MEAN POLICY LOSS tensor(0.1434, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 926\n",
      "MEAN POLICY LOSS tensor(0.1365, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 927\n",
      "MEAN POLICY LOSS tensor(0.1423, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 928\n",
      "MEAN POLICY LOSS tensor(0.1469, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 929\n",
      "MEAN POLICY LOSS tensor(0.1481, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 930\n",
      "MEAN POLICY LOSS tensor(0.1284, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 931\n",
      "MEAN POLICY LOSS tensor(0.1272, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 932\n",
      "MEAN POLICY LOSS tensor(0.1434, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 933\n",
      "MEAN POLICY LOSS tensor(0.1330, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 934\n",
      "MEAN POLICY LOSS tensor(0.1272, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 935\n",
      "MEAN POLICY LOSS tensor(0.1966, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 936\n",
      "MEAN POLICY LOSS tensor(0.1527, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 937\n",
      "MEAN POLICY LOSS tensor(0.1469, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 938\n",
      "MEAN POLICY LOSS tensor(0.1284, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 939\n",
      "MEAN POLICY LOSS tensor(0.1342, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 940\n",
      "MEAN POLICY LOSS tensor(0.1573, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 941\n",
      "MEAN POLICY LOSS tensor(0.1816, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 942\n",
      "MEAN POLICY LOSS tensor(0.1191, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 943\n",
      "MEAN POLICY LOSS tensor(0.1585, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 944\n",
      "MEAN POLICY LOSS tensor(0.1145, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 945\n",
      "MEAN POLICY LOSS tensor(0.1770, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 946\n",
      "MEAN POLICY LOSS tensor(0.1654, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 947\n",
      "MEAN POLICY LOSS tensor(0.1215, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 948\n",
      "MEAN POLICY LOSS tensor(0.1411, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 949\n",
      "MEAN POLICY LOSS tensor(0.1238, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 950\n",
      "MEAN POLICY LOSS tensor(0.1411, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 951\n",
      "MEAN POLICY LOSS tensor(0.1562, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 952\n",
      "MEAN POLICY LOSS tensor(0.1457, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 953\n",
      "MEAN POLICY LOSS tensor(0.1550, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 954\n",
      "MEAN POLICY LOSS tensor(0.1226, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 955\n",
      "MEAN POLICY LOSS tensor(0.1689, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 956\n",
      "MEAN POLICY LOSS tensor(0.2036, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 957\n",
      "MEAN POLICY LOSS tensor(0.1307, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 958\n",
      "MEAN POLICY LOSS tensor(0.1203, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 959\n",
      "MEAN POLICY LOSS tensor(0.1307, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 960\n",
      "MEAN POLICY LOSS tensor(0.1238, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 961\n",
      "MEAN POLICY LOSS tensor(0.1677, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 962\n",
      "MEAN POLICY LOSS tensor(0.1642, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 963\n",
      "MEAN POLICY LOSS tensor(0.1527, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 964\n",
      "MEAN POLICY LOSS tensor(0.1515, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 965\n",
      "MEAN POLICY LOSS tensor(0.1319, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 966\n",
      "MEAN POLICY LOSS tensor(0.1261, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 967\n",
      "MEAN POLICY LOSS tensor(0.1157, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 968\n",
      "MEAN POLICY LOSS tensor(0.1168, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 969\n",
      "MEAN POLICY LOSS tensor(0.1284, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 970\n",
      "MEAN POLICY LOSS tensor(0.1307, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 971\n",
      "MEAN POLICY LOSS tensor(0.1238, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 972\n",
      "MEAN POLICY LOSS tensor(0.1700, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 973\n",
      "MEAN POLICY LOSS tensor(0.1608, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 974\n",
      "MEAN POLICY LOSS tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 975\n",
      "MEAN POLICY LOSS tensor(0.1909, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 976\n",
      "MEAN POLICY LOSS tensor(0.1145, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 977\n",
      "MEAN POLICY LOSS tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 978\n",
      "MEAN POLICY LOSS tensor(0.1654, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 979\n",
      "MEAN POLICY LOSS tensor(0.1654, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 980\n",
      "MEAN POLICY LOSS tensor(0.1319, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 981\n",
      "MEAN POLICY LOSS tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 982\n",
      "MEAN POLICY LOSS tensor(0.1781, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 983\n",
      "MEAN POLICY LOSS tensor(0.1411, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 984\n",
      "MEAN POLICY LOSS tensor(0.1504, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 985\n",
      "MEAN POLICY LOSS tensor(0.1538, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 986\n",
      "MEAN POLICY LOSS tensor(0.1747, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 987\n",
      "MEAN POLICY LOSS tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 988\n",
      "MEAN POLICY LOSS tensor(0.1238, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 989\n",
      "MEAN POLICY LOSS tensor(0.1457, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 990\n",
      "MEAN POLICY LOSS tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 991\n",
      "MEAN POLICY LOSS tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 992\n",
      "MEAN POLICY LOSS tensor(0.1203, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 993\n",
      "MEAN POLICY LOSS tensor(0.1515, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 994\n",
      "MEAN POLICY LOSS tensor(0.2036, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 995\n",
      "MEAN POLICY LOSS tensor(0.1735, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 996\n",
      "MEAN POLICY LOSS tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 997\n",
      "MEAN POLICY LOSS tensor(0.1585, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 998\n",
      "MEAN POLICY LOSS tensor(0.1434, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 999\n",
      "MEAN POLICY LOSS tensor(0.1411, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "Run 3/10\n",
      "EPSIODE# 0\n",
      "MEAN POLICY LOSS tensor(8688.5391, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 1\n",
      "MEAN POLICY LOSS tensor(6346.0737, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 2\n",
      "MEAN POLICY LOSS tensor(14659.5801, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 3\n",
      "MEAN POLICY LOSS tensor(11530.5586, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 4\n",
      "MEAN POLICY LOSS tensor(3168.7949, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 5\n",
      "MEAN POLICY LOSS tensor(8977.9502, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 6\n",
      "MEAN POLICY LOSS tensor(16996.9062, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 7\n",
      "MEAN POLICY LOSS tensor(15992.4600, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 8\n",
      "MEAN POLICY LOSS tensor(6642.9834, grad_fn=<AddBackward0>)\n",
      "REWARD -460.0\n",
      "EPSIODE# 9\n",
      "MEAN POLICY LOSS tensor(6045.7817, grad_fn=<AddBackward0>)\n",
      "REWARD -413.0\n",
      "EPSIODE# 10\n",
      "MEAN POLICY LOSS tensor(14555.1611, grad_fn=<AddBackward0>)\n",
      "REWARD -339.0\n",
      "EPSIODE# 11\n",
      "MEAN POLICY LOSS tensor(30695.9707, grad_fn=<AddBackward0>)\n",
      "REWARD -419.0\n",
      "EPSIODE# 12\n",
      "MEAN POLICY LOSS tensor(7627.6924, grad_fn=<AddBackward0>)\n",
      "REWARD -189.0\n",
      "EPSIODE# 13\n",
      "MEAN POLICY LOSS tensor(5108.4912, grad_fn=<AddBackward0>)\n",
      "REWARD -239.0\n",
      "EPSIODE# 14\n",
      "MEAN POLICY LOSS tensor(18314.4590, grad_fn=<AddBackward0>)\n",
      "REWARD -433.0\n",
      "EPSIODE# 15\n",
      "MEAN POLICY LOSS tensor(2959.1328, grad_fn=<AddBackward0>)\n",
      "REWARD -342.0\n",
      "EPSIODE# 16\n",
      "MEAN POLICY LOSS tensor(2533.6567, grad_fn=<AddBackward0>)\n",
      "REWARD -479.0\n",
      "EPSIODE# 17\n",
      "MEAN POLICY LOSS tensor(4000.3862, grad_fn=<AddBackward0>)\n",
      "REWARD -388.0\n",
      "EPSIODE# 18\n",
      "MEAN POLICY LOSS tensor(4973.1802, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 19\n",
      "MEAN POLICY LOSS tensor(13111.2686, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 20\n",
      "MEAN POLICY LOSS tensor(25117.0195, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 21\n",
      "MEAN POLICY LOSS tensor(6783.6621, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 22\n",
      "MEAN POLICY LOSS tensor(17062.0879, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 23\n",
      "MEAN POLICY LOSS tensor(5842.3628, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 24\n",
      "MEAN POLICY LOSS tensor(6757.7192, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 25\n",
      "MEAN POLICY LOSS tensor(19784.6992, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 26\n",
      "MEAN POLICY LOSS tensor(4749.0034, grad_fn=<AddBackward0>)\n",
      "REWARD -464.0\n",
      "EPSIODE# 27\n",
      "MEAN POLICY LOSS tensor(4313.6338, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 28\n",
      "MEAN POLICY LOSS tensor(2359.2417, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 29\n",
      "MEAN POLICY LOSS tensor(7512.2598, grad_fn=<AddBackward0>)\n",
      "REWARD -373.0\n",
      "EPSIODE# 30\n",
      "MEAN POLICY LOSS tensor(3567.6624, grad_fn=<AddBackward0>)\n",
      "REWARD -262.0\n",
      "EPSIODE# 31\n",
      "MEAN POLICY LOSS tensor(4625.3735, grad_fn=<AddBackward0>)\n",
      "REWARD -490.0\n",
      "EPSIODE# 32\n",
      "MEAN POLICY LOSS tensor(5790.0557, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 33\n",
      "MEAN POLICY LOSS tensor(4441.2549, grad_fn=<AddBackward0>)\n",
      "REWARD -293.0\n",
      "EPSIODE# 34\n",
      "MEAN POLICY LOSS tensor(9530.6787, grad_fn=<AddBackward0>)\n",
      "REWARD -438.0\n",
      "EPSIODE# 35\n",
      "MEAN POLICY LOSS tensor(5860.5879, grad_fn=<AddBackward0>)\n",
      "REWARD -401.0\n",
      "EPSIODE# 36\n",
      "MEAN POLICY LOSS tensor(1336.1562, grad_fn=<AddBackward0>)\n",
      "REWARD -479.0\n",
      "EPSIODE# 37\n",
      "MEAN POLICY LOSS tensor(5656.4229, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 38\n",
      "MEAN POLICY LOSS tensor(6729.4976, grad_fn=<AddBackward0>)\n",
      "REWARD -366.0\n",
      "EPSIODE# 39\n",
      "MEAN POLICY LOSS tensor(1914.2123, grad_fn=<AddBackward0>)\n",
      "REWARD -278.0\n",
      "EPSIODE# 40\n",
      "MEAN POLICY LOSS tensor(7081.7847, grad_fn=<AddBackward0>)\n",
      "REWARD -289.0\n",
      "EPSIODE# 41\n",
      "MEAN POLICY LOSS tensor(2247.1658, grad_fn=<AddBackward0>)\n",
      "REWARD -302.0\n",
      "EPSIODE# 42\n",
      "MEAN POLICY LOSS tensor(7439.6914, grad_fn=<AddBackward0>)\n",
      "REWARD -343.0\n",
      "EPSIODE# 43\n",
      "MEAN POLICY LOSS tensor(2823.3276, grad_fn=<AddBackward0>)\n",
      "REWARD -238.0\n",
      "EPSIODE# 44\n",
      "MEAN POLICY LOSS tensor(1658.0602, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 45\n",
      "MEAN POLICY LOSS tensor(8119.7217, grad_fn=<AddBackward0>)\n",
      "REWARD -243.0\n",
      "EPSIODE# 46\n",
      "MEAN POLICY LOSS tensor(2490.2266, grad_fn=<AddBackward0>)\n",
      "REWARD -244.0\n",
      "EPSIODE# 47\n",
      "MEAN POLICY LOSS tensor(1400.7566, grad_fn=<AddBackward0>)\n",
      "REWARD -307.0\n",
      "EPSIODE# 48\n",
      "MEAN POLICY LOSS tensor(8547.8213, grad_fn=<AddBackward0>)\n",
      "REWARD -189.0\n",
      "EPSIODE# 49\n",
      "MEAN POLICY LOSS tensor(1216.7969, grad_fn=<AddBackward0>)\n",
      "REWARD -330.0\n",
      "EPSIODE# 50\n",
      "MEAN POLICY LOSS tensor(1297.7220, grad_fn=<AddBackward0>)\n",
      "REWARD -279.0\n",
      "EPSIODE# 51\n",
      "MEAN POLICY LOSS tensor(3987.7939, grad_fn=<AddBackward0>)\n",
      "REWARD -314.0\n",
      "EPSIODE# 52\n",
      "MEAN POLICY LOSS tensor(22099.7773, grad_fn=<AddBackward0>)\n",
      "REWARD -229.0\n",
      "EPSIODE# 53\n",
      "MEAN POLICY LOSS tensor(2170.0183, grad_fn=<AddBackward0>)\n",
      "REWARD -297.0\n",
      "EPSIODE# 54\n",
      "MEAN POLICY LOSS tensor(1030.9596, grad_fn=<AddBackward0>)\n",
      "REWARD -222.0\n",
      "EPSIODE# 55\n",
      "MEAN POLICY LOSS tensor(4084.6157, grad_fn=<AddBackward0>)\n",
      "REWARD -244.0\n",
      "EPSIODE# 56\n",
      "MEAN POLICY LOSS tensor(1549.2908, grad_fn=<AddBackward0>)\n",
      "REWARD -251.0\n",
      "EPSIODE# 57\n",
      "MEAN POLICY LOSS tensor(1654.7919, grad_fn=<AddBackward0>)\n",
      "REWARD -386.0\n",
      "EPSIODE# 58\n",
      "MEAN POLICY LOSS tensor(4793.7705, grad_fn=<AddBackward0>)\n",
      "REWARD -243.0\n",
      "EPSIODE# 59\n",
      "MEAN POLICY LOSS tensor(17413.2344, grad_fn=<AddBackward0>)\n",
      "REWARD -316.0\n",
      "EPSIODE# 60\n",
      "MEAN POLICY LOSS tensor(1581.8876, grad_fn=<AddBackward0>)\n",
      "REWARD -349.0\n",
      "EPSIODE# 61\n",
      "MEAN POLICY LOSS tensor(3368.1619, grad_fn=<AddBackward0>)\n",
      "REWARD -208.0\n",
      "EPSIODE# 62\n",
      "MEAN POLICY LOSS tensor(227.5876, grad_fn=<AddBackward0>)\n",
      "REWARD -340.0\n",
      "EPSIODE# 63\n",
      "MEAN POLICY LOSS tensor(13715.3223, grad_fn=<AddBackward0>)\n",
      "REWARD -321.0\n",
      "EPSIODE# 64\n",
      "MEAN POLICY LOSS tensor(171.1785, grad_fn=<AddBackward0>)\n",
      "REWARD -320.0\n",
      "EPSIODE# 65\n",
      "MEAN POLICY LOSS tensor(4846.5806, grad_fn=<AddBackward0>)\n",
      "REWARD -207.0\n",
      "EPSIODE# 66\n",
      "MEAN POLICY LOSS tensor(2713.1953, grad_fn=<AddBackward0>)\n",
      "REWARD -336.0\n",
      "EPSIODE# 67\n",
      "MEAN POLICY LOSS tensor(3887.7056, grad_fn=<AddBackward0>)\n",
      "REWARD -375.0\n",
      "EPSIODE# 68\n",
      "MEAN POLICY LOSS tensor(1793.7377, grad_fn=<AddBackward0>)\n",
      "REWARD -286.0\n",
      "EPSIODE# 69\n",
      "MEAN POLICY LOSS tensor(232.3597, grad_fn=<AddBackward0>)\n",
      "REWARD -228.0\n",
      "EPSIODE# 70\n",
      "MEAN POLICY LOSS tensor(32036.3027, grad_fn=<AddBackward0>)\n",
      "REWARD -375.0\n",
      "EPSIODE# 71\n",
      "MEAN POLICY LOSS tensor(7960.7544, grad_fn=<AddBackward0>)\n",
      "REWARD -376.0\n",
      "EPSIODE# 72\n",
      "MEAN POLICY LOSS tensor(4750.3174, grad_fn=<AddBackward0>)\n",
      "REWARD -226.0\n",
      "EPSIODE# 73\n",
      "MEAN POLICY LOSS tensor(13668.5410, grad_fn=<AddBackward0>)\n",
      "REWARD -277.0\n",
      "EPSIODE# 74\n",
      "MEAN POLICY LOSS tensor(5664.6431, grad_fn=<AddBackward0>)\n",
      "REWARD -206.0\n",
      "EPSIODE# 75\n",
      "MEAN POLICY LOSS tensor(17219.4336, grad_fn=<AddBackward0>)\n",
      "REWARD -298.0\n",
      "EPSIODE# 76\n",
      "MEAN POLICY LOSS tensor(5017.2046, grad_fn=<AddBackward0>)\n",
      "REWARD -267.0\n",
      "EPSIODE# 77\n",
      "MEAN POLICY LOSS tensor(1118.1316, grad_fn=<AddBackward0>)\n",
      "REWARD -274.0\n",
      "EPSIODE# 78\n",
      "MEAN POLICY LOSS tensor(5453.2319, grad_fn=<AddBackward0>)\n",
      "REWARD -326.0\n",
      "EPSIODE# 79\n",
      "MEAN POLICY LOSS tensor(6301.5435, grad_fn=<AddBackward0>)\n",
      "REWARD -439.0\n",
      "EPSIODE# 80\n",
      "MEAN POLICY LOSS tensor(2184.1780, grad_fn=<AddBackward0>)\n",
      "REWARD -348.0\n",
      "EPSIODE# 81\n",
      "MEAN POLICY LOSS tensor(4185.6353, grad_fn=<AddBackward0>)\n",
      "REWARD -209.0\n",
      "EPSIODE# 82\n",
      "MEAN POLICY LOSS tensor(3095.7876, grad_fn=<AddBackward0>)\n",
      "REWARD -257.0\n",
      "EPSIODE# 83\n",
      "MEAN POLICY LOSS tensor(4143.3433, grad_fn=<AddBackward0>)\n",
      "REWARD -266.0\n",
      "EPSIODE# 84\n",
      "MEAN POLICY LOSS tensor(734.3589, grad_fn=<AddBackward0>)\n",
      "REWARD -250.0\n",
      "EPSIODE# 85\n",
      "MEAN POLICY LOSS tensor(2551.3950, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 86\n",
      "MEAN POLICY LOSS tensor(10296.2852, grad_fn=<AddBackward0>)\n",
      "REWARD -471.0\n",
      "EPSIODE# 87\n",
      "MEAN POLICY LOSS tensor(3516.0654, grad_fn=<AddBackward0>)\n",
      "REWARD -315.0\n",
      "EPSIODE# 88\n",
      "MEAN POLICY LOSS tensor(715.2982, grad_fn=<AddBackward0>)\n",
      "REWARD -330.0\n",
      "EPSIODE# 89\n",
      "MEAN POLICY LOSS tensor(11334.1484, grad_fn=<AddBackward0>)\n",
      "REWARD -240.0\n",
      "EPSIODE# 90\n",
      "MEAN POLICY LOSS tensor(4970.8945, grad_fn=<AddBackward0>)\n",
      "REWARD -206.0\n",
      "EPSIODE# 91\n",
      "MEAN POLICY LOSS tensor(12111.2168, grad_fn=<AddBackward0>)\n",
      "REWARD -238.0\n",
      "EPSIODE# 92\n",
      "MEAN POLICY LOSS tensor(1637.0548, grad_fn=<AddBackward0>)\n",
      "REWARD -327.0\n",
      "EPSIODE# 93\n",
      "MEAN POLICY LOSS tensor(8019.6084, grad_fn=<AddBackward0>)\n",
      "REWARD -284.0\n",
      "EPSIODE# 94\n",
      "MEAN POLICY LOSS tensor(397.7551, grad_fn=<AddBackward0>)\n",
      "REWARD -226.0\n",
      "EPSIODE# 95\n",
      "MEAN POLICY LOSS tensor(25791.0117, grad_fn=<AddBackward0>)\n",
      "REWARD -239.0\n",
      "EPSIODE# 96\n",
      "MEAN POLICY LOSS tensor(4302.4541, grad_fn=<AddBackward0>)\n",
      "REWARD -250.0\n",
      "EPSIODE# 97\n",
      "MEAN POLICY LOSS tensor(84.0856, grad_fn=<AddBackward0>)\n",
      "REWARD -334.0\n",
      "EPSIODE# 98\n",
      "MEAN POLICY LOSS tensor(999.6178, grad_fn=<AddBackward0>)\n",
      "REWARD -258.0\n",
      "EPSIODE# 99\n",
      "MEAN POLICY LOSS tensor(514.6539, grad_fn=<AddBackward0>)\n",
      "REWARD -231.0\n",
      "EPSIODE# 100\n",
      "MEAN POLICY LOSS tensor(1257.6251, grad_fn=<AddBackward0>)\n",
      "REWARD -240.0\n",
      "EPSIODE# 101\n",
      "MEAN POLICY LOSS tensor(1900.7391, grad_fn=<AddBackward0>)\n",
      "REWARD -191.0\n",
      "EPSIODE# 102\n",
      "MEAN POLICY LOSS tensor(327.8297, grad_fn=<AddBackward0>)\n",
      "REWARD -197.0\n",
      "EPSIODE# 103\n",
      "MEAN POLICY LOSS tensor(2172.0859, grad_fn=<AddBackward0>)\n",
      "REWARD -303.0\n",
      "EPSIODE# 104\n",
      "MEAN POLICY LOSS tensor(6632.0562, grad_fn=<AddBackward0>)\n",
      "REWARD -223.0\n",
      "EPSIODE# 105\n",
      "MEAN POLICY LOSS tensor(4584.2598, grad_fn=<AddBackward0>)\n",
      "REWARD -276.0\n",
      "EPSIODE# 106\n",
      "MEAN POLICY LOSS tensor(7637.2886, grad_fn=<AddBackward0>)\n",
      "REWARD -192.0\n",
      "EPSIODE# 107\n",
      "MEAN POLICY LOSS tensor(1017.3996, grad_fn=<AddBackward0>)\n",
      "REWARD -230.0\n",
      "EPSIODE# 108\n",
      "MEAN POLICY LOSS tensor(17122.8379, grad_fn=<AddBackward0>)\n",
      "REWARD -228.0\n",
      "EPSIODE# 109\n",
      "MEAN POLICY LOSS tensor(3735.2488, grad_fn=<AddBackward0>)\n",
      "REWARD -345.0\n",
      "EPSIODE# 110\n",
      "MEAN POLICY LOSS tensor(687.4107, grad_fn=<AddBackward0>)\n",
      "REWARD -197.0\n",
      "EPSIODE# 111\n",
      "MEAN POLICY LOSS tensor(1168.8962, grad_fn=<AddBackward0>)\n",
      "REWARD -242.0\n",
      "EPSIODE# 112\n",
      "MEAN POLICY LOSS tensor(1335.6768, grad_fn=<AddBackward0>)\n",
      "REWARD -219.0\n",
      "EPSIODE# 113\n",
      "MEAN POLICY LOSS tensor(1058.2111, grad_fn=<AddBackward0>)\n",
      "REWARD -211.0\n",
      "EPSIODE# 114\n",
      "MEAN POLICY LOSS tensor(1084.2230, grad_fn=<AddBackward0>)\n",
      "REWARD -258.0\n",
      "EPSIODE# 115\n",
      "MEAN POLICY LOSS tensor(725.5521, grad_fn=<AddBackward0>)\n",
      "REWARD -366.0\n",
      "EPSIODE# 116\n",
      "MEAN POLICY LOSS tensor(936.3077, grad_fn=<AddBackward0>)\n",
      "REWARD -353.0\n",
      "EPSIODE# 117\n",
      "MEAN POLICY LOSS tensor(2189.5945, grad_fn=<AddBackward0>)\n",
      "REWARD -276.0\n",
      "EPSIODE# 118\n",
      "MEAN POLICY LOSS tensor(2148.0715, grad_fn=<AddBackward0>)\n",
      "REWARD -177.0\n",
      "EPSIODE# 119\n",
      "MEAN POLICY LOSS tensor(5313.9199, grad_fn=<AddBackward0>)\n",
      "REWARD -241.0\n",
      "EPSIODE# 120\n",
      "MEAN POLICY LOSS tensor(1372.4708, grad_fn=<AddBackward0>)\n",
      "REWARD -186.0\n",
      "EPSIODE# 121\n",
      "MEAN POLICY LOSS tensor(5904.3467, grad_fn=<AddBackward0>)\n",
      "REWARD -249.0\n",
      "EPSIODE# 122\n",
      "MEAN POLICY LOSS tensor(1932.8846, grad_fn=<AddBackward0>)\n",
      "REWARD -236.0\n",
      "EPSIODE# 123\n",
      "MEAN POLICY LOSS tensor(875.0039, grad_fn=<AddBackward0>)\n",
      "REWARD -380.0\n",
      "EPSIODE# 124\n",
      "MEAN POLICY LOSS tensor(2950.1326, grad_fn=<AddBackward0>)\n",
      "REWARD -265.0\n",
      "EPSIODE# 125\n",
      "MEAN POLICY LOSS tensor(2675.7100, grad_fn=<AddBackward0>)\n",
      "REWARD -290.0\n",
      "EPSIODE# 126\n",
      "MEAN POLICY LOSS tensor(2905.4714, grad_fn=<AddBackward0>)\n",
      "REWARD -335.0\n",
      "EPSIODE# 127\n",
      "MEAN POLICY LOSS tensor(1060.3510, grad_fn=<AddBackward0>)\n",
      "REWARD -239.0\n",
      "EPSIODE# 128\n",
      "MEAN POLICY LOSS tensor(1048.9375, grad_fn=<AddBackward0>)\n",
      "REWARD -281.0\n",
      "EPSIODE# 129\n",
      "MEAN POLICY LOSS tensor(6806.1812, grad_fn=<AddBackward0>)\n",
      "REWARD -380.0\n",
      "EPSIODE# 130\n",
      "MEAN POLICY LOSS tensor(1257.1042, grad_fn=<AddBackward0>)\n",
      "REWARD -294.0\n",
      "EPSIODE# 131\n",
      "MEAN POLICY LOSS tensor(29.3148, grad_fn=<AddBackward0>)\n",
      "REWARD -421.0\n",
      "EPSIODE# 132\n",
      "MEAN POLICY LOSS tensor(1788.8530, grad_fn=<AddBackward0>)\n",
      "REWARD -257.0\n",
      "EPSIODE# 133\n",
      "MEAN POLICY LOSS tensor(866.4397, grad_fn=<AddBackward0>)\n",
      "REWARD -282.0\n",
      "EPSIODE# 134\n",
      "MEAN POLICY LOSS tensor(16093.0625, grad_fn=<AddBackward0>)\n",
      "REWARD -344.0\n",
      "EPSIODE# 135\n",
      "MEAN POLICY LOSS tensor(833.1278, grad_fn=<AddBackward0>)\n",
      "REWARD -331.0\n",
      "EPSIODE# 136\n",
      "MEAN POLICY LOSS tensor(5820.5171, grad_fn=<AddBackward0>)\n",
      "REWARD -289.0\n",
      "EPSIODE# 137\n",
      "MEAN POLICY LOSS tensor(4369.8901, grad_fn=<AddBackward0>)\n",
      "REWARD -284.0\n",
      "EPSIODE# 138\n",
      "MEAN POLICY LOSS tensor(1451.6121, grad_fn=<AddBackward0>)\n",
      "REWARD -334.0\n",
      "EPSIODE# 139\n",
      "MEAN POLICY LOSS tensor(11150.0410, grad_fn=<AddBackward0>)\n",
      "REWARD -300.0\n",
      "EPSIODE# 140\n",
      "MEAN POLICY LOSS tensor(15265.6650, grad_fn=<AddBackward0>)\n",
      "REWARD -312.0\n",
      "EPSIODE# 141\n",
      "MEAN POLICY LOSS tensor(15692.5117, grad_fn=<AddBackward0>)\n",
      "REWARD -282.0\n",
      "EPSIODE# 142\n",
      "MEAN POLICY LOSS tensor(245.5544, grad_fn=<AddBackward0>)\n",
      "REWARD -207.0\n",
      "EPSIODE# 143\n",
      "MEAN POLICY LOSS tensor(826.5251, grad_fn=<AddBackward0>)\n",
      "REWARD -189.0\n",
      "EPSIODE# 144\n",
      "MEAN POLICY LOSS tensor(156.2601, grad_fn=<AddBackward0>)\n",
      "REWARD -190.0\n",
      "EPSIODE# 145\n",
      "MEAN POLICY LOSS tensor(1250.4459, grad_fn=<AddBackward0>)\n",
      "REWARD -164.0\n",
      "EPSIODE# 146\n",
      "MEAN POLICY LOSS tensor(381.3836, grad_fn=<AddBackward0>)\n",
      "REWARD -239.0\n",
      "EPSIODE# 147\n",
      "MEAN POLICY LOSS tensor(488.9071, grad_fn=<AddBackward0>)\n",
      "REWARD -176.0\n",
      "EPSIODE# 148\n",
      "MEAN POLICY LOSS tensor(985.3762, grad_fn=<AddBackward0>)\n",
      "REWARD -209.0\n",
      "EPSIODE# 149\n",
      "MEAN POLICY LOSS tensor(345.8172, grad_fn=<AddBackward0>)\n",
      "REWARD -173.0\n",
      "EPSIODE# 150\n",
      "MEAN POLICY LOSS tensor(420.8048, grad_fn=<AddBackward0>)\n",
      "REWARD -202.0\n",
      "EPSIODE# 151\n",
      "MEAN POLICY LOSS tensor(37.8314, grad_fn=<AddBackward0>)\n",
      "REWARD -157.0\n",
      "EPSIODE# 152\n",
      "MEAN POLICY LOSS tensor(212.4356, grad_fn=<AddBackward0>)\n",
      "REWARD -174.0\n",
      "EPSIODE# 153\n",
      "MEAN POLICY LOSS tensor(279.4001, grad_fn=<AddBackward0>)\n",
      "REWARD -177.0\n",
      "EPSIODE# 154\n",
      "MEAN POLICY LOSS tensor(81.4282, grad_fn=<AddBackward0>)\n",
      "REWARD -273.0\n",
      "EPSIODE# 155\n",
      "MEAN POLICY LOSS tensor(12337.5352, grad_fn=<AddBackward0>)\n",
      "REWARD -261.0\n",
      "EPSIODE# 156\n",
      "MEAN POLICY LOSS tensor(82.1843, grad_fn=<AddBackward0>)\n",
      "REWARD -316.0\n",
      "EPSIODE# 157\n",
      "MEAN POLICY LOSS tensor(20.6605, grad_fn=<AddBackward0>)\n",
      "REWARD -232.0\n",
      "EPSIODE# 158\n",
      "MEAN POLICY LOSS tensor(2172.0083, grad_fn=<AddBackward0>)\n",
      "REWARD -197.0\n",
      "EPSIODE# 159\n",
      "MEAN POLICY LOSS tensor(18.0899, grad_fn=<AddBackward0>)\n",
      "REWARD -184.0\n",
      "EPSIODE# 160\n",
      "MEAN POLICY LOSS tensor(7.0665, grad_fn=<AddBackward0>)\n",
      "REWARD -223.0\n",
      "EPSIODE# 161\n",
      "MEAN POLICY LOSS tensor(912.9476, grad_fn=<AddBackward0>)\n",
      "REWARD -228.0\n",
      "EPSIODE# 162\n",
      "MEAN POLICY LOSS tensor(470.0349, grad_fn=<AddBackward0>)\n",
      "REWARD -193.0\n",
      "EPSIODE# 163\n",
      "MEAN POLICY LOSS tensor(5521.2861, grad_fn=<AddBackward0>)\n",
      "REWARD -153.0\n",
      "EPSIODE# 164\n",
      "MEAN POLICY LOSS tensor(7669.3184, grad_fn=<AddBackward0>)\n",
      "REWARD -219.0\n",
      "EPSIODE# 165\n",
      "MEAN POLICY LOSS tensor(96.0618, grad_fn=<AddBackward0>)\n",
      "REWARD -276.0\n",
      "EPSIODE# 166\n",
      "MEAN POLICY LOSS tensor(6.8453, grad_fn=<AddBackward0>)\n",
      "REWARD -232.0\n",
      "EPSIODE# 167\n",
      "MEAN POLICY LOSS tensor(13.5292, grad_fn=<AddBackward0>)\n",
      "REWARD -173.0\n",
      "EPSIODE# 168\n",
      "MEAN POLICY LOSS tensor(3835.9282, grad_fn=<AddBackward0>)\n",
      "REWARD -202.0\n",
      "EPSIODE# 169\n",
      "MEAN POLICY LOSS tensor(9.5017, grad_fn=<AddBackward0>)\n",
      "REWARD -196.0\n",
      "EPSIODE# 170\n",
      "MEAN POLICY LOSS tensor(1082.9377, grad_fn=<AddBackward0>)\n",
      "REWARD -177.0\n",
      "EPSIODE# 171\n",
      "MEAN POLICY LOSS tensor(391.5938, grad_fn=<AddBackward0>)\n",
      "REWARD -239.0\n",
      "EPSIODE# 172\n",
      "MEAN POLICY LOSS tensor(19203.8281, grad_fn=<AddBackward0>)\n",
      "REWARD -224.0\n",
      "EPSIODE# 173\n",
      "MEAN POLICY LOSS tensor(2916.4199, grad_fn=<AddBackward0>)\n",
      "REWARD -232.0\n",
      "EPSIODE# 174\n",
      "MEAN POLICY LOSS tensor(4052.6272, grad_fn=<AddBackward0>)\n",
      "REWARD -165.0\n",
      "EPSIODE# 175\n",
      "MEAN POLICY LOSS tensor(1506.1340, grad_fn=<AddBackward0>)\n",
      "REWARD -142.0\n",
      "EPSIODE# 176\n",
      "MEAN POLICY LOSS tensor(1696.4412, grad_fn=<AddBackward0>)\n",
      "REWARD -168.0\n",
      "EPSIODE# 177\n",
      "MEAN POLICY LOSS tensor(3.7112, grad_fn=<AddBackward0>)\n",
      "REWARD -216.0\n",
      "EPSIODE# 178\n",
      "MEAN POLICY LOSS tensor(820.5312, grad_fn=<AddBackward0>)\n",
      "REWARD -206.0\n",
      "EPSIODE# 179\n",
      "MEAN POLICY LOSS tensor(5849.4790, grad_fn=<AddBackward0>)\n",
      "REWARD -166.0\n",
      "EPSIODE# 180\n",
      "MEAN POLICY LOSS tensor(326.2741, grad_fn=<AddBackward0>)\n",
      "REWARD -246.0\n",
      "EPSIODE# 181\n",
      "MEAN POLICY LOSS tensor(5799.6025, grad_fn=<AddBackward0>)\n",
      "REWARD -178.0\n",
      "EPSIODE# 182\n",
      "MEAN POLICY LOSS tensor(492.7206, grad_fn=<AddBackward0>)\n",
      "REWARD -173.0\n",
      "EPSIODE# 183\n",
      "MEAN POLICY LOSS tensor(412.1958, grad_fn=<AddBackward0>)\n",
      "REWARD -134.0\n",
      "EPSIODE# 184\n",
      "MEAN POLICY LOSS tensor(180.6024, grad_fn=<AddBackward0>)\n",
      "REWARD -202.0\n",
      "EPSIODE# 185\n",
      "MEAN POLICY LOSS tensor(101.6898, grad_fn=<AddBackward0>)\n",
      "REWARD -206.0\n",
      "EPSIODE# 186\n",
      "MEAN POLICY LOSS tensor(12.2873, grad_fn=<AddBackward0>)\n",
      "REWARD -149.0\n",
      "EPSIODE# 187\n",
      "MEAN POLICY LOSS tensor(41.2371, grad_fn=<AddBackward0>)\n",
      "REWARD -171.0\n",
      "EPSIODE# 188\n",
      "MEAN POLICY LOSS tensor(798.2734, grad_fn=<AddBackward0>)\n",
      "REWARD -201.0\n",
      "EPSIODE# 189\n",
      "MEAN POLICY LOSS tensor(105.5505, grad_fn=<AddBackward0>)\n",
      "REWARD -186.0\n",
      "EPSIODE# 190\n",
      "MEAN POLICY LOSS tensor(1210.5171, grad_fn=<AddBackward0>)\n",
      "REWARD -304.0\n",
      "EPSIODE# 191\n",
      "MEAN POLICY LOSS tensor(96.7558, grad_fn=<AddBackward0>)\n",
      "REWARD -158.0\n",
      "EPSIODE# 192\n",
      "MEAN POLICY LOSS tensor(572.9512, grad_fn=<AddBackward0>)\n",
      "REWARD -190.0\n",
      "EPSIODE# 193\n",
      "MEAN POLICY LOSS tensor(2412.7097, grad_fn=<AddBackward0>)\n",
      "REWARD -192.0\n",
      "EPSIODE# 194\n",
      "MEAN POLICY LOSS tensor(266.4963, grad_fn=<AddBackward0>)\n",
      "REWARD -198.0\n",
      "EPSIODE# 195\n",
      "MEAN POLICY LOSS tensor(1353.0459, grad_fn=<AddBackward0>)\n",
      "REWARD -197.0\n",
      "EPSIODE# 196\n",
      "MEAN POLICY LOSS tensor(567.2541, grad_fn=<AddBackward0>)\n",
      "REWARD -226.0\n",
      "EPSIODE# 197\n",
      "MEAN POLICY LOSS tensor(109.0107, grad_fn=<AddBackward0>)\n",
      "REWARD -191.0\n",
      "EPSIODE# 198\n",
      "MEAN POLICY LOSS tensor(382.0551, grad_fn=<AddBackward0>)\n",
      "REWARD -182.0\n",
      "EPSIODE# 199\n",
      "MEAN POLICY LOSS tensor(28.0004, grad_fn=<AddBackward0>)\n",
      "REWARD -209.0\n",
      "EPSIODE# 200\n",
      "MEAN POLICY LOSS tensor(1025.7554, grad_fn=<AddBackward0>)\n",
      "REWARD -154.0\n",
      "EPSIODE# 201\n",
      "MEAN POLICY LOSS tensor(163.9450, grad_fn=<AddBackward0>)\n",
      "REWARD -130.0\n",
      "EPSIODE# 202\n",
      "MEAN POLICY LOSS tensor(5.4260, grad_fn=<AddBackward0>)\n",
      "REWARD -182.0\n",
      "EPSIODE# 203\n",
      "MEAN POLICY LOSS tensor(2161.9155, grad_fn=<AddBackward0>)\n",
      "REWARD -182.0\n",
      "EPSIODE# 204\n",
      "MEAN POLICY LOSS tensor(68.5404, grad_fn=<AddBackward0>)\n",
      "REWARD -166.0\n",
      "EPSIODE# 205\n",
      "MEAN POLICY LOSS tensor(6068.9194, grad_fn=<AddBackward0>)\n",
      "REWARD -160.0\n",
      "EPSIODE# 206\n",
      "MEAN POLICY LOSS tensor(674.1484, grad_fn=<AddBackward0>)\n",
      "REWARD -145.0\n",
      "EPSIODE# 207\n",
      "MEAN POLICY LOSS tensor(17613.3770, grad_fn=<AddBackward0>)\n",
      "REWARD -186.0\n",
      "EPSIODE# 208\n",
      "MEAN POLICY LOSS tensor(82.9194, grad_fn=<AddBackward0>)\n",
      "REWARD -273.0\n",
      "EPSIODE# 209\n",
      "MEAN POLICY LOSS tensor(152.5959, grad_fn=<AddBackward0>)\n",
      "REWARD -178.0\n",
      "EPSIODE# 210\n",
      "MEAN POLICY LOSS tensor(598.6223, grad_fn=<AddBackward0>)\n",
      "REWARD -154.0\n",
      "EPSIODE# 211\n",
      "MEAN POLICY LOSS tensor(25.3757, grad_fn=<AddBackward0>)\n",
      "REWARD -182.0\n",
      "EPSIODE# 212\n",
      "MEAN POLICY LOSS tensor(167.9693, grad_fn=<AddBackward0>)\n",
      "REWARD -149.0\n",
      "EPSIODE# 213\n",
      "MEAN POLICY LOSS tensor(112.5708, grad_fn=<AddBackward0>)\n",
      "REWARD -285.0\n",
      "EPSIODE# 214\n",
      "MEAN POLICY LOSS tensor(235.3314, grad_fn=<AddBackward0>)\n",
      "REWARD -192.0\n",
      "EPSIODE# 215\n",
      "MEAN POLICY LOSS tensor(21.5701, grad_fn=<AddBackward0>)\n",
      "REWARD -186.0\n",
      "EPSIODE# 216\n",
      "MEAN POLICY LOSS tensor(1740.9966, grad_fn=<AddBackward0>)\n",
      "REWARD -165.0\n",
      "EPSIODE# 217\n",
      "MEAN POLICY LOSS tensor(6965.2144, grad_fn=<AddBackward0>)\n",
      "REWARD -178.0\n",
      "EPSIODE# 218\n",
      "MEAN POLICY LOSS tensor(52.8089, grad_fn=<AddBackward0>)\n",
      "REWARD -156.0\n",
      "EPSIODE# 219\n",
      "MEAN POLICY LOSS tensor(6.1436, grad_fn=<AddBackward0>)\n",
      "REWARD -188.0\n",
      "EPSIODE# 220\n",
      "MEAN POLICY LOSS tensor(261.6537, grad_fn=<AddBackward0>)\n",
      "REWARD -191.0\n",
      "EPSIODE# 221\n",
      "MEAN POLICY LOSS tensor(1.3930, grad_fn=<AddBackward0>)\n",
      "REWARD -174.0\n",
      "EPSIODE# 222\n",
      "MEAN POLICY LOSS tensor(310.8852, grad_fn=<AddBackward0>)\n",
      "REWARD -206.0\n",
      "EPSIODE# 223\n",
      "MEAN POLICY LOSS tensor(36.1973, grad_fn=<AddBackward0>)\n",
      "REWARD -165.0\n",
      "EPSIODE# 224\n",
      "MEAN POLICY LOSS tensor(4247.9805, grad_fn=<AddBackward0>)\n",
      "REWARD -153.0\n",
      "EPSIODE# 225\n",
      "MEAN POLICY LOSS tensor(332.9804, grad_fn=<AddBackward0>)\n",
      "REWARD -127.0\n",
      "EPSIODE# 226\n",
      "MEAN POLICY LOSS tensor(74.4474, grad_fn=<AddBackward0>)\n",
      "REWARD -154.0\n",
      "EPSIODE# 227\n",
      "MEAN POLICY LOSS tensor(8061.5122, grad_fn=<AddBackward0>)\n",
      "REWARD -217.0\n",
      "EPSIODE# 228\n",
      "MEAN POLICY LOSS tensor(534.8763, grad_fn=<AddBackward0>)\n",
      "REWARD -184.0\n",
      "EPSIODE# 229\n",
      "MEAN POLICY LOSS tensor(1024.2163, grad_fn=<AddBackward0>)\n",
      "REWARD -177.0\n",
      "EPSIODE# 230\n",
      "MEAN POLICY LOSS tensor(3788.6340, grad_fn=<AddBackward0>)\n",
      "REWARD -155.0\n",
      "EPSIODE# 231\n",
      "MEAN POLICY LOSS tensor(68.0756, grad_fn=<AddBackward0>)\n",
      "REWARD -182.0\n",
      "EPSIODE# 232\n",
      "MEAN POLICY LOSS tensor(337.7787, grad_fn=<AddBackward0>)\n",
      "REWARD -168.0\n",
      "EPSIODE# 233\n",
      "MEAN POLICY LOSS tensor(69.0468, grad_fn=<AddBackward0>)\n",
      "REWARD -144.0\n",
      "EPSIODE# 234\n",
      "MEAN POLICY LOSS tensor(9.1179, grad_fn=<AddBackward0>)\n",
      "REWARD -197.0\n",
      "EPSIODE# 235\n",
      "MEAN POLICY LOSS tensor(1.2676, grad_fn=<AddBackward0>)\n",
      "REWARD -176.0\n",
      "EPSIODE# 236\n",
      "MEAN POLICY LOSS tensor(471.9755, grad_fn=<AddBackward0>)\n",
      "REWARD -251.0\n",
      "EPSIODE# 237\n",
      "MEAN POLICY LOSS tensor(109.2979, grad_fn=<AddBackward0>)\n",
      "REWARD -170.0\n",
      "EPSIODE# 238\n",
      "MEAN POLICY LOSS tensor(674.7357, grad_fn=<AddBackward0>)\n",
      "REWARD -221.0\n",
      "EPSIODE# 239\n",
      "MEAN POLICY LOSS tensor(13.9951, grad_fn=<AddBackward0>)\n",
      "REWARD -151.0\n",
      "EPSIODE# 240\n",
      "MEAN POLICY LOSS tensor(0., grad_fn=<AddBackward0>)\n",
      "REWARD -257.0\n",
      "EPSIODE# 241\n",
      "MEAN POLICY LOSS tensor(5.1366, grad_fn=<AddBackward0>)\n",
      "REWARD -181.0\n",
      "EPSIODE# 242\n",
      "MEAN POLICY LOSS tensor(75.6813, grad_fn=<AddBackward0>)\n",
      "REWARD -159.0\n",
      "EPSIODE# 243\n",
      "MEAN POLICY LOSS tensor(12.5224, grad_fn=<AddBackward0>)\n",
      "REWARD -149.0\n",
      "EPSIODE# 244\n",
      "MEAN POLICY LOSS tensor(175.5627, grad_fn=<AddBackward0>)\n",
      "REWARD -181.0\n",
      "EPSIODE# 245\n",
      "MEAN POLICY LOSS tensor(2649.8547, grad_fn=<AddBackward0>)\n",
      "REWARD -140.0\n",
      "EPSIODE# 246\n",
      "MEAN POLICY LOSS tensor(73.6559, grad_fn=<AddBackward0>)\n",
      "REWARD -199.0\n",
      "EPSIODE# 247\n",
      "MEAN POLICY LOSS tensor(29.0659, grad_fn=<AddBackward0>)\n",
      "REWARD -198.0\n",
      "EPSIODE# 248\n",
      "MEAN POLICY LOSS tensor(708.3585, grad_fn=<AddBackward0>)\n",
      "REWARD -198.0\n",
      "EPSIODE# 249\n",
      "MEAN POLICY LOSS tensor(12.2956, grad_fn=<AddBackward0>)\n",
      "REWARD -264.0\n",
      "EPSIODE# 250\n",
      "MEAN POLICY LOSS tensor(5.5728, grad_fn=<AddBackward0>)\n",
      "REWARD -202.0\n",
      "EPSIODE# 251\n",
      "MEAN POLICY LOSS tensor(10.8723, grad_fn=<AddBackward0>)\n",
      "REWARD -181.0\n",
      "EPSIODE# 252\n",
      "MEAN POLICY LOSS tensor(3899.9451, grad_fn=<AddBackward0>)\n",
      "REWARD -175.0\n",
      "EPSIODE# 253\n",
      "MEAN POLICY LOSS tensor(43.4811, grad_fn=<AddBackward0>)\n",
      "REWARD -200.0\n",
      "EPSIODE# 254\n",
      "MEAN POLICY LOSS tensor(189.3138, grad_fn=<AddBackward0>)\n",
      "REWARD -166.0\n",
      "EPSIODE# 255\n",
      "MEAN POLICY LOSS tensor(362.4581, grad_fn=<AddBackward0>)\n",
      "REWARD -187.0\n",
      "EPSIODE# 256\n",
      "MEAN POLICY LOSS tensor(481.3812, grad_fn=<AddBackward0>)\n",
      "REWARD -212.0\n",
      "EPSIODE# 257\n",
      "MEAN POLICY LOSS tensor(209.8661, grad_fn=<AddBackward0>)\n",
      "REWARD -206.0\n",
      "EPSIODE# 258\n",
      "MEAN POLICY LOSS tensor(0.7665, grad_fn=<AddBackward0>)\n",
      "REWARD -166.0\n",
      "EPSIODE# 259\n",
      "MEAN POLICY LOSS tensor(3385.3931, grad_fn=<AddBackward0>)\n",
      "REWARD -158.0\n",
      "EPSIODE# 260\n",
      "MEAN POLICY LOSS tensor(29.2555, grad_fn=<AddBackward0>)\n",
      "REWARD -172.0\n",
      "EPSIODE# 261\n",
      "MEAN POLICY LOSS tensor(0.0215, grad_fn=<AddBackward0>)\n",
      "REWARD -192.0\n",
      "EPSIODE# 262\n",
      "MEAN POLICY LOSS tensor(34.5565, grad_fn=<AddBackward0>)\n",
      "REWARD -154.0\n",
      "EPSIODE# 263\n",
      "MEAN POLICY LOSS tensor(77.0156, grad_fn=<AddBackward0>)\n",
      "REWARD -159.0\n",
      "EPSIODE# 264\n",
      "MEAN POLICY LOSS tensor(8586.9258, grad_fn=<AddBackward0>)\n",
      "REWARD -262.0\n",
      "EPSIODE# 265\n",
      "MEAN POLICY LOSS tensor(1.6233, grad_fn=<AddBackward0>)\n",
      "REWARD -162.0\n",
      "EPSIODE# 266\n",
      "MEAN POLICY LOSS tensor(101.2793, grad_fn=<AddBackward0>)\n",
      "REWARD -143.0\n",
      "EPSIODE# 267\n",
      "MEAN POLICY LOSS tensor(3594.4399, grad_fn=<AddBackward0>)\n",
      "REWARD -191.0\n",
      "EPSIODE# 268\n",
      "MEAN POLICY LOSS tensor(13822.5283, grad_fn=<AddBackward0>)\n",
      "REWARD -165.0\n",
      "EPSIODE# 269\n",
      "MEAN POLICY LOSS tensor(2571.8027, grad_fn=<AddBackward0>)\n",
      "REWARD -178.0\n",
      "EPSIODE# 270\n",
      "MEAN POLICY LOSS tensor(40.5257, grad_fn=<AddBackward0>)\n",
      "REWARD -148.0\n",
      "EPSIODE# 271\n",
      "MEAN POLICY LOSS tensor(1445.6768, grad_fn=<AddBackward0>)\n",
      "REWARD -209.0\n",
      "EPSIODE# 272\n",
      "MEAN POLICY LOSS tensor(27.9956, grad_fn=<AddBackward0>)\n",
      "REWARD -154.0\n",
      "EPSIODE# 273\n",
      "MEAN POLICY LOSS tensor(212.5709, grad_fn=<AddBackward0>)\n",
      "REWARD -193.0\n",
      "EPSIODE# 274\n",
      "MEAN POLICY LOSS tensor(268.2877, grad_fn=<AddBackward0>)\n",
      "REWARD -142.0\n",
      "EPSIODE# 275\n",
      "MEAN POLICY LOSS tensor(18038.6152, grad_fn=<AddBackward0>)\n",
      "REWARD -161.0\n",
      "EPSIODE# 276\n",
      "MEAN POLICY LOSS tensor(19.3876, grad_fn=<AddBackward0>)\n",
      "REWARD -133.0\n",
      "EPSIODE# 277\n",
      "MEAN POLICY LOSS tensor(253.2682, grad_fn=<AddBackward0>)\n",
      "REWARD -166.0\n",
      "EPSIODE# 278\n",
      "MEAN POLICY LOSS tensor(2.5962, grad_fn=<AddBackward0>)\n",
      "REWARD -177.0\n",
      "EPSIODE# 279\n",
      "MEAN POLICY LOSS tensor(0.3599, grad_fn=<AddBackward0>)\n",
      "REWARD -199.0\n",
      "EPSIODE# 280\n",
      "MEAN POLICY LOSS tensor(783.0896, grad_fn=<AddBackward0>)\n",
      "REWARD -150.0\n",
      "EPSIODE# 281\n",
      "MEAN POLICY LOSS tensor(121.8904, grad_fn=<AddBackward0>)\n",
      "REWARD -165.0\n",
      "EPSIODE# 282\n",
      "MEAN POLICY LOSS tensor(0., grad_fn=<AddBackward0>)\n",
      "REWARD -228.0\n",
      "EPSIODE# 283\n",
      "MEAN POLICY LOSS tensor(77.3934, grad_fn=<AddBackward0>)\n",
      "REWARD -129.0\n",
      "EPSIODE# 284\n",
      "MEAN POLICY LOSS tensor(32.5715, grad_fn=<AddBackward0>)\n",
      "REWARD -184.0\n",
      "EPSIODE# 285\n",
      "MEAN POLICY LOSS tensor(334.8668, grad_fn=<AddBackward0>)\n",
      "REWARD -155.0\n",
      "EPSIODE# 286\n",
      "MEAN POLICY LOSS tensor(0.0728, grad_fn=<AddBackward0>)\n",
      "REWARD -207.0\n",
      "EPSIODE# 287\n",
      "MEAN POLICY LOSS tensor(38.6527, grad_fn=<AddBackward0>)\n",
      "REWARD -169.0\n",
      "EPSIODE# 288\n",
      "MEAN POLICY LOSS tensor(7.9124, grad_fn=<AddBackward0>)\n",
      "REWARD -185.0\n",
      "EPSIODE# 289\n",
      "MEAN POLICY LOSS tensor(206.3986, grad_fn=<AddBackward0>)\n",
      "REWARD -129.0\n",
      "EPSIODE# 290\n",
      "MEAN POLICY LOSS tensor(74.1071, grad_fn=<AddBackward0>)\n",
      "REWARD -170.0\n",
      "EPSIODE# 291\n",
      "MEAN POLICY LOSS tensor(615.2379, grad_fn=<AddBackward0>)\n",
      "REWARD -233.0\n",
      "EPSIODE# 292\n",
      "MEAN POLICY LOSS tensor(360.6349, grad_fn=<AddBackward0>)\n",
      "REWARD -172.0\n",
      "EPSIODE# 293\n",
      "MEAN POLICY LOSS tensor(29.9114, grad_fn=<AddBackward0>)\n",
      "REWARD -161.0\n",
      "EPSIODE# 294\n",
      "MEAN POLICY LOSS tensor(574.1592, grad_fn=<AddBackward0>)\n",
      "REWARD -156.0\n",
      "EPSIODE# 295\n",
      "MEAN POLICY LOSS tensor(0.4629, grad_fn=<AddBackward0>)\n",
      "REWARD -160.0\n",
      "EPSIODE# 296\n",
      "MEAN POLICY LOSS tensor(0.1510, grad_fn=<AddBackward0>)\n",
      "REWARD -311.0\n",
      "EPSIODE# 297\n",
      "MEAN POLICY LOSS tensor(1.0885, grad_fn=<AddBackward0>)\n",
      "REWARD -153.0\n",
      "EPSIODE# 298\n",
      "MEAN POLICY LOSS tensor(3.7986, grad_fn=<AddBackward0>)\n",
      "REWARD -174.0\n",
      "EPSIODE# 299\n",
      "MEAN POLICY LOSS tensor(68.1698, grad_fn=<AddBackward0>)\n",
      "REWARD -119.0\n",
      "EPSIODE# 300\n",
      "MEAN POLICY LOSS tensor(0.0090, grad_fn=<AddBackward0>)\n",
      "REWARD -316.0\n",
      "EPSIODE# 301\n",
      "MEAN POLICY LOSS tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "REWARD -282.0\n",
      "EPSIODE# 302\n",
      "MEAN POLICY LOSS tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "REWARD -169.0\n",
      "EPSIODE# 303\n",
      "MEAN POLICY LOSS tensor(10.9860, grad_fn=<AddBackward0>)\n",
      "REWARD -161.0\n",
      "EPSIODE# 304\n",
      "MEAN POLICY LOSS tensor(24.1990, grad_fn=<AddBackward0>)\n",
      "REWARD -116.0\n",
      "EPSIODE# 305\n",
      "MEAN POLICY LOSS tensor(22.6475, grad_fn=<AddBackward0>)\n",
      "REWARD -148.0\n",
      "EPSIODE# 306\n",
      "MEAN POLICY LOSS tensor(1287.3842, grad_fn=<AddBackward0>)\n",
      "REWARD -158.0\n",
      "EPSIODE# 307\n",
      "MEAN POLICY LOSS tensor(0.0542, grad_fn=<AddBackward0>)\n",
      "REWARD -185.0\n",
      "EPSIODE# 308\n",
      "MEAN POLICY LOSS tensor(0.0220, grad_fn=<AddBackward0>)\n",
      "REWARD -190.0\n",
      "EPSIODE# 309\n",
      "MEAN POLICY LOSS tensor(4344.6187, grad_fn=<AddBackward0>)\n",
      "REWARD -153.0\n",
      "EPSIODE# 310\n",
      "MEAN POLICY LOSS tensor(19.9619, grad_fn=<AddBackward0>)\n",
      "REWARD -180.0\n",
      "EPSIODE# 311\n",
      "MEAN POLICY LOSS tensor(3.4247, grad_fn=<AddBackward0>)\n",
      "REWARD -174.0\n",
      "EPSIODE# 312\n",
      "MEAN POLICY LOSS tensor(1700.6958, grad_fn=<AddBackward0>)\n",
      "REWARD -179.0\n",
      "EPSIODE# 313\n",
      "MEAN POLICY LOSS tensor(1359.8427, grad_fn=<AddBackward0>)\n",
      "REWARD -175.0\n",
      "EPSIODE# 314\n",
      "MEAN POLICY LOSS tensor(104.9570, grad_fn=<AddBackward0>)\n",
      "REWARD -158.0\n",
      "EPSIODE# 315\n",
      "MEAN POLICY LOSS tensor(119.0746, grad_fn=<AddBackward0>)\n",
      "REWARD -195.0\n",
      "EPSIODE# 316\n",
      "MEAN POLICY LOSS tensor(12.4348, grad_fn=<AddBackward0>)\n",
      "REWARD -150.0\n",
      "EPSIODE# 317\n",
      "MEAN POLICY LOSS tensor(10.2269, grad_fn=<AddBackward0>)\n",
      "REWARD -177.0\n",
      "EPSIODE# 318\n",
      "MEAN POLICY LOSS tensor(1.5175, grad_fn=<AddBackward0>)\n",
      "REWARD -179.0\n",
      "EPSIODE# 319\n",
      "MEAN POLICY LOSS tensor(18.9062, grad_fn=<AddBackward0>)\n",
      "REWARD -213.0\n",
      "EPSIODE# 320\n",
      "MEAN POLICY LOSS tensor(1204.3654, grad_fn=<AddBackward0>)\n",
      "REWARD -205.0\n",
      "EPSIODE# 321\n",
      "MEAN POLICY LOSS tensor(124.3171, grad_fn=<AddBackward0>)\n",
      "REWARD -184.0\n",
      "EPSIODE# 322\n",
      "MEAN POLICY LOSS tensor(203.6453, grad_fn=<AddBackward0>)\n",
      "REWARD -180.0\n",
      "EPSIODE# 323\n",
      "MEAN POLICY LOSS tensor(12.6210, grad_fn=<AddBackward0>)\n",
      "REWARD -150.0\n",
      "EPSIODE# 324\n",
      "MEAN POLICY LOSS tensor(3.3137, grad_fn=<AddBackward0>)\n",
      "REWARD -188.0\n",
      "EPSIODE# 325\n",
      "MEAN POLICY LOSS tensor(0.0626, grad_fn=<AddBackward0>)\n",
      "REWARD -195.0\n",
      "EPSIODE# 326\n",
      "MEAN POLICY LOSS tensor(0.3663, grad_fn=<AddBackward0>)\n",
      "REWARD -155.0\n",
      "EPSIODE# 327\n",
      "MEAN POLICY LOSS tensor(2.3897, grad_fn=<AddBackward0>)\n",
      "REWARD -133.0\n",
      "EPSIODE# 328\n",
      "MEAN POLICY LOSS tensor(2.9844, grad_fn=<AddBackward0>)\n",
      "REWARD -158.0\n",
      "EPSIODE# 329\n",
      "MEAN POLICY LOSS tensor(1.0372, grad_fn=<AddBackward0>)\n",
      "REWARD -168.0\n",
      "EPSIODE# 330\n",
      "MEAN POLICY LOSS tensor(0.0839, grad_fn=<AddBackward0>)\n",
      "REWARD -205.0\n",
      "EPSIODE# 331\n",
      "MEAN POLICY LOSS tensor(7.3406, grad_fn=<AddBackward0>)\n",
      "REWARD -156.0\n",
      "EPSIODE# 332\n",
      "MEAN POLICY LOSS tensor(1.3723, grad_fn=<AddBackward0>)\n",
      "REWARD -164.0\n",
      "EPSIODE# 333\n",
      "MEAN POLICY LOSS tensor(78.4172, grad_fn=<AddBackward0>)\n",
      "REWARD -162.0\n",
      "EPSIODE# 334\n",
      "MEAN POLICY LOSS tensor(0.2766, grad_fn=<AddBackward0>)\n",
      "REWARD -186.0\n",
      "EPSIODE# 335\n",
      "MEAN POLICY LOSS tensor(204.6622, grad_fn=<AddBackward0>)\n",
      "REWARD -168.0\n",
      "EPSIODE# 336\n",
      "MEAN POLICY LOSS tensor(0.0431, grad_fn=<AddBackward0>)\n",
      "REWARD -170.0\n",
      "EPSIODE# 337\n",
      "MEAN POLICY LOSS tensor(0.0110, grad_fn=<AddBackward0>)\n",
      "REWARD -169.0\n",
      "EPSIODE# 338\n",
      "MEAN POLICY LOSS tensor(9.0173, grad_fn=<AddBackward0>)\n",
      "REWARD -186.0\n",
      "EPSIODE# 339\n",
      "MEAN POLICY LOSS tensor(55.5428, grad_fn=<AddBackward0>)\n",
      "REWARD -148.0\n",
      "EPSIODE# 340\n",
      "MEAN POLICY LOSS tensor(1.2428, grad_fn=<AddBackward0>)\n",
      "REWARD -167.0\n",
      "EPSIODE# 341\n",
      "MEAN POLICY LOSS tensor(0.3491, grad_fn=<AddBackward0>)\n",
      "REWARD -174.0\n",
      "EPSIODE# 342\n",
      "MEAN POLICY LOSS tensor(30.2515, grad_fn=<AddBackward0>)\n",
      "REWARD -199.0\n",
      "EPSIODE# 343\n",
      "MEAN POLICY LOSS tensor(0., grad_fn=<AddBackward0>)\n",
      "REWARD -287.0\n",
      "EPSIODE# 344\n",
      "MEAN POLICY LOSS tensor(0.0021, grad_fn=<AddBackward0>)\n",
      "REWARD -197.0\n",
      "EPSIODE# 345\n",
      "MEAN POLICY LOSS tensor(2.2268, grad_fn=<AddBackward0>)\n",
      "REWARD -162.0\n",
      "EPSIODE# 346\n",
      "MEAN POLICY LOSS tensor(413.6675, grad_fn=<AddBackward0>)\n",
      "REWARD -172.0\n",
      "EPSIODE# 347\n",
      "MEAN POLICY LOSS tensor(17.0448, grad_fn=<AddBackward0>)\n",
      "REWARD -174.0\n",
      "EPSIODE# 348\n",
      "MEAN POLICY LOSS tensor(10.2478, grad_fn=<AddBackward0>)\n",
      "REWARD -140.0\n",
      "EPSIODE# 349\n",
      "MEAN POLICY LOSS tensor(23.6296, grad_fn=<AddBackward0>)\n",
      "REWARD -289.0\n",
      "EPSIODE# 350\n",
      "MEAN POLICY LOSS tensor(944.2025, grad_fn=<AddBackward0>)\n",
      "REWARD -180.0\n",
      "EPSIODE# 351\n",
      "MEAN POLICY LOSS tensor(0.6936, grad_fn=<AddBackward0>)\n",
      "REWARD -153.0\n",
      "EPSIODE# 352\n",
      "MEAN POLICY LOSS tensor(5.4255, grad_fn=<AddBackward0>)\n",
      "REWARD -131.0\n",
      "EPSIODE# 353\n",
      "MEAN POLICY LOSS tensor(513.4380, grad_fn=<AddBackward0>)\n",
      "REWARD -179.0\n",
      "EPSIODE# 354\n",
      "MEAN POLICY LOSS tensor(0.5286, grad_fn=<AddBackward0>)\n",
      "REWARD -171.0\n",
      "EPSIODE# 355\n",
      "MEAN POLICY LOSS tensor(1.0326, grad_fn=<AddBackward0>)\n",
      "REWARD -167.0\n",
      "EPSIODE# 356\n",
      "MEAN POLICY LOSS tensor(8.6624, grad_fn=<AddBackward0>)\n",
      "REWARD -153.0\n",
      "EPSIODE# 357\n",
      "MEAN POLICY LOSS tensor(0.1313, grad_fn=<AddBackward0>)\n",
      "REWARD -153.0\n",
      "EPSIODE# 358\n",
      "MEAN POLICY LOSS tensor(109.9062, grad_fn=<AddBackward0>)\n",
      "REWARD -116.0\n",
      "EPSIODE# 359\n",
      "MEAN POLICY LOSS tensor(0.1322, grad_fn=<AddBackward0>)\n",
      "REWARD -214.0\n",
      "EPSIODE# 360\n",
      "MEAN POLICY LOSS tensor(67.1039, grad_fn=<AddBackward0>)\n",
      "REWARD -204.0\n",
      "EPSIODE# 361\n",
      "MEAN POLICY LOSS tensor(0.6294, grad_fn=<AddBackward0>)\n",
      "REWARD -141.0\n",
      "EPSIODE# 362\n",
      "MEAN POLICY LOSS tensor(0.2174, grad_fn=<AddBackward0>)\n",
      "REWARD -174.0\n",
      "EPSIODE# 363\n",
      "MEAN POLICY LOSS tensor(363.6704, grad_fn=<AddBackward0>)\n",
      "REWARD -161.0\n",
      "EPSIODE# 364\n",
      "MEAN POLICY LOSS tensor(8.7269, grad_fn=<AddBackward0>)\n",
      "REWARD -159.0\n",
      "EPSIODE# 365\n",
      "MEAN POLICY LOSS tensor(2001.2961, grad_fn=<AddBackward0>)\n",
      "REWARD -159.0\n",
      "EPSIODE# 366\n",
      "MEAN POLICY LOSS tensor(0.2864, grad_fn=<AddBackward0>)\n",
      "REWARD -148.0\n",
      "EPSIODE# 367\n",
      "MEAN POLICY LOSS tensor(552.1793, grad_fn=<AddBackward0>)\n",
      "REWARD -166.0\n",
      "EPSIODE# 368\n",
      "MEAN POLICY LOSS tensor(714.3207, grad_fn=<AddBackward0>)\n",
      "REWARD -151.0\n",
      "EPSIODE# 369\n",
      "MEAN POLICY LOSS tensor(10.0439, grad_fn=<AddBackward0>)\n",
      "REWARD -128.0\n",
      "EPSIODE# 370\n",
      "MEAN POLICY LOSS tensor(3.6566, grad_fn=<AddBackward0>)\n",
      "REWARD -153.0\n",
      "EPSIODE# 371\n",
      "MEAN POLICY LOSS tensor(3.4420, grad_fn=<AddBackward0>)\n",
      "REWARD -167.0\n",
      "EPSIODE# 372\n",
      "MEAN POLICY LOSS tensor(4.7930, grad_fn=<AddBackward0>)\n",
      "REWARD -350.0\n",
      "EPSIODE# 373\n",
      "MEAN POLICY LOSS tensor(0.0031, grad_fn=<AddBackward0>)\n",
      "REWARD -218.0\n",
      "EPSIODE# 374\n",
      "MEAN POLICY LOSS tensor(0.1866, grad_fn=<AddBackward0>)\n",
      "REWARD -152.0\n",
      "EPSIODE# 375\n",
      "MEAN POLICY LOSS tensor(7.8474, grad_fn=<AddBackward0>)\n",
      "REWARD -329.0\n",
      "EPSIODE# 376\n",
      "MEAN POLICY LOSS tensor(0.6146, grad_fn=<AddBackward0>)\n",
      "REWARD -149.0\n",
      "EPSIODE# 377\n",
      "MEAN POLICY LOSS tensor(44.4258, grad_fn=<AddBackward0>)\n",
      "REWARD -183.0\n",
      "EPSIODE# 378\n",
      "MEAN POLICY LOSS tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "REWARD -234.0\n",
      "EPSIODE# 379\n",
      "MEAN POLICY LOSS tensor(3.7169, grad_fn=<AddBackward0>)\n",
      "REWARD -154.0\n",
      "EPSIODE# 380\n",
      "MEAN POLICY LOSS tensor(0.4031, grad_fn=<AddBackward0>)\n",
      "REWARD -158.0\n",
      "EPSIODE# 381\n",
      "MEAN POLICY LOSS tensor(7.4357, grad_fn=<AddBackward0>)\n",
      "REWARD -124.0\n",
      "EPSIODE# 382\n",
      "MEAN POLICY LOSS tensor(0.5676, grad_fn=<AddBackward0>)\n",
      "REWARD -155.0\n",
      "EPSIODE# 383\n",
      "MEAN POLICY LOSS tensor(3.3437, grad_fn=<AddBackward0>)\n",
      "REWARD -157.0\n",
      "EPSIODE# 384\n",
      "MEAN POLICY LOSS tensor(1027.7751, grad_fn=<AddBackward0>)\n",
      "REWARD -141.0\n",
      "EPSIODE# 385\n",
      "MEAN POLICY LOSS tensor(0.0035, grad_fn=<AddBackward0>)\n",
      "REWARD -191.0\n",
      "EPSIODE# 386\n",
      "MEAN POLICY LOSS tensor(0.5333, grad_fn=<AddBackward0>)\n",
      "REWARD -137.0\n",
      "EPSIODE# 387\n",
      "MEAN POLICY LOSS tensor(0., grad_fn=<AddBackward0>)\n",
      "REWARD -177.0\n",
      "EPSIODE# 388\n",
      "MEAN POLICY LOSS tensor(4.1332, grad_fn=<AddBackward0>)\n",
      "REWARD -156.0\n",
      "EPSIODE# 389\n",
      "MEAN POLICY LOSS tensor(5.5246, grad_fn=<AddBackward0>)\n",
      "REWARD -275.0\n",
      "EPSIODE# 390\n",
      "MEAN POLICY LOSS tensor(1726.4602, grad_fn=<AddBackward0>)\n",
      "REWARD -165.0\n",
      "EPSIODE# 391\n",
      "MEAN POLICY LOSS tensor(0.0023, grad_fn=<AddBackward0>)\n",
      "REWARD -160.0\n",
      "EPSIODE# 392\n",
      "MEAN POLICY LOSS tensor(0.0759, grad_fn=<AddBackward0>)\n",
      "REWARD -210.0\n",
      "EPSIODE# 393\n",
      "MEAN POLICY LOSS tensor(0.2043, grad_fn=<AddBackward0>)\n",
      "REWARD -149.0\n",
      "EPSIODE# 394\n",
      "MEAN POLICY LOSS tensor(0.4242, grad_fn=<AddBackward0>)\n",
      "REWARD -168.0\n",
      "EPSIODE# 395\n",
      "MEAN POLICY LOSS tensor(8.3772, grad_fn=<AddBackward0>)\n",
      "REWARD -139.0\n",
      "EPSIODE# 396\n",
      "MEAN POLICY LOSS tensor(53.1841, grad_fn=<AddBackward0>)\n",
      "REWARD -187.0\n",
      "EPSIODE# 397\n",
      "MEAN POLICY LOSS tensor(1.3410, grad_fn=<AddBackward0>)\n",
      "REWARD -141.0\n",
      "EPSIODE# 398\n",
      "MEAN POLICY LOSS tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "REWARD -167.0\n",
      "EPSIODE# 399\n",
      "MEAN POLICY LOSS tensor(0.5515, grad_fn=<AddBackward0>)\n",
      "REWARD -191.0\n",
      "EPSIODE# 400\n",
      "MEAN POLICY LOSS tensor(5.0650, grad_fn=<AddBackward0>)\n",
      "REWARD -129.0\n",
      "EPSIODE# 401\n",
      "MEAN POLICY LOSS tensor(6.6212, grad_fn=<AddBackward0>)\n",
      "REWARD -119.0\n",
      "EPSIODE# 402\n",
      "MEAN POLICY LOSS tensor(75.7073, grad_fn=<AddBackward0>)\n",
      "REWARD -152.0\n",
      "EPSIODE# 403\n",
      "MEAN POLICY LOSS tensor(5.4470, grad_fn=<AddBackward0>)\n",
      "REWARD -127.0\n",
      "EPSIODE# 404\n",
      "MEAN POLICY LOSS tensor(0.0316, grad_fn=<AddBackward0>)\n",
      "REWARD -165.0\n",
      "EPSIODE# 405\n",
      "MEAN POLICY LOSS tensor(23.8137, grad_fn=<AddBackward0>)\n",
      "REWARD -238.0\n",
      "EPSIODE# 406\n",
      "MEAN POLICY LOSS tensor(0.9698, grad_fn=<AddBackward0>)\n",
      "REWARD -155.0\n",
      "EPSIODE# 407\n",
      "MEAN POLICY LOSS tensor(0.0274, grad_fn=<AddBackward0>)\n",
      "REWARD -165.0\n",
      "EPSIODE# 408\n",
      "MEAN POLICY LOSS tensor(42.7499, grad_fn=<AddBackward0>)\n",
      "REWARD -156.0\n",
      "EPSIODE# 409\n",
      "MEAN POLICY LOSS tensor(1.4712, grad_fn=<AddBackward0>)\n",
      "REWARD -193.0\n",
      "EPSIODE# 410\n",
      "MEAN POLICY LOSS tensor(0.1740, grad_fn=<AddBackward0>)\n",
      "REWARD -197.0\n",
      "EPSIODE# 411\n",
      "MEAN POLICY LOSS tensor(156.4632, grad_fn=<AddBackward0>)\n",
      "REWARD -174.0\n",
      "EPSIODE# 412\n",
      "MEAN POLICY LOSS tensor(1930.0593, grad_fn=<AddBackward0>)\n",
      "REWARD -142.0\n",
      "EPSIODE# 413\n",
      "MEAN POLICY LOSS tensor(0.3881, grad_fn=<AddBackward0>)\n",
      "REWARD -190.0\n",
      "EPSIODE# 414\n",
      "MEAN POLICY LOSS tensor(0.0159, grad_fn=<AddBackward0>)\n",
      "REWARD -201.0\n",
      "EPSIODE# 415\n",
      "MEAN POLICY LOSS tensor(711.3837, grad_fn=<AddBackward0>)\n",
      "REWARD -166.0\n",
      "EPSIODE# 416\n",
      "MEAN POLICY LOSS tensor(96.2280, grad_fn=<AddBackward0>)\n",
      "REWARD -166.0\n",
      "EPSIODE# 417\n",
      "MEAN POLICY LOSS tensor(15.4816, grad_fn=<AddBackward0>)\n",
      "REWARD -141.0\n",
      "EPSIODE# 418\n",
      "MEAN POLICY LOSS tensor(2.1110, grad_fn=<AddBackward0>)\n",
      "REWARD -166.0\n",
      "EPSIODE# 419\n",
      "MEAN POLICY LOSS tensor(0.0071, grad_fn=<AddBackward0>)\n",
      "REWARD -262.0\n",
      "EPSIODE# 420\n",
      "MEAN POLICY LOSS tensor(10048.0361, grad_fn=<AddBackward0>)\n",
      "REWARD -168.0\n",
      "EPSIODE# 421\n",
      "MEAN POLICY LOSS tensor(1.5108, grad_fn=<AddBackward0>)\n",
      "REWARD -175.0\n",
      "EPSIODE# 422\n",
      "MEAN POLICY LOSS tensor(5.0915, grad_fn=<AddBackward0>)\n",
      "REWARD -229.0\n",
      "EPSIODE# 423\n",
      "MEAN POLICY LOSS tensor(0.5345, grad_fn=<AddBackward0>)\n",
      "REWARD -156.0\n",
      "EPSIODE# 424\n",
      "MEAN POLICY LOSS tensor(4.3571, grad_fn=<AddBackward0>)\n",
      "REWARD -291.0\n",
      "EPSIODE# 425\n",
      "MEAN POLICY LOSS tensor(34.2781, grad_fn=<AddBackward0>)\n",
      "REWARD -168.0\n",
      "EPSIODE# 426\n",
      "MEAN POLICY LOSS tensor(56.5614, grad_fn=<AddBackward0>)\n",
      "REWARD -116.0\n",
      "EPSIODE# 427\n",
      "MEAN POLICY LOSS tensor(83.6978, grad_fn=<AddBackward0>)\n",
      "REWARD -156.0\n",
      "EPSIODE# 428\n",
      "MEAN POLICY LOSS tensor(9413.5117, grad_fn=<AddBackward0>)\n",
      "REWARD -178.0\n",
      "EPSIODE# 429\n",
      "MEAN POLICY LOSS tensor(75.9914, grad_fn=<AddBackward0>)\n",
      "REWARD -158.0\n",
      "EPSIODE# 430\n",
      "MEAN POLICY LOSS tensor(9.3692, grad_fn=<AddBackward0>)\n",
      "REWARD -159.0\n",
      "EPSIODE# 431\n",
      "MEAN POLICY LOSS tensor(43.9203, grad_fn=<AddBackward0>)\n",
      "REWARD -193.0\n",
      "EPSIODE# 432\n",
      "MEAN POLICY LOSS tensor(0.2520, grad_fn=<AddBackward0>)\n",
      "REWARD -155.0\n",
      "EPSIODE# 433\n",
      "MEAN POLICY LOSS tensor(4.8196, grad_fn=<AddBackward0>)\n",
      "REWARD -158.0\n",
      "EPSIODE# 434\n",
      "MEAN POLICY LOSS tensor(240.9082, grad_fn=<AddBackward0>)\n",
      "REWARD -122.0\n",
      "EPSIODE# 435\n",
      "MEAN POLICY LOSS tensor(36.3363, grad_fn=<AddBackward0>)\n",
      "REWARD -159.0\n",
      "EPSIODE# 436\n",
      "MEAN POLICY LOSS tensor(4.1840, grad_fn=<AddBackward0>)\n",
      "REWARD -167.0\n",
      "EPSIODE# 437\n",
      "MEAN POLICY LOSS tensor(117.1933, grad_fn=<AddBackward0>)\n",
      "REWARD -179.0\n",
      "EPSIODE# 438\n",
      "MEAN POLICY LOSS tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "REWARD -155.0\n",
      "EPSIODE# 439\n",
      "MEAN POLICY LOSS tensor(8.8135, grad_fn=<AddBackward0>)\n",
      "REWARD -143.0\n",
      "EPSIODE# 440\n",
      "MEAN POLICY LOSS tensor(0.3942, grad_fn=<AddBackward0>)\n",
      "REWARD -172.0\n",
      "EPSIODE# 441\n",
      "MEAN POLICY LOSS tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "REWARD -329.0\n",
      "EPSIODE# 442\n",
      "MEAN POLICY LOSS tensor(0., grad_fn=<AddBackward0>)\n",
      "REWARD -241.0\n",
      "EPSIODE# 443\n",
      "MEAN POLICY LOSS tensor(0.3224, grad_fn=<AddBackward0>)\n",
      "REWARD -214.0\n",
      "EPSIODE# 444\n",
      "MEAN POLICY LOSS tensor(4.1515, grad_fn=<AddBackward0>)\n",
      "REWARD -137.0\n",
      "EPSIODE# 445\n",
      "MEAN POLICY LOSS tensor(1.9352, grad_fn=<AddBackward0>)\n",
      "REWARD -206.0\n",
      "EPSIODE# 446\n",
      "MEAN POLICY LOSS tensor(1624.9359, grad_fn=<AddBackward0>)\n",
      "REWARD -161.0\n",
      "EPSIODE# 447\n",
      "MEAN POLICY LOSS tensor(14.8725, grad_fn=<AddBackward0>)\n",
      "REWARD -165.0\n",
      "EPSIODE# 448\n",
      "MEAN POLICY LOSS tensor(21.3102, grad_fn=<AddBackward0>)\n",
      "REWARD -182.0\n",
      "EPSIODE# 449\n",
      "MEAN POLICY LOSS tensor(31898.5391, grad_fn=<AddBackward0>)\n",
      "REWARD -183.0\n",
      "EPSIODE# 450\n",
      "MEAN POLICY LOSS tensor(423.0226, grad_fn=<AddBackward0>)\n",
      "REWARD -186.0\n",
      "EPSIODE# 451\n",
      "MEAN POLICY LOSS tensor(18147.7207, grad_fn=<AddBackward0>)\n",
      "REWARD -163.0\n",
      "EPSIODE# 452\n",
      "MEAN POLICY LOSS tensor(46.3829, grad_fn=<AddBackward0>)\n",
      "REWARD -177.0\n",
      "EPSIODE# 453\n",
      "MEAN POLICY LOSS tensor(0.0236, grad_fn=<AddBackward0>)\n",
      "REWARD -222.0\n",
      "EPSIODE# 454\n",
      "MEAN POLICY LOSS tensor(30.1024, grad_fn=<AddBackward0>)\n",
      "REWARD -184.0\n",
      "EPSIODE# 455\n",
      "MEAN POLICY LOSS tensor(43.6421, grad_fn=<AddBackward0>)\n",
      "REWARD -156.0\n",
      "EPSIODE# 456\n",
      "MEAN POLICY LOSS tensor(2017.7180, grad_fn=<AddBackward0>)\n",
      "REWARD -168.0\n",
      "EPSIODE# 457\n",
      "MEAN POLICY LOSS tensor(12406.8809, grad_fn=<AddBackward0>)\n",
      "REWARD -154.0\n",
      "EPSIODE# 458\n",
      "MEAN POLICY LOSS tensor(5.0532, grad_fn=<AddBackward0>)\n",
      "REWARD -188.0\n",
      "EPSIODE# 459\n",
      "MEAN POLICY LOSS tensor(1100.6982, grad_fn=<AddBackward0>)\n",
      "REWARD -191.0\n",
      "EPSIODE# 460\n",
      "MEAN POLICY LOSS tensor(0.7362, grad_fn=<AddBackward0>)\n",
      "REWARD -379.0\n",
      "EPSIODE# 461\n",
      "MEAN POLICY LOSS tensor(10.5615, grad_fn=<AddBackward0>)\n",
      "REWARD -172.0\n",
      "EPSIODE# 462\n",
      "MEAN POLICY LOSS tensor(136.5760, grad_fn=<AddBackward0>)\n",
      "REWARD -161.0\n",
      "EPSIODE# 463\n",
      "MEAN POLICY LOSS tensor(0., grad_fn=<AddBackward0>)\n",
      "REWARD -256.0\n",
      "EPSIODE# 464\n",
      "MEAN POLICY LOSS tensor(0.0315, grad_fn=<AddBackward0>)\n",
      "REWARD -246.0\n",
      "EPSIODE# 465\n",
      "MEAN POLICY LOSS tensor(1845.4521, grad_fn=<AddBackward0>)\n",
      "REWARD -188.0\n",
      "EPSIODE# 466\n",
      "MEAN POLICY LOSS tensor(299.2599, grad_fn=<AddBackward0>)\n",
      "REWARD -332.0\n",
      "EPSIODE# 467\n",
      "MEAN POLICY LOSS tensor(708.0616, grad_fn=<AddBackward0>)\n",
      "REWARD -219.0\n",
      "EPSIODE# 468\n",
      "MEAN POLICY LOSS tensor(6.6409, grad_fn=<AddBackward0>)\n",
      "REWARD -152.0\n",
      "EPSIODE# 469\n",
      "MEAN POLICY LOSS tensor(0.4981, grad_fn=<AddBackward0>)\n",
      "REWARD -220.0\n",
      "EPSIODE# 470\n",
      "MEAN POLICY LOSS tensor(598.3455, grad_fn=<AddBackward0>)\n",
      "REWARD -193.0\n",
      "EPSIODE# 471\n",
      "MEAN POLICY LOSS tensor(674.9980, grad_fn=<AddBackward0>)\n",
      "REWARD -213.0\n",
      "EPSIODE# 472\n",
      "MEAN POLICY LOSS tensor(1535.1038, grad_fn=<AddBackward0>)\n",
      "REWARD -167.0\n",
      "EPSIODE# 473\n",
      "MEAN POLICY LOSS tensor(412.8014, grad_fn=<AddBackward0>)\n",
      "REWARD -218.0\n",
      "EPSIODE# 474\n",
      "MEAN POLICY LOSS tensor(236.1770, grad_fn=<AddBackward0>)\n",
      "REWARD -196.0\n",
      "EPSIODE# 475\n",
      "MEAN POLICY LOSS tensor(4557.4009, grad_fn=<AddBackward0>)\n",
      "REWARD -185.0\n",
      "EPSIODE# 476\n",
      "MEAN POLICY LOSS tensor(420.2878, grad_fn=<AddBackward0>)\n",
      "REWARD -276.0\n",
      "EPSIODE# 477\n",
      "MEAN POLICY LOSS tensor(0.7280, grad_fn=<AddBackward0>)\n",
      "REWARD -220.0\n",
      "EPSIODE# 478\n",
      "MEAN POLICY LOSS tensor(0.0140, grad_fn=<AddBackward0>)\n",
      "REWARD -219.0\n",
      "EPSIODE# 479\n",
      "MEAN POLICY LOSS tensor(0.5434, grad_fn=<AddBackward0>)\n",
      "REWARD -260.0\n",
      "EPSIODE# 480\n",
      "MEAN POLICY LOSS tensor(4.4468, grad_fn=<AddBackward0>)\n",
      "REWARD -163.0\n",
      "EPSIODE# 481\n",
      "MEAN POLICY LOSS tensor(100.3257, grad_fn=<AddBackward0>)\n",
      "REWARD -225.0\n",
      "EPSIODE# 482\n",
      "MEAN POLICY LOSS tensor(0.3761, grad_fn=<AddBackward0>)\n",
      "REWARD -263.0\n",
      "EPSIODE# 483\n",
      "MEAN POLICY LOSS tensor(3.0540, grad_fn=<AddBackward0>)\n",
      "REWARD -220.0\n",
      "EPSIODE# 484\n",
      "MEAN POLICY LOSS tensor(1884.7776, grad_fn=<AddBackward0>)\n",
      "REWARD -189.0\n",
      "EPSIODE# 485\n",
      "MEAN POLICY LOSS tensor(32003.9141, grad_fn=<AddBackward0>)\n",
      "REWARD -315.0\n",
      "EPSIODE# 486\n",
      "MEAN POLICY LOSS tensor(66.6218, grad_fn=<AddBackward0>)\n",
      "REWARD -222.0\n",
      "EPSIODE# 487\n",
      "MEAN POLICY LOSS tensor(0.5673, grad_fn=<AddBackward0>)\n",
      "REWARD -218.0\n",
      "EPSIODE# 488\n",
      "MEAN POLICY LOSS tensor(16.1358, grad_fn=<AddBackward0>)\n",
      "REWARD -170.0\n",
      "EPSIODE# 489\n",
      "MEAN POLICY LOSS tensor(10.8015, grad_fn=<AddBackward0>)\n",
      "REWARD -175.0\n",
      "EPSIODE# 490\n",
      "MEAN POLICY LOSS tensor(32.4381, grad_fn=<AddBackward0>)\n",
      "REWARD -175.0\n",
      "EPSIODE# 491\n",
      "MEAN POLICY LOSS tensor(13.7810, grad_fn=<AddBackward0>)\n",
      "REWARD -238.0\n",
      "EPSIODE# 492\n",
      "MEAN POLICY LOSS tensor(8.1320, grad_fn=<AddBackward0>)\n",
      "REWARD -204.0\n",
      "EPSIODE# 493\n",
      "MEAN POLICY LOSS tensor(35.3719, grad_fn=<AddBackward0>)\n",
      "REWARD -191.0\n",
      "EPSIODE# 494\n",
      "MEAN POLICY LOSS tensor(3.7997, grad_fn=<AddBackward0>)\n",
      "REWARD -247.0\n",
      "EPSIODE# 495\n",
      "MEAN POLICY LOSS tensor(1.9613, grad_fn=<AddBackward0>)\n",
      "REWARD -223.0\n",
      "EPSIODE# 496\n",
      "MEAN POLICY LOSS tensor(967.2489, grad_fn=<AddBackward0>)\n",
      "REWARD -164.0\n",
      "EPSIODE# 497\n",
      "MEAN POLICY LOSS tensor(15.9046, grad_fn=<AddBackward0>)\n",
      "REWARD -160.0\n",
      "EPSIODE# 498\n",
      "MEAN POLICY LOSS tensor(79.7367, grad_fn=<AddBackward0>)\n",
      "REWARD -147.0\n",
      "EPSIODE# 499\n",
      "MEAN POLICY LOSS tensor(2061.1216, grad_fn=<AddBackward0>)\n",
      "REWARD -188.0\n",
      "EPSIODE# 500\n",
      "MEAN POLICY LOSS tensor(621.9791, grad_fn=<AddBackward0>)\n",
      "REWARD -227.0\n",
      "EPSIODE# 501\n",
      "MEAN POLICY LOSS tensor(130.5291, grad_fn=<AddBackward0>)\n",
      "REWARD -191.0\n",
      "EPSIODE# 502\n",
      "MEAN POLICY LOSS tensor(3744.5918, grad_fn=<AddBackward0>)\n",
      "REWARD -172.0\n",
      "EPSIODE# 503\n",
      "MEAN POLICY LOSS tensor(1.5141, grad_fn=<AddBackward0>)\n",
      "REWARD -202.0\n",
      "EPSIODE# 504\n",
      "MEAN POLICY LOSS tensor(1.5143, grad_fn=<AddBackward0>)\n",
      "REWARD -287.0\n",
      "EPSIODE# 505\n",
      "MEAN POLICY LOSS tensor(1680.4552, grad_fn=<AddBackward0>)\n",
      "REWARD -182.0\n",
      "EPSIODE# 506\n",
      "MEAN POLICY LOSS tensor(14.9795, grad_fn=<AddBackward0>)\n",
      "REWARD -217.0\n",
      "EPSIODE# 507\n",
      "MEAN POLICY LOSS tensor(1644.6328, grad_fn=<AddBackward0>)\n",
      "REWARD -200.0\n",
      "EPSIODE# 508\n",
      "MEAN POLICY LOSS tensor(3.1664, grad_fn=<AddBackward0>)\n",
      "REWARD -219.0\n",
      "EPSIODE# 509\n",
      "MEAN POLICY LOSS tensor(2011.6260, grad_fn=<AddBackward0>)\n",
      "REWARD -266.0\n",
      "EPSIODE# 510\n",
      "MEAN POLICY LOSS tensor(26.2199, grad_fn=<AddBackward0>)\n",
      "REWARD -267.0\n",
      "EPSIODE# 511\n",
      "MEAN POLICY LOSS tensor(264.3904, grad_fn=<AddBackward0>)\n",
      "REWARD -171.0\n",
      "EPSIODE# 512\n",
      "MEAN POLICY LOSS tensor(1707.4468, grad_fn=<AddBackward0>)\n",
      "REWARD -165.0\n",
      "EPSIODE# 513\n",
      "MEAN POLICY LOSS tensor(4.7130, grad_fn=<AddBackward0>)\n",
      "REWARD -217.0\n",
      "EPSIODE# 514\n",
      "MEAN POLICY LOSS tensor(18382.9219, grad_fn=<AddBackward0>)\n",
      "REWARD -188.0\n",
      "EPSIODE# 515\n",
      "MEAN POLICY LOSS tensor(13816.5918, grad_fn=<AddBackward0>)\n",
      "REWARD -193.0\n",
      "EPSIODE# 516\n",
      "MEAN POLICY LOSS tensor(774.4258, grad_fn=<AddBackward0>)\n",
      "REWARD -267.0\n",
      "EPSIODE# 517\n",
      "MEAN POLICY LOSS tensor(41.9418, grad_fn=<AddBackward0>)\n",
      "REWARD -210.0\n",
      "EPSIODE# 518\n",
      "MEAN POLICY LOSS tensor(87.9028, grad_fn=<AddBackward0>)\n",
      "REWARD -233.0\n",
      "EPSIODE# 519\n",
      "MEAN POLICY LOSS tensor(14697.6006, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 520\n",
      "MEAN POLICY LOSS tensor(2310.7881, grad_fn=<AddBackward0>)\n",
      "REWARD -230.0\n",
      "EPSIODE# 521\n",
      "MEAN POLICY LOSS tensor(12309.2148, grad_fn=<AddBackward0>)\n",
      "REWARD -247.0\n",
      "EPSIODE# 522\n",
      "MEAN POLICY LOSS tensor(10565.0342, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 523\n",
      "MEAN POLICY LOSS tensor(26.0214, grad_fn=<AddBackward0>)\n",
      "REWARD -295.0\n",
      "EPSIODE# 524\n",
      "MEAN POLICY LOSS tensor(3518.1572, grad_fn=<AddBackward0>)\n",
      "REWARD -431.0\n",
      "EPSIODE# 525\n",
      "MEAN POLICY LOSS tensor(2016.9698, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 526\n",
      "MEAN POLICY LOSS tensor(180.0867, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 527\n",
      "MEAN POLICY LOSS tensor(882.2083, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 528\n",
      "MEAN POLICY LOSS tensor(1380.1670, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 529\n",
      "MEAN POLICY LOSS tensor(790.1446, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 530\n",
      "MEAN POLICY LOSS tensor(130.9572, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 531\n",
      "MEAN POLICY LOSS tensor(22.7087, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 532\n",
      "MEAN POLICY LOSS tensor(239.7361, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 533\n",
      "MEAN POLICY LOSS tensor(165.0039, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 534\n",
      "MEAN POLICY LOSS tensor(103.5866, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 535\n",
      "MEAN POLICY LOSS tensor(456.6357, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 536\n",
      "MEAN POLICY LOSS tensor(59.6733, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 537\n",
      "MEAN POLICY LOSS tensor(523.2404, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 538\n",
      "MEAN POLICY LOSS tensor(235.1624, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 539\n",
      "MEAN POLICY LOSS tensor(157.2350, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 540\n",
      "MEAN POLICY LOSS tensor(316.7006, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 541\n",
      "MEAN POLICY LOSS tensor(455.6255, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 542\n",
      "MEAN POLICY LOSS tensor(67.9345, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 543\n",
      "MEAN POLICY LOSS tensor(252.8333, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 544\n",
      "MEAN POLICY LOSS tensor(176.4087, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 545\n",
      "MEAN POLICY LOSS tensor(253.7219, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 546\n",
      "MEAN POLICY LOSS tensor(245.8574, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 547\n",
      "MEAN POLICY LOSS tensor(206.7466, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 548\n",
      "MEAN POLICY LOSS tensor(124.8064, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 549\n",
      "MEAN POLICY LOSS tensor(141.9262, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 550\n",
      "MEAN POLICY LOSS tensor(100.7947, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 551\n",
      "MEAN POLICY LOSS tensor(94.7057, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 552\n",
      "MEAN POLICY LOSS tensor(150.5916, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 553\n",
      "MEAN POLICY LOSS tensor(129.4787, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 554\n",
      "MEAN POLICY LOSS tensor(91.8508, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 555\n",
      "MEAN POLICY LOSS tensor(45.4382, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 556\n",
      "MEAN POLICY LOSS tensor(115.8422, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 557\n",
      "MEAN POLICY LOSS tensor(68.1767, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 558\n",
      "MEAN POLICY LOSS tensor(46.1430, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 559\n",
      "MEAN POLICY LOSS tensor(47.8731, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 560\n",
      "MEAN POLICY LOSS tensor(58.8696, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 561\n",
      "MEAN POLICY LOSS tensor(41.4722, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 562\n",
      "MEAN POLICY LOSS tensor(51.4173, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 563\n",
      "MEAN POLICY LOSS tensor(43.3048, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 564\n",
      "MEAN POLICY LOSS tensor(44.6032, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 565\n",
      "MEAN POLICY LOSS tensor(47.9707, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 566\n",
      "MEAN POLICY LOSS tensor(36.2040, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 567\n",
      "MEAN POLICY LOSS tensor(32.4605, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 568\n",
      "MEAN POLICY LOSS tensor(34.4353, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 569\n",
      "MEAN POLICY LOSS tensor(23.2201, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 570\n",
      "MEAN POLICY LOSS tensor(26.2217, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 571\n",
      "MEAN POLICY LOSS tensor(32.4367, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 572\n",
      "MEAN POLICY LOSS tensor(25.1686, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 573\n",
      "MEAN POLICY LOSS tensor(26.3348, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 574\n",
      "MEAN POLICY LOSS tensor(34.0523, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 575\n",
      "MEAN POLICY LOSS tensor(20.6545, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 576\n",
      "MEAN POLICY LOSS tensor(28.8733, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 577\n",
      "MEAN POLICY LOSS tensor(23.2189, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 578\n",
      "MEAN POLICY LOSS tensor(18.8100, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 579\n",
      "MEAN POLICY LOSS tensor(25.8598, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 580\n",
      "MEAN POLICY LOSS tensor(27.4929, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 581\n",
      "MEAN POLICY LOSS tensor(21.6630, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 582\n",
      "MEAN POLICY LOSS tensor(16.9686, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 583\n",
      "MEAN POLICY LOSS tensor(17.0700, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 584\n",
      "MEAN POLICY LOSS tensor(16.1831, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 585\n",
      "MEAN POLICY LOSS tensor(17.4327, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 586\n",
      "MEAN POLICY LOSS tensor(12.7149, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 587\n",
      "MEAN POLICY LOSS tensor(16.7688, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 588\n",
      "MEAN POLICY LOSS tensor(18.3783, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 589\n",
      "MEAN POLICY LOSS tensor(16.6958, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 590\n",
      "MEAN POLICY LOSS tensor(18.6460, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 591\n",
      "MEAN POLICY LOSS tensor(16.7433, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 592\n",
      "MEAN POLICY LOSS tensor(15.1137, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 593\n",
      "MEAN POLICY LOSS tensor(22.0833, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 594\n",
      "MEAN POLICY LOSS tensor(17.7595, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 595\n",
      "MEAN POLICY LOSS tensor(13.4162, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 596\n",
      "MEAN POLICY LOSS tensor(12.6402, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 597\n",
      "MEAN POLICY LOSS tensor(15.1033, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 598\n",
      "MEAN POLICY LOSS tensor(18.5973, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 599\n",
      "MEAN POLICY LOSS tensor(12.5707, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 600\n",
      "MEAN POLICY LOSS tensor(11.8729, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 601\n",
      "MEAN POLICY LOSS tensor(13.5801, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 602\n",
      "MEAN POLICY LOSS tensor(15.6727, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 603\n",
      "MEAN POLICY LOSS tensor(13.1040, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 604\n",
      "MEAN POLICY LOSS tensor(10.5545, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 605\n",
      "MEAN POLICY LOSS tensor(10.9164, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 606\n",
      "MEAN POLICY LOSS tensor(10.1272, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 607\n",
      "MEAN POLICY LOSS tensor(14.2490, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 608\n",
      "MEAN POLICY LOSS tensor(9.2097, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 609\n",
      "MEAN POLICY LOSS tensor(14.7697, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 610\n",
      "MEAN POLICY LOSS tensor(11.8821, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 611\n",
      "MEAN POLICY LOSS tensor(12.0924, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 612\n",
      "MEAN POLICY LOSS tensor(10.8921, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 613\n",
      "MEAN POLICY LOSS tensor(10.9911, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 614\n",
      "MEAN POLICY LOSS tensor(11.5932, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 615\n",
      "MEAN POLICY LOSS tensor(11.1648, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 616\n",
      "MEAN POLICY LOSS tensor(9.3116, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 617\n",
      "MEAN POLICY LOSS tensor(11.0119, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 618\n",
      "MEAN POLICY LOSS tensor(11.6766, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 619\n",
      "MEAN POLICY LOSS tensor(10.4115, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 620\n",
      "MEAN POLICY LOSS tensor(8.6771, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 621\n",
      "MEAN POLICY LOSS tensor(11.9621, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 622\n",
      "MEAN POLICY LOSS tensor(9.1599, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 623\n",
      "MEAN POLICY LOSS tensor(7.9246, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 624\n",
      "MEAN POLICY LOSS tensor(9.4349, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 625\n",
      "MEAN POLICY LOSS tensor(7.4645, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 626\n",
      "MEAN POLICY LOSS tensor(9.4291, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 627\n",
      "MEAN POLICY LOSS tensor(9.0545, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 628\n",
      "MEAN POLICY LOSS tensor(8.8039, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 629\n",
      "MEAN POLICY LOSS tensor(8.6088, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 630\n",
      "MEAN POLICY LOSS tensor(8.7171, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 631\n",
      "MEAN POLICY LOSS tensor(8.6748, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 632\n",
      "MEAN POLICY LOSS tensor(7.4576, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 633\n",
      "MEAN POLICY LOSS tensor(11.1983, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 634\n",
      "MEAN POLICY LOSS tensor(8.3049, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 635\n",
      "MEAN POLICY LOSS tensor(8.2077, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 636\n",
      "MEAN POLICY LOSS tensor(8.6864, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 637\n",
      "MEAN POLICY LOSS tensor(8.5313, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 638\n",
      "MEAN POLICY LOSS tensor(6.7017, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 639\n",
      "MEAN POLICY LOSS tensor(7.3754, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 640\n",
      "MEAN POLICY LOSS tensor(8.8386, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 641\n",
      "MEAN POLICY LOSS tensor(6.1930, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 642\n",
      "MEAN POLICY LOSS tensor(6.1814, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 643\n",
      "MEAN POLICY LOSS tensor(6.4476, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 644\n",
      "MEAN POLICY LOSS tensor(5.8603, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 645\n",
      "MEAN POLICY LOSS tensor(6.2000, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 646\n",
      "MEAN POLICY LOSS tensor(8.9938, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 647\n",
      "MEAN POLICY LOSS tensor(6.5038, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 648\n",
      "MEAN POLICY LOSS tensor(8.1359, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 649\n",
      "MEAN POLICY LOSS tensor(6.6473, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 650\n",
      "MEAN POLICY LOSS tensor(5.7874, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 651\n",
      "MEAN POLICY LOSS tensor(8.1521, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 652\n",
      "MEAN POLICY LOSS tensor(6.7283, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 653\n",
      "MEAN POLICY LOSS tensor(7.2463, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 654\n",
      "MEAN POLICY LOSS tensor(6.0923, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 655\n",
      "MEAN POLICY LOSS tensor(8.2042, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 656\n",
      "MEAN POLICY LOSS tensor(7.4715, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 657\n",
      "MEAN POLICY LOSS tensor(7.3349, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 658\n",
      "MEAN POLICY LOSS tensor(6.7839, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 659\n",
      "MEAN POLICY LOSS tensor(5.6114, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 660\n",
      "MEAN POLICY LOSS tensor(5.1051, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 661\n",
      "MEAN POLICY LOSS tensor(7.0981, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 662\n",
      "MEAN POLICY LOSS tensor(5.9575, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 663\n",
      "MEAN POLICY LOSS tensor(5.1780, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 664\n",
      "MEAN POLICY LOSS tensor(6.6450, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 665\n",
      "MEAN POLICY LOSS tensor(5.7353, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 666\n",
      "MEAN POLICY LOSS tensor(6.7978, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 667\n",
      "MEAN POLICY LOSS tensor(6.1803, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 668\n",
      "MEAN POLICY LOSS tensor(6.6045, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 669\n",
      "MEAN POLICY LOSS tensor(5.4974, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 670\n",
      "MEAN POLICY LOSS tensor(5.8117, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 671\n",
      "MEAN POLICY LOSS tensor(4.1678, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 672\n",
      "MEAN POLICY LOSS tensor(5.5530, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 673\n",
      "MEAN POLICY LOSS tensor(4.6243, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 674\n",
      "MEAN POLICY LOSS tensor(6.6045, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 675\n",
      "MEAN POLICY LOSS tensor(5.0403, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 676\n",
      "MEAN POLICY LOSS tensor(7.2891, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 677\n",
      "MEAN POLICY LOSS tensor(5.1329, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 678\n",
      "MEAN POLICY LOSS tensor(6.0889, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 679\n",
      "MEAN POLICY LOSS tensor(4.9529, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 680\n",
      "MEAN POLICY LOSS tensor(5.3782, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 681\n",
      "MEAN POLICY LOSS tensor(4.9842, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 682\n",
      "MEAN POLICY LOSS tensor(4.4796, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 683\n",
      "MEAN POLICY LOSS tensor(5.1271, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 684\n",
      "MEAN POLICY LOSS tensor(4.1678, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 685\n",
      "MEAN POLICY LOSS tensor(4.6787, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 686\n",
      "MEAN POLICY LOSS tensor(7.5143, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 687\n",
      "MEAN POLICY LOSS tensor(4.3448, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 688\n",
      "MEAN POLICY LOSS tensor(4.8604, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 689\n",
      "MEAN POLICY LOSS tensor(5.4512, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 690\n",
      "MEAN POLICY LOSS tensor(4.3610, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 691\n",
      "MEAN POLICY LOSS tensor(5.0496, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 692\n",
      "MEAN POLICY LOSS tensor(4.9240, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 693\n",
      "MEAN POLICY LOSS tensor(3.8519, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 694\n",
      "MEAN POLICY LOSS tensor(5.0959, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 695\n",
      "MEAN POLICY LOSS tensor(4.9263, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 696\n",
      "MEAN POLICY LOSS tensor(4.2233, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 697\n",
      "MEAN POLICY LOSS tensor(5.6867, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 698\n",
      "MEAN POLICY LOSS tensor(5.1155, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 699\n",
      "MEAN POLICY LOSS tensor(4.8488, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 700\n",
      "MEAN POLICY LOSS tensor(6.3527, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 701\n",
      "MEAN POLICY LOSS tensor(4.5687, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 702\n",
      "MEAN POLICY LOSS tensor(4.0416, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 703\n",
      "MEAN POLICY LOSS tensor(3.8091, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 704\n",
      "MEAN POLICY LOSS tensor(4.0602, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 705\n",
      "MEAN POLICY LOSS tensor(5.0831, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 706\n",
      "MEAN POLICY LOSS tensor(3.5863, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 707\n",
      "MEAN POLICY LOSS tensor(5.0658, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 708\n",
      "MEAN POLICY LOSS tensor(4.0173, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 709\n",
      "MEAN POLICY LOSS tensor(4.4507, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 710\n",
      "MEAN POLICY LOSS tensor(5.0542, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 711\n",
      "MEAN POLICY LOSS tensor(4.4611, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 712\n",
      "MEAN POLICY LOSS tensor(4.3900, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 713\n",
      "MEAN POLICY LOSS tensor(3.9664, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 714\n",
      "MEAN POLICY LOSS tensor(3.7570, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 715\n",
      "MEAN POLICY LOSS tensor(4.0139, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 716\n",
      "MEAN POLICY LOSS tensor(4.8824, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 717\n",
      "MEAN POLICY LOSS tensor(4.3622, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 718\n",
      "MEAN POLICY LOSS tensor(3.8739, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 719\n",
      "MEAN POLICY LOSS tensor(3.7778, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 720\n",
      "MEAN POLICY LOSS tensor(4.0011, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 721\n",
      "MEAN POLICY LOSS tensor(4.2580, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 722\n",
      "MEAN POLICY LOSS tensor(3.9618, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 723\n",
      "MEAN POLICY LOSS tensor(3.6627, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 724\n",
      "MEAN POLICY LOSS tensor(4.0324, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 725\n",
      "MEAN POLICY LOSS tensor(4.5468, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 726\n",
      "MEAN POLICY LOSS tensor(3.3630, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 727\n",
      "MEAN POLICY LOSS tensor(3.5019, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 728\n",
      "MEAN POLICY LOSS tensor(3.3572, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 729\n",
      "MEAN POLICY LOSS tensor(4.0648, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 730\n",
      "MEAN POLICY LOSS tensor(3.5250, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 731\n",
      "MEAN POLICY LOSS tensor(3.6905, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 732\n",
      "MEAN POLICY LOSS tensor(4.0301, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 733\n",
      "MEAN POLICY LOSS tensor(3.3364, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 734\n",
      "MEAN POLICY LOSS tensor(4.9657, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 735\n",
      "MEAN POLICY LOSS tensor(2.9592, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 736\n",
      "MEAN POLICY LOSS tensor(2.9349, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 737\n",
      "MEAN POLICY LOSS tensor(3.6893, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 738\n",
      "MEAN POLICY LOSS tensor(3.8970, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 739\n",
      "MEAN POLICY LOSS tensor(3.4371, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 740\n",
      "MEAN POLICY LOSS tensor(3.7929, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 741\n",
      "MEAN POLICY LOSS tensor(2.6983, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 742\n",
      "MEAN POLICY LOSS tensor(3.0286, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 743\n",
      "MEAN POLICY LOSS tensor(2.9534, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 744\n",
      "MEAN POLICY LOSS tensor(3.4012, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 745\n",
      "MEAN POLICY LOSS tensor(2.8811, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 746\n",
      "MEAN POLICY LOSS tensor(4.3795, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 747\n",
      "MEAN POLICY LOSS tensor(3.1524, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 748\n",
      "MEAN POLICY LOSS tensor(3.7148, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 749\n",
      "MEAN POLICY LOSS tensor(3.2600, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 750\n",
      "MEAN POLICY LOSS tensor(3.7356, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 751\n",
      "MEAN POLICY LOSS tensor(3.3561, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 752\n",
      "MEAN POLICY LOSS tensor(2.9395, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 753\n",
      "MEAN POLICY LOSS tensor(3.6395, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 754\n",
      "MEAN POLICY LOSS tensor(3.0888, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 755\n",
      "MEAN POLICY LOSS tensor(2.5421, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 756\n",
      "MEAN POLICY LOSS tensor(2.1707, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 757\n",
      "MEAN POLICY LOSS tensor(3.0876, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 758\n",
      "MEAN POLICY LOSS tensor(3.3352, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 759\n",
      "MEAN POLICY LOSS tensor(2.9280, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 760\n",
      "MEAN POLICY LOSS tensor(2.5479, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 761\n",
      "MEAN POLICY LOSS tensor(2.8718, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 762\n",
      "MEAN POLICY LOSS tensor(4.0868, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 763\n",
      "MEAN POLICY LOSS tensor(3.0610, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 764\n",
      "MEAN POLICY LOSS tensor(3.1432, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 765\n",
      "MEAN POLICY LOSS tensor(3.7535, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 766\n",
      "MEAN POLICY LOSS tensor(3.2357, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 767\n",
      "MEAN POLICY LOSS tensor(2.9083, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 768\n",
      "MEAN POLICY LOSS tensor(2.7342, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 769\n",
      "MEAN POLICY LOSS tensor(2.3096, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 770\n",
      "MEAN POLICY LOSS tensor(3.2566, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 771\n",
      "MEAN POLICY LOSS tensor(2.7851, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 772\n",
      "MEAN POLICY LOSS tensor(2.8070, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 773\n",
      "MEAN POLICY LOSS tensor(3.6442, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 774\n",
      "MEAN POLICY LOSS tensor(2.4449, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 775\n",
      "MEAN POLICY LOSS tensor(2.9928, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 776\n",
      "MEAN POLICY LOSS tensor(2.8730, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 777\n",
      "MEAN POLICY LOSS tensor(2.3790, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 778\n",
      "MEAN POLICY LOSS tensor(2.2390, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 779\n",
      "MEAN POLICY LOSS tensor(2.8082, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 780\n",
      "MEAN POLICY LOSS tensor(2.6833, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 781\n",
      "MEAN POLICY LOSS tensor(2.5757, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 782\n",
      "MEAN POLICY LOSS tensor(4.5306, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 783\n",
      "MEAN POLICY LOSS tensor(2.3165, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 784\n",
      "MEAN POLICY LOSS tensor(2.5155, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 785\n",
      "MEAN POLICY LOSS tensor(3.0194, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 786\n",
      "MEAN POLICY LOSS tensor(2.3177, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 787\n",
      "MEAN POLICY LOSS tensor(2.9453, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 788\n",
      "MEAN POLICY LOSS tensor(2.3153, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 789\n",
      "MEAN POLICY LOSS tensor(2.2691, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 790\n",
      "MEAN POLICY LOSS tensor(2.7515, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 791\n",
      "MEAN POLICY LOSS tensor(2.6948, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 792\n",
      "MEAN POLICY LOSS tensor(3.2890, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 793\n",
      "MEAN POLICY LOSS tensor(2.1811, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 794\n",
      "MEAN POLICY LOSS tensor(2.9766, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 795\n",
      "MEAN POLICY LOSS tensor(2.5178, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 796\n",
      "MEAN POLICY LOSS tensor(2.1418, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 797\n",
      "MEAN POLICY LOSS tensor(2.1834, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 798\n",
      "MEAN POLICY LOSS tensor(2.2598, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 799\n",
      "MEAN POLICY LOSS tensor(2.3304, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 800\n",
      "MEAN POLICY LOSS tensor(2.7990, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 801\n",
      "MEAN POLICY LOSS tensor(1.9625, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 802\n",
      "MEAN POLICY LOSS tensor(2.5467, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 803\n",
      "MEAN POLICY LOSS tensor(2.3512, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 804\n",
      "MEAN POLICY LOSS tensor(2.4044, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 805\n",
      "MEAN POLICY LOSS tensor(2.1430, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 806\n",
      "MEAN POLICY LOSS tensor(2.9858, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 807\n",
      "MEAN POLICY LOSS tensor(2.2263, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 808\n",
      "MEAN POLICY LOSS tensor(1.9324, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 809\n",
      "MEAN POLICY LOSS tensor(2.1487, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 810\n",
      "MEAN POLICY LOSS tensor(3.2299, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 811\n",
      "MEAN POLICY LOSS tensor(2.8383, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 812\n",
      "MEAN POLICY LOSS tensor(2.9847, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 813\n",
      "MEAN POLICY LOSS tensor(1.9093, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 814\n",
      "MEAN POLICY LOSS tensor(2.0469, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 815\n",
      "MEAN POLICY LOSS tensor(2.4507, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 816\n",
      "MEAN POLICY LOSS tensor(2.6694, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 817\n",
      "MEAN POLICY LOSS tensor(2.8105, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 818\n",
      "MEAN POLICY LOSS tensor(2.5051, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 819\n",
      "MEAN POLICY LOSS tensor(2.1129, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 820\n",
      "MEAN POLICY LOSS tensor(2.0851, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 821\n",
      "MEAN POLICY LOSS tensor(2.0018, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 822\n",
      "MEAN POLICY LOSS tensor(2.3651, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 823\n",
      "MEAN POLICY LOSS tensor(2.0921, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 824\n",
      "MEAN POLICY LOSS tensor(1.8676, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 825\n",
      "MEAN POLICY LOSS tensor(2.0388, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 826\n",
      "MEAN POLICY LOSS tensor(2.6798, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 827\n",
      "MEAN POLICY LOSS tensor(1.8988, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 828\n",
      "MEAN POLICY LOSS tensor(2.8892, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 829\n",
      "MEAN POLICY LOSS tensor(2.1268, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 830\n",
      "MEAN POLICY LOSS tensor(2.2968, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 831\n",
      "MEAN POLICY LOSS tensor(1.9243, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 832\n",
      "MEAN POLICY LOSS tensor(2.2355, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 833\n",
      "MEAN POLICY LOSS tensor(2.9650, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 834\n",
      "MEAN POLICY LOSS tensor(2.6798, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 835\n",
      "MEAN POLICY LOSS tensor(2.6555, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 836\n",
      "MEAN POLICY LOSS tensor(2.2263, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 837\n",
      "MEAN POLICY LOSS tensor(2.2425, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 838\n",
      "MEAN POLICY LOSS tensor(2.0816, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 839\n",
      "MEAN POLICY LOSS tensor(1.8260, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 840\n",
      "MEAN POLICY LOSS tensor(2.2205, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 841\n",
      "MEAN POLICY LOSS tensor(1.7045, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 842\n",
      "MEAN POLICY LOSS tensor(1.7970, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 843\n",
      "MEAN POLICY LOSS tensor(1.5697, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 844\n",
      "MEAN POLICY LOSS tensor(1.8746, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 845\n",
      "MEAN POLICY LOSS tensor(1.6837, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 846\n",
      "MEAN POLICY LOSS tensor(2.3512, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 847\n",
      "MEAN POLICY LOSS tensor(1.8075, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 848\n",
      "MEAN POLICY LOSS tensor(2.0562, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 849\n",
      "MEAN POLICY LOSS tensor(1.9231, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 850\n",
      "MEAN POLICY LOSS tensor(1.8260, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 851\n",
      "MEAN POLICY LOSS tensor(2.4681, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 852\n",
      "MEAN POLICY LOSS tensor(1.9960, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 853\n",
      "MEAN POLICY LOSS tensor(1.8329, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 854\n",
      "MEAN POLICY LOSS tensor(2.2239, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 855\n",
      "MEAN POLICY LOSS tensor(1.7323, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 856\n",
      "MEAN POLICY LOSS tensor(1.7242, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 857\n",
      "MEAN POLICY LOSS tensor(1.6079, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 858\n",
      "MEAN POLICY LOSS tensor(2.5433, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 859\n",
      "MEAN POLICY LOSS tensor(2.0030, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 860\n",
      "MEAN POLICY LOSS tensor(1.9775, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 861\n",
      "MEAN POLICY LOSS tensor(1.6553, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 862\n",
      "MEAN POLICY LOSS tensor(1.8931, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 863\n",
      "MEAN POLICY LOSS tensor(1.8584, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 864\n",
      "MEAN POLICY LOSS tensor(1.6403, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 865\n",
      "MEAN POLICY LOSS tensor(1.8908, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 866\n",
      "MEAN POLICY LOSS tensor(1.9659, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 867\n",
      "MEAN POLICY LOSS tensor(1.6067, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 868\n",
      "MEAN POLICY LOSS tensor(1.4355, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 869\n",
      "MEAN POLICY LOSS tensor(1.7276, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 870\n",
      "MEAN POLICY LOSS tensor(1.8919, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 871\n",
      "MEAN POLICY LOSS tensor(1.9636, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 872\n",
      "MEAN POLICY LOSS tensor(2.1985, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 873\n",
      "MEAN POLICY LOSS tensor(1.8896, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 874\n",
      "MEAN POLICY LOSS tensor(1.8641, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 875\n",
      "MEAN POLICY LOSS tensor(2.0238, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 876\n",
      "MEAN POLICY LOSS tensor(1.5234, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 877\n",
      "MEAN POLICY LOSS tensor(1.6403, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 878\n",
      "MEAN POLICY LOSS tensor(2.0874, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 879\n",
      "MEAN POLICY LOSS tensor(1.3129, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 880\n",
      "MEAN POLICY LOSS tensor(1.8248, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 881\n",
      "MEAN POLICY LOSS tensor(1.8607, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 882\n",
      "MEAN POLICY LOSS tensor(1.5663, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 883\n",
      "MEAN POLICY LOSS tensor(1.7762, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 884\n",
      "MEAN POLICY LOSS tensor(2.2517, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 885\n",
      "MEAN POLICY LOSS tensor(1.8271, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 886\n",
      "MEAN POLICY LOSS tensor(2.0018, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 887\n",
      "MEAN POLICY LOSS tensor(1.5292, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 888\n",
      "MEAN POLICY LOSS tensor(1.6195, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 889\n",
      "MEAN POLICY LOSS tensor(1.6218, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 890\n",
      "MEAN POLICY LOSS tensor(2.2228, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 891\n",
      "MEAN POLICY LOSS tensor(1.4483, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 892\n",
      "MEAN POLICY LOSS tensor(2.0458, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 893\n",
      "MEAN POLICY LOSS tensor(1.4714, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 894\n",
      "MEAN POLICY LOSS tensor(1.4159, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 895\n",
      "MEAN POLICY LOSS tensor(1.2805, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 896\n",
      "MEAN POLICY LOSS tensor(1.4992, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 897\n",
      "MEAN POLICY LOSS tensor(1.3719, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 898\n",
      "MEAN POLICY LOSS tensor(2.0249, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 899\n",
      "MEAN POLICY LOSS tensor(1.5489, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 900\n",
      "MEAN POLICY LOSS tensor(1.4309, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 901\n",
      "MEAN POLICY LOSS tensor(1.8607, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 902\n",
      "MEAN POLICY LOSS tensor(1.3858, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 903\n",
      "MEAN POLICY LOSS tensor(1.5986, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 904\n",
      "MEAN POLICY LOSS tensor(1.4587, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 905\n",
      "MEAN POLICY LOSS tensor(1.5026, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 906\n",
      "MEAN POLICY LOSS tensor(1.0549, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 907\n",
      "MEAN POLICY LOSS tensor(1.7600, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 908\n",
      "MEAN POLICY LOSS tensor(1.8769, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 909\n",
      "MEAN POLICY LOSS tensor(1.8896, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 910\n",
      "MEAN POLICY LOSS tensor(1.5790, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 911\n",
      "MEAN POLICY LOSS tensor(1.6287, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 912\n",
      "MEAN POLICY LOSS tensor(1.2446, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 913\n",
      "MEAN POLICY LOSS tensor(1.2227, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 914\n",
      "MEAN POLICY LOSS tensor(1.3036, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 915\n",
      "MEAN POLICY LOSS tensor(1.8410, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 916\n",
      "MEAN POLICY LOSS tensor(1.7288, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 917\n",
      "MEAN POLICY LOSS tensor(1.6148, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 918\n",
      "MEAN POLICY LOSS tensor(1.5905, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 919\n",
      "MEAN POLICY LOSS tensor(1.3418, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 920\n",
      "MEAN POLICY LOSS tensor(1.3904, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 921\n",
      "MEAN POLICY LOSS tensor(1.7276, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 922\n",
      "MEAN POLICY LOSS tensor(1.3858, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 923\n",
      "MEAN POLICY LOSS tensor(1.4378, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 924\n",
      "MEAN POLICY LOSS tensor(1.6952, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 925\n",
      "MEAN POLICY LOSS tensor(1.5778, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 926\n",
      "MEAN POLICY LOSS tensor(1.2238, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 927\n",
      "MEAN POLICY LOSS tensor(1.4054, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 928\n",
      "MEAN POLICY LOSS tensor(1.5755, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 929\n",
      "MEAN POLICY LOSS tensor(1.2817, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 930\n",
      "MEAN POLICY LOSS tensor(1.2701, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 931\n",
      "MEAN POLICY LOSS tensor(1.5663, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 932\n",
      "MEAN POLICY LOSS tensor(1.6449, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 933\n",
      "MEAN POLICY LOSS tensor(1.1556, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 934\n",
      "MEAN POLICY LOSS tensor(1.2030, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 935\n",
      "MEAN POLICY LOSS tensor(1.7658, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 936\n",
      "MEAN POLICY LOSS tensor(1.8225, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 937\n",
      "MEAN POLICY LOSS tensor(1.0977, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 938\n",
      "MEAN POLICY LOSS tensor(1.1938, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 939\n",
      "MEAN POLICY LOSS tensor(1.5535, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 940\n",
      "MEAN POLICY LOSS tensor(1.3858, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 941\n",
      "MEAN POLICY LOSS tensor(1.2990, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 942\n",
      "MEAN POLICY LOSS tensor(1.6484, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 943\n",
      "MEAN POLICY LOSS tensor(1.3094, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 944\n",
      "MEAN POLICY LOSS tensor(1.6333, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 945\n",
      "MEAN POLICY LOSS tensor(1.1949, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 946\n",
      "MEAN POLICY LOSS tensor(1.4066, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 947\n",
      "MEAN POLICY LOSS tensor(1.4853, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 948\n",
      "MEAN POLICY LOSS tensor(1.2736, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 949\n",
      "MEAN POLICY LOSS tensor(1.3048, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 950\n",
      "MEAN POLICY LOSS tensor(1.1463, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 951\n",
      "MEAN POLICY LOSS tensor(1.3569, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 952\n",
      "MEAN POLICY LOSS tensor(1.5362, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 953\n",
      "MEAN POLICY LOSS tensor(1.4899, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 954\n",
      "MEAN POLICY LOSS tensor(1.3522, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 955\n",
      "MEAN POLICY LOSS tensor(1.3569, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 956\n",
      "MEAN POLICY LOSS tensor(1.2435, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 957\n",
      "MEAN POLICY LOSS tensor(1.4054, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 958\n",
      "MEAN POLICY LOSS tensor(1.3788, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 959\n",
      "MEAN POLICY LOSS tensor(1.0306, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 960\n",
      "MEAN POLICY LOSS tensor(1.5188, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 961\n",
      "MEAN POLICY LOSS tensor(1.0954, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 962\n",
      "MEAN POLICY LOSS tensor(1.5929, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 963\n",
      "MEAN POLICY LOSS tensor(1.1880, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 964\n",
      "MEAN POLICY LOSS tensor(1.2238, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 965\n",
      "MEAN POLICY LOSS tensor(1.2146, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 966\n",
      "MEAN POLICY LOSS tensor(1.2111, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 967\n",
      "MEAN POLICY LOSS tensor(1.4436, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 968\n",
      "MEAN POLICY LOSS tensor(1.1116, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 969\n",
      "MEAN POLICY LOSS tensor(1.3233, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 970\n",
      "MEAN POLICY LOSS tensor(1.2111, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 971\n",
      "MEAN POLICY LOSS tensor(1.0376, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 972\n",
      "MEAN POLICY LOSS tensor(1.2840, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 973\n",
      "MEAN POLICY LOSS tensor(1.0920, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 974\n",
      "MEAN POLICY LOSS tensor(1.4321, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 975\n",
      "MEAN POLICY LOSS tensor(1.4887, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 976\n",
      "MEAN POLICY LOSS tensor(1.0792, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 977\n",
      "MEAN POLICY LOSS tensor(1.0538, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 978\n",
      "MEAN POLICY LOSS tensor(1.2527, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 979\n",
      "MEAN POLICY LOSS tensor(0.9022, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 980\n",
      "MEAN POLICY LOSS tensor(1.6802, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 981\n",
      "MEAN POLICY LOSS tensor(1.1961, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 982\n",
      "MEAN POLICY LOSS tensor(1.1290, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 983\n",
      "MEAN POLICY LOSS tensor(1.0781, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 984\n",
      "MEAN POLICY LOSS tensor(1.1857, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 985\n",
      "MEAN POLICY LOSS tensor(1.4529, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 986\n",
      "MEAN POLICY LOSS tensor(1.1209, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 987\n",
      "MEAN POLICY LOSS tensor(1.3245, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 988\n",
      "MEAN POLICY LOSS tensor(1.2423, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 989\n",
      "MEAN POLICY LOSS tensor(1.4263, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 990\n",
      "MEAN POLICY LOSS tensor(1.3187, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 991\n",
      "MEAN POLICY LOSS tensor(1.4043, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 992\n",
      "MEAN POLICY LOSS tensor(1.0896, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 993\n",
      "MEAN POLICY LOSS tensor(1.4436, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 994\n",
      "MEAN POLICY LOSS tensor(1.1151, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 995\n",
      "MEAN POLICY LOSS tensor(1.5651, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 996\n",
      "MEAN POLICY LOSS tensor(0.9959, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 997\n",
      "MEAN POLICY LOSS tensor(1.2689, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 998\n",
      "MEAN POLICY LOSS tensor(1.4390, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 999\n",
      "MEAN POLICY LOSS tensor(1.3650, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "Run 4/10\n",
      "EPSIODE# 0\n",
      "MEAN POLICY LOSS tensor(10615.5342, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 1\n",
      "MEAN POLICY LOSS tensor(9756.9131, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 2\n",
      "MEAN POLICY LOSS tensor(8618.7969, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 3\n",
      "MEAN POLICY LOSS tensor(11499.8057, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 4\n",
      "MEAN POLICY LOSS tensor(9088.4219, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 5\n",
      "MEAN POLICY LOSS tensor(5414.2227, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 6\n",
      "MEAN POLICY LOSS tensor(7631.7007, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 7\n",
      "MEAN POLICY LOSS tensor(6681.6592, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 8\n",
      "MEAN POLICY LOSS tensor(10381.7949, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 9\n",
      "MEAN POLICY LOSS tensor(3300.0579, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 10\n",
      "MEAN POLICY LOSS tensor(2666.7939, grad_fn=<AddBackward0>)\n",
      "REWARD -254.0\n",
      "EPSIODE# 11\n",
      "MEAN POLICY LOSS tensor(1681.5493, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 12\n",
      "MEAN POLICY LOSS tensor(6300.2915, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 13\n",
      "MEAN POLICY LOSS tensor(1032.9127, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 14\n",
      "MEAN POLICY LOSS tensor(209.4662, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 15\n",
      "MEAN POLICY LOSS tensor(308.6986, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 16\n",
      "MEAN POLICY LOSS tensor(1135.5651, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 17\n",
      "MEAN POLICY LOSS tensor(80.1247, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 18\n",
      "MEAN POLICY LOSS tensor(2745.5537, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 19\n",
      "MEAN POLICY LOSS tensor(103.5509, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 20\n",
      "MEAN POLICY LOSS tensor(84.1998, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 21\n",
      "MEAN POLICY LOSS tensor(1671.5243, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 22\n",
      "MEAN POLICY LOSS tensor(27608.9043, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 23\n",
      "MEAN POLICY LOSS tensor(36134.2656, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 24\n",
      "MEAN POLICY LOSS tensor(25992.9727, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 25\n",
      "MEAN POLICY LOSS tensor(1625.3484, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 26\n",
      "MEAN POLICY LOSS tensor(2027.3523, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 27\n",
      "MEAN POLICY LOSS tensor(34.4533, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 28\n",
      "MEAN POLICY LOSS tensor(364.3370, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 29\n",
      "MEAN POLICY LOSS tensor(597.3305, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 30\n",
      "MEAN POLICY LOSS tensor(154.5340, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 31\n",
      "MEAN POLICY LOSS tensor(1681.5933, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 32\n",
      "MEAN POLICY LOSS tensor(3494.8235, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 33\n",
      "MEAN POLICY LOSS tensor(230.9340, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 34\n",
      "MEAN POLICY LOSS tensor(22479.9883, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 35\n",
      "MEAN POLICY LOSS tensor(716.2957, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 36\n",
      "MEAN POLICY LOSS tensor(96.4508, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 37\n",
      "MEAN POLICY LOSS tensor(1616.6846, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 38\n",
      "MEAN POLICY LOSS tensor(122.7667, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 39\n",
      "MEAN POLICY LOSS tensor(1355.4918, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 40\n",
      "MEAN POLICY LOSS tensor(3202.0339, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 41\n",
      "MEAN POLICY LOSS tensor(1944.3188, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 42\n",
      "MEAN POLICY LOSS tensor(294.5639, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 43\n",
      "MEAN POLICY LOSS tensor(1504.2014, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 44\n",
      "MEAN POLICY LOSS tensor(1635.1287, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 45\n",
      "MEAN POLICY LOSS tensor(1222.2834, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 46\n",
      "MEAN POLICY LOSS tensor(637.2706, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 47\n",
      "MEAN POLICY LOSS tensor(26440.5547, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 48\n",
      "MEAN POLICY LOSS tensor(176.5948, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 49\n",
      "MEAN POLICY LOSS tensor(1359.8425, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 50\n",
      "MEAN POLICY LOSS tensor(2284.5757, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 51\n",
      "MEAN POLICY LOSS tensor(1226.2201, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 52\n",
      "MEAN POLICY LOSS tensor(40969.1094, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 53\n",
      "MEAN POLICY LOSS tensor(1397.3641, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 54\n",
      "MEAN POLICY LOSS tensor(658.0977, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 55\n",
      "MEAN POLICY LOSS tensor(17930.0566, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 56\n",
      "MEAN POLICY LOSS tensor(1969.9360, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 57\n",
      "MEAN POLICY LOSS tensor(109.8841, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 58\n",
      "MEAN POLICY LOSS tensor(938.3359, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 59\n",
      "MEAN POLICY LOSS tensor(2987.3240, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 60\n",
      "MEAN POLICY LOSS tensor(895.8593, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 61\n",
      "MEAN POLICY LOSS tensor(982.3560, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 62\n",
      "MEAN POLICY LOSS tensor(3593.4578, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 63\n",
      "MEAN POLICY LOSS tensor(2734.8608, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 64\n",
      "MEAN POLICY LOSS tensor(1995.8185, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 65\n",
      "MEAN POLICY LOSS tensor(18199.8594, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 66\n",
      "MEAN POLICY LOSS tensor(17778.4980, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 67\n",
      "MEAN POLICY LOSS tensor(756.0665, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 68\n",
      "MEAN POLICY LOSS tensor(906.5063, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 69\n",
      "MEAN POLICY LOSS tensor(22168.3184, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 70\n",
      "MEAN POLICY LOSS tensor(3385.5293, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 71\n",
      "MEAN POLICY LOSS tensor(16272.2051, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 72\n",
      "MEAN POLICY LOSS tensor(3978.8503, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 73\n",
      "MEAN POLICY LOSS tensor(11232.9453, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 74\n",
      "MEAN POLICY LOSS tensor(4658.3813, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 75\n",
      "MEAN POLICY LOSS tensor(3311.2761, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 76\n",
      "MEAN POLICY LOSS tensor(4386.2988, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 77\n",
      "MEAN POLICY LOSS tensor(18561.0234, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 78\n",
      "MEAN POLICY LOSS tensor(5702.8086, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 79\n",
      "MEAN POLICY LOSS tensor(6073.2861, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 80\n",
      "MEAN POLICY LOSS tensor(5548.1333, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 81\n",
      "MEAN POLICY LOSS tensor(5849.7500, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 82\n",
      "MEAN POLICY LOSS tensor(3884.8806, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 83\n",
      "MEAN POLICY LOSS tensor(5013.2568, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 84\n",
      "MEAN POLICY LOSS tensor(4593.1333, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 85\n",
      "MEAN POLICY LOSS tensor(4989.8813, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 86\n",
      "MEAN POLICY LOSS tensor(1459.5073, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 87\n",
      "MEAN POLICY LOSS tensor(1304.0168, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 88\n",
      "MEAN POLICY LOSS tensor(2190.5442, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 89\n",
      "MEAN POLICY LOSS tensor(3578.1899, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 90\n",
      "MEAN POLICY LOSS tensor(2450.0693, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 91\n",
      "MEAN POLICY LOSS tensor(27606.9805, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 92\n",
      "MEAN POLICY LOSS tensor(15504.0635, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 93\n",
      "MEAN POLICY LOSS tensor(1682.1656, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 94\n",
      "MEAN POLICY LOSS tensor(2512.1304, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 95\n",
      "MEAN POLICY LOSS tensor(24379.1328, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 96\n",
      "MEAN POLICY LOSS tensor(2102.9355, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 97\n",
      "MEAN POLICY LOSS tensor(1970.2765, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 98\n",
      "MEAN POLICY LOSS tensor(15232.8467, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 99\n",
      "MEAN POLICY LOSS tensor(3937.1924, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 100\n",
      "MEAN POLICY LOSS tensor(1966.8479, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 101\n",
      "MEAN POLICY LOSS tensor(343.3602, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 102\n",
      "MEAN POLICY LOSS tensor(3624.5671, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 103\n",
      "MEAN POLICY LOSS tensor(2777.3669, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 104\n",
      "MEAN POLICY LOSS tensor(3089.7903, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 105\n",
      "MEAN POLICY LOSS tensor(14256.2256, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 106\n",
      "MEAN POLICY LOSS tensor(3453.2546, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 107\n",
      "MEAN POLICY LOSS tensor(3042.6750, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 108\n",
      "MEAN POLICY LOSS tensor(2556.5676, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 109\n",
      "MEAN POLICY LOSS tensor(3199.4753, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 110\n",
      "MEAN POLICY LOSS tensor(13478.9287, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 111\n",
      "MEAN POLICY LOSS tensor(12553.1279, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 112\n",
      "MEAN POLICY LOSS tensor(3127.1047, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 113\n",
      "MEAN POLICY LOSS tensor(13054.1025, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 114\n",
      "MEAN POLICY LOSS tensor(3434.8633, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 115\n",
      "MEAN POLICY LOSS tensor(3628.2327, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 116\n",
      "MEAN POLICY LOSS tensor(17261.1406, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 117\n",
      "MEAN POLICY LOSS tensor(11422.2510, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 118\n",
      "MEAN POLICY LOSS tensor(10944.9141, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 119\n",
      "MEAN POLICY LOSS tensor(2114.2822, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 120\n",
      "MEAN POLICY LOSS tensor(5576.8906, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 121\n",
      "MEAN POLICY LOSS tensor(6830.5405, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 122\n",
      "MEAN POLICY LOSS tensor(5113.3657, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 123\n",
      "MEAN POLICY LOSS tensor(8784.9619, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 124\n",
      "MEAN POLICY LOSS tensor(3883.9109, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 125\n",
      "MEAN POLICY LOSS tensor(5101.9893, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 126\n",
      "MEAN POLICY LOSS tensor(4583.6396, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 127\n",
      "MEAN POLICY LOSS tensor(8472.8887, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 128\n",
      "MEAN POLICY LOSS tensor(5068.3286, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 129\n",
      "MEAN POLICY LOSS tensor(5242.5698, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 130\n",
      "MEAN POLICY LOSS tensor(5335.7422, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 131\n",
      "MEAN POLICY LOSS tensor(4920.6694, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 132\n",
      "MEAN POLICY LOSS tensor(39315.6641, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 133\n",
      "MEAN POLICY LOSS tensor(9702.0449, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 134\n",
      "MEAN POLICY LOSS tensor(2772.5452, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 135\n",
      "MEAN POLICY LOSS tensor(2748.9016, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 136\n",
      "MEAN POLICY LOSS tensor(4637.5894, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 137\n",
      "MEAN POLICY LOSS tensor(4124.8677, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 138\n",
      "MEAN POLICY LOSS tensor(11248.5293, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 139\n",
      "MEAN POLICY LOSS tensor(4361.2002, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 140\n",
      "MEAN POLICY LOSS tensor(3650.1316, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 141\n",
      "MEAN POLICY LOSS tensor(2418.9272, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 142\n",
      "MEAN POLICY LOSS tensor(12128.5947, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 143\n",
      "MEAN POLICY LOSS tensor(3290.3630, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 144\n",
      "MEAN POLICY LOSS tensor(3601.6135, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 145\n",
      "MEAN POLICY LOSS tensor(4013.8091, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 146\n",
      "MEAN POLICY LOSS tensor(1731.6565, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 147\n",
      "MEAN POLICY LOSS tensor(2428.4727, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 148\n",
      "MEAN POLICY LOSS tensor(963.1072, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 149\n",
      "MEAN POLICY LOSS tensor(38590.9922, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 150\n",
      "MEAN POLICY LOSS tensor(1104.3199, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 151\n",
      "MEAN POLICY LOSS tensor(2335.5935, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 152\n",
      "MEAN POLICY LOSS tensor(3198.7566, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 153\n",
      "MEAN POLICY LOSS tensor(1289.8408, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 154\n",
      "MEAN POLICY LOSS tensor(2317.9436, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 155\n",
      "MEAN POLICY LOSS tensor(2182.1614, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 156\n",
      "MEAN POLICY LOSS tensor(2949.0330, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 157\n",
      "MEAN POLICY LOSS tensor(2216.0938, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 158\n",
      "MEAN POLICY LOSS tensor(2553.5667, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 159\n",
      "MEAN POLICY LOSS tensor(2472.2969, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 160\n",
      "MEAN POLICY LOSS tensor(921.2557, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 161\n",
      "MEAN POLICY LOSS tensor(2076.4988, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 162\n",
      "MEAN POLICY LOSS tensor(1298.1855, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 163\n",
      "MEAN POLICY LOSS tensor(1665.0099, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 164\n",
      "MEAN POLICY LOSS tensor(19562.1816, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 165\n",
      "MEAN POLICY LOSS tensor(1120.5543, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 166\n",
      "MEAN POLICY LOSS tensor(352.4337, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 167\n",
      "MEAN POLICY LOSS tensor(1397.5060, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 168\n",
      "MEAN POLICY LOSS tensor(1149.7299, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 169\n",
      "MEAN POLICY LOSS tensor(1021.8427, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 170\n",
      "MEAN POLICY LOSS tensor(1399.5563, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 171\n",
      "MEAN POLICY LOSS tensor(710.0710, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 172\n",
      "MEAN POLICY LOSS tensor(977.2911, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 173\n",
      "MEAN POLICY LOSS tensor(24560.0469, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 174\n",
      "MEAN POLICY LOSS tensor(385.6501, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 175\n",
      "MEAN POLICY LOSS tensor(824.8484, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 176\n",
      "MEAN POLICY LOSS tensor(109.9285, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 177\n",
      "MEAN POLICY LOSS tensor(519.8533, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 178\n",
      "MEAN POLICY LOSS tensor(1154.3851, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 179\n",
      "MEAN POLICY LOSS tensor(1116.9163, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 180\n",
      "MEAN POLICY LOSS tensor(1067.1941, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 181\n",
      "MEAN POLICY LOSS tensor(443.0941, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 182\n",
      "MEAN POLICY LOSS tensor(509.7618, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 183\n",
      "MEAN POLICY LOSS tensor(1013.7720, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 184\n",
      "MEAN POLICY LOSS tensor(371.6420, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 185\n",
      "MEAN POLICY LOSS tensor(802.7443, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 186\n",
      "MEAN POLICY LOSS tensor(246.3072, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 187\n",
      "MEAN POLICY LOSS tensor(587.5207, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 188\n",
      "MEAN POLICY LOSS tensor(188.3249, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 189\n",
      "MEAN POLICY LOSS tensor(326.5289, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 190\n",
      "MEAN POLICY LOSS tensor(364.6925, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 191\n",
      "MEAN POLICY LOSS tensor(378.9397, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 192\n",
      "MEAN POLICY LOSS tensor(60.8354, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 193\n",
      "MEAN POLICY LOSS tensor(219.4763, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 194\n",
      "MEAN POLICY LOSS tensor(379.2042, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 195\n",
      "MEAN POLICY LOSS tensor(231.0739, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 196\n",
      "MEAN POLICY LOSS tensor(555.6665, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 197\n",
      "MEAN POLICY LOSS tensor(246.5473, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 198\n",
      "MEAN POLICY LOSS tensor(414.6255, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 199\n",
      "MEAN POLICY LOSS tensor(247.9384, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 200\n",
      "MEAN POLICY LOSS tensor(396.3707, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 201\n",
      "MEAN POLICY LOSS tensor(414.6496, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 202\n",
      "MEAN POLICY LOSS tensor(468.4093, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 203\n",
      "MEAN POLICY LOSS tensor(165.1941, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 204\n",
      "MEAN POLICY LOSS tensor(218.5595, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 205\n",
      "MEAN POLICY LOSS tensor(330.4545, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 206\n",
      "MEAN POLICY LOSS tensor(32856.9453, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 207\n",
      "MEAN POLICY LOSS tensor(296.9707, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 208\n",
      "MEAN POLICY LOSS tensor(500.2393, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 209\n",
      "MEAN POLICY LOSS tensor(287.6133, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 210\n",
      "MEAN POLICY LOSS tensor(568.0319, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 211\n",
      "MEAN POLICY LOSS tensor(10.8029, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 212\n",
      "MEAN POLICY LOSS tensor(455.2084, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 213\n",
      "MEAN POLICY LOSS tensor(163.2995, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 214\n",
      "MEAN POLICY LOSS tensor(653.6454, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 215\n",
      "MEAN POLICY LOSS tensor(746.0076, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 216\n",
      "MEAN POLICY LOSS tensor(284.8877, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 217\n",
      "MEAN POLICY LOSS tensor(371.4527, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 218\n",
      "MEAN POLICY LOSS tensor(274.5166, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 219\n",
      "MEAN POLICY LOSS tensor(563.4149, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 220\n",
      "MEAN POLICY LOSS tensor(350.0788, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 221\n",
      "MEAN POLICY LOSS tensor(748.5195, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 222\n",
      "MEAN POLICY LOSS tensor(37046.5703, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 223\n",
      "MEAN POLICY LOSS tensor(294.3983, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 224\n",
      "MEAN POLICY LOSS tensor(430.9872, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 225\n",
      "MEAN POLICY LOSS tensor(543.4590, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 226\n",
      "MEAN POLICY LOSS tensor(402.4968, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 227\n",
      "MEAN POLICY LOSS tensor(491.2435, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 228\n",
      "MEAN POLICY LOSS tensor(166.9336, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 229\n",
      "MEAN POLICY LOSS tensor(387.1870, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 230\n",
      "MEAN POLICY LOSS tensor(483.6004, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 231\n",
      "MEAN POLICY LOSS tensor(199.2026, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 232\n",
      "MEAN POLICY LOSS tensor(428.6855, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 233\n",
      "MEAN POLICY LOSS tensor(729.3767, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 234\n",
      "MEAN POLICY LOSS tensor(467.7322, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 235\n",
      "MEAN POLICY LOSS tensor(925.9716, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 236\n",
      "MEAN POLICY LOSS tensor(446.7096, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 237\n",
      "MEAN POLICY LOSS tensor(467.0269, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 238\n",
      "MEAN POLICY LOSS tensor(46825.7891, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 239\n",
      "MEAN POLICY LOSS tensor(886.5891, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 240\n",
      "MEAN POLICY LOSS tensor(713.9373, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 241\n",
      "MEAN POLICY LOSS tensor(782.9020, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 242\n",
      "MEAN POLICY LOSS tensor(609.6691, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 243\n",
      "MEAN POLICY LOSS tensor(508.0126, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 244\n",
      "MEAN POLICY LOSS tensor(753.3417, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 245\n",
      "MEAN POLICY LOSS tensor(688.0261, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 246\n",
      "MEAN POLICY LOSS tensor(650.7615, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 247\n",
      "MEAN POLICY LOSS tensor(739.6094, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 248\n",
      "MEAN POLICY LOSS tensor(827.4316, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 249\n",
      "MEAN POLICY LOSS tensor(1326.9967, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 250\n",
      "MEAN POLICY LOSS tensor(832.7468, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 251\n",
      "MEAN POLICY LOSS tensor(619.3781, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 252\n",
      "MEAN POLICY LOSS tensor(779.5657, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 253\n",
      "MEAN POLICY LOSS tensor(706.7875, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 254\n",
      "MEAN POLICY LOSS tensor(316.5610, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 255\n",
      "MEAN POLICY LOSS tensor(643.0565, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 256\n",
      "MEAN POLICY LOSS tensor(663.1500, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 257\n",
      "MEAN POLICY LOSS tensor(291.2251, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 258\n",
      "MEAN POLICY LOSS tensor(645.5405, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 259\n",
      "MEAN POLICY LOSS tensor(87.5519, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 260\n",
      "MEAN POLICY LOSS tensor(407.8101, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 261\n",
      "MEAN POLICY LOSS tensor(436.4883, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 262\n",
      "MEAN POLICY LOSS tensor(427.8136, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 263\n",
      "MEAN POLICY LOSS tensor(383.7051, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 264\n",
      "MEAN POLICY LOSS tensor(580.3085, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 265\n",
      "MEAN POLICY LOSS tensor(499.9974, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 266\n",
      "MEAN POLICY LOSS tensor(281.2635, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 267\n",
      "MEAN POLICY LOSS tensor(196.7118, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 268\n",
      "MEAN POLICY LOSS tensor(223.1263, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 269\n",
      "MEAN POLICY LOSS tensor(231.1484, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 270\n",
      "MEAN POLICY LOSS tensor(200.2553, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 271\n",
      "MEAN POLICY LOSS tensor(329.7869, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 272\n",
      "MEAN POLICY LOSS tensor(204.2962, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 273\n",
      "MEAN POLICY LOSS tensor(123.5843, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 274\n",
      "MEAN POLICY LOSS tensor(276.3410, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 275\n",
      "MEAN POLICY LOSS tensor(82.9865, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 276\n",
      "MEAN POLICY LOSS tensor(154.6652, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 277\n",
      "MEAN POLICY LOSS tensor(271.9848, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 278\n",
      "MEAN POLICY LOSS tensor(230.7821, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 279\n",
      "MEAN POLICY LOSS tensor(109.9017, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 280\n",
      "MEAN POLICY LOSS tensor(183.2905, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 281\n",
      "MEAN POLICY LOSS tensor(131.3814, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 282\n",
      "MEAN POLICY LOSS tensor(113.4165, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 283\n",
      "MEAN POLICY LOSS tensor(93.0216, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 284\n",
      "MEAN POLICY LOSS tensor(173.8141, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 285\n",
      "MEAN POLICY LOSS tensor(204.5634, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 286\n",
      "MEAN POLICY LOSS tensor(139.3712, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 287\n",
      "MEAN POLICY LOSS tensor(105.7672, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 288\n",
      "MEAN POLICY LOSS tensor(146.2206, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 289\n",
      "MEAN POLICY LOSS tensor(164.5628, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 290\n",
      "MEAN POLICY LOSS tensor(75.2860, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 291\n",
      "MEAN POLICY LOSS tensor(118.7890, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 292\n",
      "MEAN POLICY LOSS tensor(19.6073, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 293\n",
      "MEAN POLICY LOSS tensor(86.1008, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 294\n",
      "MEAN POLICY LOSS tensor(53.0302, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 295\n",
      "MEAN POLICY LOSS tensor(77.1360, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 296\n",
      "MEAN POLICY LOSS tensor(59.2700, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 297\n",
      "MEAN POLICY LOSS tensor(22.0178, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 298\n",
      "MEAN POLICY LOSS tensor(32.1053, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 299\n",
      "MEAN POLICY LOSS tensor(36.2202, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 300\n",
      "MEAN POLICY LOSS tensor(57.8730, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 301\n",
      "MEAN POLICY LOSS tensor(90.5559, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 302\n",
      "MEAN POLICY LOSS tensor(45.0547, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 303\n",
      "MEAN POLICY LOSS tensor(52.2178, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 304\n",
      "MEAN POLICY LOSS tensor(52.0074, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 305\n",
      "MEAN POLICY LOSS tensor(107.3108, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 306\n",
      "MEAN POLICY LOSS tensor(50.4068, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 307\n",
      "MEAN POLICY LOSS tensor(83.6148, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 308\n",
      "MEAN POLICY LOSS tensor(24.1186, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 309\n",
      "MEAN POLICY LOSS tensor(86.4432, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 310\n",
      "MEAN POLICY LOSS tensor(51.2859, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 311\n",
      "MEAN POLICY LOSS tensor(102.9799, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 312\n",
      "MEAN POLICY LOSS tensor(57.5972, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 313\n",
      "MEAN POLICY LOSS tensor(48.7327, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 314\n",
      "MEAN POLICY LOSS tensor(68.6731, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 315\n",
      "MEAN POLICY LOSS tensor(59.8780, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 316\n",
      "MEAN POLICY LOSS tensor(22.0810, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 317\n",
      "MEAN POLICY LOSS tensor(54.2474, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 318\n",
      "MEAN POLICY LOSS tensor(48.8775, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 319\n",
      "MEAN POLICY LOSS tensor(94.5877, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 320\n",
      "MEAN POLICY LOSS tensor(24.3865, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 321\n",
      "MEAN POLICY LOSS tensor(61.7875, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 322\n",
      "MEAN POLICY LOSS tensor(72.6815, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 323\n",
      "MEAN POLICY LOSS tensor(45.0146, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 324\n",
      "MEAN POLICY LOSS tensor(45.2761, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 325\n",
      "MEAN POLICY LOSS tensor(73.2899, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 326\n",
      "MEAN POLICY LOSS tensor(50.6556, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 327\n",
      "MEAN POLICY LOSS tensor(39.6753, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 328\n",
      "MEAN POLICY LOSS tensor(58.0499, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 329\n",
      "MEAN POLICY LOSS tensor(51.5690, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 330\n",
      "MEAN POLICY LOSS tensor(49.9953, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 331\n",
      "MEAN POLICY LOSS tensor(38.6307, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 332\n",
      "MEAN POLICY LOSS tensor(41.7521, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 333\n",
      "MEAN POLICY LOSS tensor(29.1581, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 334\n",
      "MEAN POLICY LOSS tensor(42.3754, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 335\n",
      "MEAN POLICY LOSS tensor(39.4738, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 336\n",
      "MEAN POLICY LOSS tensor(65.8019, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 337\n",
      "MEAN POLICY LOSS tensor(39.4384, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 338\n",
      "MEAN POLICY LOSS tensor(58.5351, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 339\n",
      "MEAN POLICY LOSS tensor(37.0579, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 340\n",
      "MEAN POLICY LOSS tensor(55.3077, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 341\n",
      "MEAN POLICY LOSS tensor(51.5737, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 342\n",
      "MEAN POLICY LOSS tensor(51.7557, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 343\n",
      "MEAN POLICY LOSS tensor(12.0854, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 344\n",
      "MEAN POLICY LOSS tensor(48.1939, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 345\n",
      "MEAN POLICY LOSS tensor(30.6253, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 346\n",
      "MEAN POLICY LOSS tensor(19.4862, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 347\n",
      "MEAN POLICY LOSS tensor(43.6894, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 348\n",
      "MEAN POLICY LOSS tensor(45.6015, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 349\n",
      "MEAN POLICY LOSS tensor(55.2902, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 350\n",
      "MEAN POLICY LOSS tensor(25.7525, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 351\n",
      "MEAN POLICY LOSS tensor(29.3177, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 352\n",
      "MEAN POLICY LOSS tensor(50.6120, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 353\n",
      "MEAN POLICY LOSS tensor(40.4529, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 354\n",
      "MEAN POLICY LOSS tensor(38.5314, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 355\n",
      "MEAN POLICY LOSS tensor(31.9040, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 356\n",
      "MEAN POLICY LOSS tensor(10.2656, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 357\n",
      "MEAN POLICY LOSS tensor(17.4570, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 358\n",
      "MEAN POLICY LOSS tensor(21.5616, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 359\n",
      "MEAN POLICY LOSS tensor(27.5300, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 360\n",
      "MEAN POLICY LOSS tensor(36.3990, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 361\n",
      "MEAN POLICY LOSS tensor(48.1550, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 362\n",
      "MEAN POLICY LOSS tensor(52.9912, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 363\n",
      "MEAN POLICY LOSS tensor(37.4370, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 364\n",
      "MEAN POLICY LOSS tensor(13.7486, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 365\n",
      "MEAN POLICY LOSS tensor(37.6872, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 366\n",
      "MEAN POLICY LOSS tensor(21.2387, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 367\n",
      "MEAN POLICY LOSS tensor(33.8613, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 368\n",
      "MEAN POLICY LOSS tensor(31.5420, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 369\n",
      "MEAN POLICY LOSS tensor(34.8445, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 370\n",
      "MEAN POLICY LOSS tensor(40.3077, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 371\n",
      "MEAN POLICY LOSS tensor(16.9999, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 372\n",
      "MEAN POLICY LOSS tensor(31.3006, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 373\n",
      "MEAN POLICY LOSS tensor(27.4112, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 374\n",
      "MEAN POLICY LOSS tensor(32.4825, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 375\n",
      "MEAN POLICY LOSS tensor(27.0075, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 376\n",
      "MEAN POLICY LOSS tensor(37.1769, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 377\n",
      "MEAN POLICY LOSS tensor(29.7249, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 378\n",
      "MEAN POLICY LOSS tensor(29.0503, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 379\n",
      "MEAN POLICY LOSS tensor(30.5516, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 380\n",
      "MEAN POLICY LOSS tensor(41.3944, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 381\n",
      "MEAN POLICY LOSS tensor(22.6867, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 382\n",
      "MEAN POLICY LOSS tensor(29.3281, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 383\n",
      "MEAN POLICY LOSS tensor(23.0409, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 384\n",
      "MEAN POLICY LOSS tensor(25.8204, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 385\n",
      "MEAN POLICY LOSS tensor(24.7651, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 386\n",
      "MEAN POLICY LOSS tensor(28.3878, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 387\n",
      "MEAN POLICY LOSS tensor(28.2886, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 388\n",
      "MEAN POLICY LOSS tensor(34.0320, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 389\n",
      "MEAN POLICY LOSS tensor(22.8577, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 390\n",
      "MEAN POLICY LOSS tensor(33.3158, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 391\n",
      "MEAN POLICY LOSS tensor(32.6728, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 392\n",
      "MEAN POLICY LOSS tensor(24.7593, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 393\n",
      "MEAN POLICY LOSS tensor(22.7644, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 394\n",
      "MEAN POLICY LOSS tensor(28.2057, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 395\n",
      "MEAN POLICY LOSS tensor(33.9960, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 396\n",
      "MEAN POLICY LOSS tensor(17.4246, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 397\n",
      "MEAN POLICY LOSS tensor(20.8956, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 398\n",
      "MEAN POLICY LOSS tensor(22.9186, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 399\n",
      "MEAN POLICY LOSS tensor(22.7076, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 400\n",
      "MEAN POLICY LOSS tensor(17.0353, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 401\n",
      "MEAN POLICY LOSS tensor(17.2085, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 402\n",
      "MEAN POLICY LOSS tensor(21.0898, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 403\n",
      "MEAN POLICY LOSS tensor(20.9350, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 404\n",
      "MEAN POLICY LOSS tensor(16.1970, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 405\n",
      "MEAN POLICY LOSS tensor(25.2377, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 406\n",
      "MEAN POLICY LOSS tensor(27.5254, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 407\n",
      "MEAN POLICY LOSS tensor(20.1996, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 408\n",
      "MEAN POLICY LOSS tensor(26.0442, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 409\n",
      "MEAN POLICY LOSS tensor(18.9102, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 410\n",
      "MEAN POLICY LOSS tensor(22.3627, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 411\n",
      "MEAN POLICY LOSS tensor(21.3668, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 412\n",
      "MEAN POLICY LOSS tensor(16.0944, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 413\n",
      "MEAN POLICY LOSS tensor(14.0723, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 414\n",
      "MEAN POLICY LOSS tensor(21.4462, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 415\n",
      "MEAN POLICY LOSS tensor(18.7497, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 416\n",
      "MEAN POLICY LOSS tensor(18.7016, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 417\n",
      "MEAN POLICY LOSS tensor(29.3600, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 418\n",
      "MEAN POLICY LOSS tensor(15.1856, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 419\n",
      "MEAN POLICY LOSS tensor(21.2353, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 420\n",
      "MEAN POLICY LOSS tensor(22.8577, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 421\n",
      "MEAN POLICY LOSS tensor(17.7085, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 422\n",
      "MEAN POLICY LOSS tensor(19.1802, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 423\n",
      "MEAN POLICY LOSS tensor(10.0740, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 424\n",
      "MEAN POLICY LOSS tensor(17.2687, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 425\n",
      "MEAN POLICY LOSS tensor(16.7954, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 426\n",
      "MEAN POLICY LOSS tensor(14.7430, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 427\n",
      "MEAN POLICY LOSS tensor(22.4363, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 428\n",
      "MEAN POLICY LOSS tensor(11.2284, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 429\n",
      "MEAN POLICY LOSS tensor(16.4675, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 430\n",
      "MEAN POLICY LOSS tensor(19.9458, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 431\n",
      "MEAN POLICY LOSS tensor(20.6661, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 432\n",
      "MEAN POLICY LOSS tensor(16.8585, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 433\n",
      "MEAN POLICY LOSS tensor(15.5273, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 434\n",
      "MEAN POLICY LOSS tensor(19.7702, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 435\n",
      "MEAN POLICY LOSS tensor(19.8837, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 436\n",
      "MEAN POLICY LOSS tensor(59.5330, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 437\n",
      "MEAN POLICY LOSS tensor(14.9533, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 438\n",
      "MEAN POLICY LOSS tensor(10.6217, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 439\n",
      "MEAN POLICY LOSS tensor(11.0559, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 440\n",
      "MEAN POLICY LOSS tensor(25.0196, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 441\n",
      "MEAN POLICY LOSS tensor(16.0504, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 442\n",
      "MEAN POLICY LOSS tensor(22.0624, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 443\n",
      "MEAN POLICY LOSS tensor(16.2746, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 444\n",
      "MEAN POLICY LOSS tensor(18.3818, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 445\n",
      "MEAN POLICY LOSS tensor(12.0055, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 446\n",
      "MEAN POLICY LOSS tensor(13.6472, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 447\n",
      "MEAN POLICY LOSS tensor(12.9424, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 448\n",
      "MEAN POLICY LOSS tensor(6.2000, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 449\n",
      "MEAN POLICY LOSS tensor(16.0041, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 450\n",
      "MEAN POLICY LOSS tensor(16.0771, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 451\n",
      "MEAN POLICY LOSS tensor(11.7316, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 452\n",
      "MEAN POLICY LOSS tensor(12.3848, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 453\n",
      "MEAN POLICY LOSS tensor(20.0037, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 454\n",
      "MEAN POLICY LOSS tensor(14.2571, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 455\n",
      "MEAN POLICY LOSS tensor(15.6855, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 456\n",
      "MEAN POLICY LOSS tensor(12.1178, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 457\n",
      "MEAN POLICY LOSS tensor(17.9657, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 458\n",
      "MEAN POLICY LOSS tensor(11.5064, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 459\n",
      "MEAN POLICY LOSS tensor(14.3156, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 460\n",
      "MEAN POLICY LOSS tensor(13.2766, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 461\n",
      "MEAN POLICY LOSS tensor(13.4857, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 462\n",
      "MEAN POLICY LOSS tensor(12.1005, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 463\n",
      "MEAN POLICY LOSS tensor(20.4204, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 464\n",
      "MEAN POLICY LOSS tensor(13.0050, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 465\n",
      "MEAN POLICY LOSS tensor(11.2041, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 466\n",
      "MEAN POLICY LOSS tensor(12.8191, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 467\n",
      "MEAN POLICY LOSS tensor(16.4443, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 468\n",
      "MEAN POLICY LOSS tensor(13.1631, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 469\n",
      "MEAN POLICY LOSS tensor(17.7027, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 470\n",
      "MEAN POLICY LOSS tensor(11.2614, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 471\n",
      "MEAN POLICY LOSS tensor(13.6368, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 472\n",
      "MEAN POLICY LOSS tensor(10.8318, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 473\n",
      "MEAN POLICY LOSS tensor(16.7572, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 474\n",
      "MEAN POLICY LOSS tensor(8.6505, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 475\n",
      "MEAN POLICY LOSS tensor(13.7741, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 476\n",
      "MEAN POLICY LOSS tensor(15.6380, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 477\n",
      "MEAN POLICY LOSS tensor(14.3422, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 478\n",
      "MEAN POLICY LOSS tensor(10.7427, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 479\n",
      "MEAN POLICY LOSS tensor(13.8795, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 480\n",
      "MEAN POLICY LOSS tensor(16.5191, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 481\n",
      "MEAN POLICY LOSS tensor(12.0495, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 482\n",
      "MEAN POLICY LOSS tensor(9.1472, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 483\n",
      "MEAN POLICY LOSS tensor(10.0010, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 484\n",
      "MEAN POLICY LOSS tensor(9.2039, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 485\n",
      "MEAN POLICY LOSS tensor(11.0420, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 486\n",
      "MEAN POLICY LOSS tensor(10.5545, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 487\n",
      "MEAN POLICY LOSS tensor(9.6612, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 488\n",
      "MEAN POLICY LOSS tensor(10.9048, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 489\n",
      "MEAN POLICY LOSS tensor(10.4080, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 490\n",
      "MEAN POLICY LOSS tensor(12.9506, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 491\n",
      "MEAN POLICY LOSS tensor(11.8243, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 492\n",
      "MEAN POLICY LOSS tensor(11.8578, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 493\n",
      "MEAN POLICY LOSS tensor(15.1693, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 494\n",
      "MEAN POLICY LOSS tensor(13.8945, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 495\n",
      "MEAN POLICY LOSS tensor(11.0015, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 496\n",
      "MEAN POLICY LOSS tensor(11.0478, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 497\n",
      "MEAN POLICY LOSS tensor(13.3525, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 498\n",
      "MEAN POLICY LOSS tensor(11.3680, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 499\n",
      "MEAN POLICY LOSS tensor(11.9238, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 500\n",
      "MEAN POLICY LOSS tensor(13.0177, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 501\n",
      "MEAN POLICY LOSS tensor(8.8074, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 502\n",
      "MEAN POLICY LOSS tensor(7.1259, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 503\n",
      "MEAN POLICY LOSS tensor(12.7728, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 504\n",
      "MEAN POLICY LOSS tensor(7.8239, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 505\n",
      "MEAN POLICY LOSS tensor(14.8600, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 506\n",
      "MEAN POLICY LOSS tensor(10.4792, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 507\n",
      "MEAN POLICY LOSS tensor(13.4312, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 508\n",
      "MEAN POLICY LOSS tensor(11.1242, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 509\n",
      "MEAN POLICY LOSS tensor(9.8268, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 510\n",
      "MEAN POLICY LOSS tensor(13.8216, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 511\n",
      "MEAN POLICY LOSS tensor(7.8205, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 512\n",
      "MEAN POLICY LOSS tensor(5.8279, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 513\n",
      "MEAN POLICY LOSS tensor(10.8029, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 514\n",
      "MEAN POLICY LOSS tensor(10.2448, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 515\n",
      "MEAN POLICY LOSS tensor(10.7357, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 516\n",
      "MEAN POLICY LOSS tensor(7.6057, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 517\n",
      "MEAN POLICY LOSS tensor(11.7976, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 518\n",
      "MEAN POLICY LOSS tensor(12.1468, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 519\n",
      "MEAN POLICY LOSS tensor(8.2482, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 520\n",
      "MEAN POLICY LOSS tensor(12.0460, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 521\n",
      "MEAN POLICY LOSS tensor(8.6806, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 522\n",
      "MEAN POLICY LOSS tensor(8.2783, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 523\n",
      "MEAN POLICY LOSS tensor(10.2251, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 524\n",
      "MEAN POLICY LOSS tensor(8.8178, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 525\n",
      "MEAN POLICY LOSS tensor(8.9521, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 526\n",
      "MEAN POLICY LOSS tensor(11.1821, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 527\n",
      "MEAN POLICY LOSS tensor(6.4650, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 528\n",
      "MEAN POLICY LOSS tensor(9.1402, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 529\n",
      "MEAN POLICY LOSS tensor(12.2678, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 530\n",
      "MEAN POLICY LOSS tensor(10.2946, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 531\n",
      "MEAN POLICY LOSS tensor(9.6415, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 532\n",
      "MEAN POLICY LOSS tensor(6.3840, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 533\n",
      "MEAN POLICY LOSS tensor(8.0115, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 534\n",
      "MEAN POLICY LOSS tensor(10.3362, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 535\n",
      "MEAN POLICY LOSS tensor(8.8548, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 536\n",
      "MEAN POLICY LOSS tensor(8.5776, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 537\n",
      "MEAN POLICY LOSS tensor(12.2319, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 538\n",
      "MEAN POLICY LOSS tensor(8.6818, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 539\n",
      "MEAN POLICY LOSS tensor(11.4913, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 540\n",
      "MEAN POLICY LOSS tensor(7.7823, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 541\n",
      "MEAN POLICY LOSS tensor(8.5220, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 542\n",
      "MEAN POLICY LOSS tensor(7.1144, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 543\n",
      "MEAN POLICY LOSS tensor(7.5976, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 544\n",
      "MEAN POLICY LOSS tensor(11.0941, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 545\n",
      "MEAN POLICY LOSS tensor(9.4244, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 546\n",
      "MEAN POLICY LOSS tensor(8.8664, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 547\n",
      "MEAN POLICY LOSS tensor(8.0422, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 548\n",
      "MEAN POLICY LOSS tensor(6.6994, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 549\n",
      "MEAN POLICY LOSS tensor(7.3835, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 550\n",
      "MEAN POLICY LOSS tensor(10.9279, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 551\n",
      "MEAN POLICY LOSS tensor(7.7163, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 552\n",
      "MEAN POLICY LOSS tensor(10.5696, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 553\n",
      "MEAN POLICY LOSS tensor(6.5420, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 554\n",
      "MEAN POLICY LOSS tensor(10.0647, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 555\n",
      "MEAN POLICY LOSS tensor(8.2100, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 556\n",
      "MEAN POLICY LOSS tensor(9.6693, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 557\n",
      "MEAN POLICY LOSS tensor(9.6670, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 558\n",
      "MEAN POLICY LOSS tensor(8.1973, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 559\n",
      "MEAN POLICY LOSS tensor(8.7553, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 560\n",
      "MEAN POLICY LOSS tensor(8.0329, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 561\n",
      "MEAN POLICY LOSS tensor(10.7050, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 562\n",
      "MEAN POLICY LOSS tensor(10.0173, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 563\n",
      "MEAN POLICY LOSS tensor(10.8839, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 564\n",
      "MEAN POLICY LOSS tensor(13.3918, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 565\n",
      "MEAN POLICY LOSS tensor(6.5501, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 566\n",
      "MEAN POLICY LOSS tensor(8.3362, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 567\n",
      "MEAN POLICY LOSS tensor(10.0740, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 568\n",
      "MEAN POLICY LOSS tensor(6.9720, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 569\n",
      "MEAN POLICY LOSS tensor(7.2637, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 570\n",
      "MEAN POLICY LOSS tensor(8.4653, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 571\n",
      "MEAN POLICY LOSS tensor(7.9918, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 572\n",
      "MEAN POLICY LOSS tensor(10.8712, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 573\n",
      "MEAN POLICY LOSS tensor(7.6850, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 574\n",
      "MEAN POLICY LOSS tensor(6.5698, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 575\n",
      "MEAN POLICY LOSS tensor(6.8476, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 576\n",
      "MEAN POLICY LOSS tensor(6.5987, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 577\n",
      "MEAN POLICY LOSS tensor(7.1711, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 578\n",
      "MEAN POLICY LOSS tensor(7.8019, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 579\n",
      "MEAN POLICY LOSS tensor(7.5884, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 580\n",
      "MEAN POLICY LOSS tensor(7.4506, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 581\n",
      "MEAN POLICY LOSS tensor(9.3011, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 582\n",
      "MEAN POLICY LOSS tensor(9.4847, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 583\n",
      "MEAN POLICY LOSS tensor(6.0564, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 584\n",
      "MEAN POLICY LOSS tensor(7.4136, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 585\n",
      "MEAN POLICY LOSS tensor(7.8795, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 586\n",
      "MEAN POLICY LOSS tensor(6.3632, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 587\n",
      "MEAN POLICY LOSS tensor(9.3920, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 588\n",
      "MEAN POLICY LOSS tensor(5.0716, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 589\n",
      "MEAN POLICY LOSS tensor(7.2104, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 590\n",
      "MEAN POLICY LOSS tensor(6.9674, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 591\n",
      "MEAN POLICY LOSS tensor(7.3094, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 592\n",
      "MEAN POLICY LOSS tensor(5.3458, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 593\n",
      "MEAN POLICY LOSS tensor(5.5929, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 594\n",
      "MEAN POLICY LOSS tensor(8.0885, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 595\n",
      "MEAN POLICY LOSS tensor(5.9424, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 596\n",
      "MEAN POLICY LOSS tensor(7.7707, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 597\n",
      "MEAN POLICY LOSS tensor(8.0092, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 598\n",
      "MEAN POLICY LOSS tensor(6.1514, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 599\n",
      "MEAN POLICY LOSS tensor(8.0387, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 600\n",
      "MEAN POLICY LOSS tensor(7.3337, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 601\n",
      "MEAN POLICY LOSS tensor(5.2961, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 602\n",
      "MEAN POLICY LOSS tensor(7.6769, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 603\n",
      "MEAN POLICY LOSS tensor(5.8776, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 604\n",
      "MEAN POLICY LOSS tensor(6.7758, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 605\n",
      "MEAN POLICY LOSS tensor(6.5304, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 606\n",
      "MEAN POLICY LOSS tensor(5.8290, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 607\n",
      "MEAN POLICY LOSS tensor(6.0258, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 608\n",
      "MEAN POLICY LOSS tensor(4.8314, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 609\n",
      "MEAN POLICY LOSS tensor(5.4523, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 610\n",
      "MEAN POLICY LOSS tensor(10.4544, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 611\n",
      "MEAN POLICY LOSS tensor(8.1926, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 612\n",
      "MEAN POLICY LOSS tensor(3.3942, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 613\n",
      "MEAN POLICY LOSS tensor(8.3188, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 614\n",
      "MEAN POLICY LOSS tensor(8.0196, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 615\n",
      "MEAN POLICY LOSS tensor(5.3539, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 616\n",
      "MEAN POLICY LOSS tensor(6.4095, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 617\n",
      "MEAN POLICY LOSS tensor(3.2334, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 618\n",
      "MEAN POLICY LOSS tensor(5.8603, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 619\n",
      "MEAN POLICY LOSS tensor(6.5038, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 620\n",
      "MEAN POLICY LOSS tensor(4.9148, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 621\n",
      "MEAN POLICY LOSS tensor(6.1687, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 622\n",
      "MEAN POLICY LOSS tensor(6.4534, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 623\n",
      "MEAN POLICY LOSS tensor(7.1445, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 624\n",
      "MEAN POLICY LOSS tensor(6.8337, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 625\n",
      "MEAN POLICY LOSS tensor(6.2636, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 626\n",
      "MEAN POLICY LOSS tensor(6.6982, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 627\n",
      "MEAN POLICY LOSS tensor(6.7283, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 628\n",
      "MEAN POLICY LOSS tensor(5.2347, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 629\n",
      "MEAN POLICY LOSS tensor(5.8730, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 630\n",
      "MEAN POLICY LOSS tensor(8.1128, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 631\n",
      "MEAN POLICY LOSS tensor(2.4287, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 632\n",
      "MEAN POLICY LOSS tensor(5.9945, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 633\n",
      "MEAN POLICY LOSS tensor(4.7724, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 634\n",
      "MEAN POLICY LOSS tensor(6.5871, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 635\n",
      "MEAN POLICY LOSS tensor(6.7862, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 636\n",
      "MEAN POLICY LOSS tensor(5.7121, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 637\n",
      "MEAN POLICY LOSS tensor(6.6334, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 638\n",
      "MEAN POLICY LOSS tensor(5.6531, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 639\n",
      "MEAN POLICY LOSS tensor(7.3024, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 640\n",
      "MEAN POLICY LOSS tensor(5.4847, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 641\n",
      "MEAN POLICY LOSS tensor(5.5079, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 642\n",
      "MEAN POLICY LOSS tensor(5.1352, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 643\n",
      "MEAN POLICY LOSS tensor(7.1410, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 644\n",
      "MEAN POLICY LOSS tensor(6.8429, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 645\n",
      "MEAN POLICY LOSS tensor(5.1560, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 646\n",
      "MEAN POLICY LOSS tensor(5.0565, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 647\n",
      "MEAN POLICY LOSS tensor(5.1422, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 648\n",
      "MEAN POLICY LOSS tensor(5.5895, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 649\n",
      "MEAN POLICY LOSS tensor(8.9498, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 650\n",
      "MEAN POLICY LOSS tensor(5.6867, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 651\n",
      "MEAN POLICY LOSS tensor(5.7052, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 652\n",
      "MEAN POLICY LOSS tensor(6.0038, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 653\n",
      "MEAN POLICY LOSS tensor(6.6137, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 654\n",
      "MEAN POLICY LOSS tensor(5.9714, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 655\n",
      "MEAN POLICY LOSS tensor(5.3956, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 656\n",
      "MEAN POLICY LOSS tensor(6.7978, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 657\n",
      "MEAN POLICY LOSS tensor(4.6926, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 658\n",
      "MEAN POLICY LOSS tensor(5.2336, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 659\n",
      "MEAN POLICY LOSS tensor(4.6636, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 660\n",
      "MEAN POLICY LOSS tensor(5.7781, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 661\n",
      "MEAN POLICY LOSS tensor(5.0739, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 662\n",
      "MEAN POLICY LOSS tensor(4.6416, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 663\n",
      "MEAN POLICY LOSS tensor(5.1341, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 664\n",
      "MEAN POLICY LOSS tensor(5.8892, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 665\n",
      "MEAN POLICY LOSS tensor(4.1331, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 666\n",
      "MEAN POLICY LOSS tensor(4.6405, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 667\n",
      "MEAN POLICY LOSS tensor(5.8626, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 668\n",
      "MEAN POLICY LOSS tensor(4.6405, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 669\n",
      "MEAN POLICY LOSS tensor(4.2719, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 670\n",
      "MEAN POLICY LOSS tensor(5.1051, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 671\n",
      "MEAN POLICY LOSS tensor(5.7272, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 672\n",
      "MEAN POLICY LOSS tensor(4.8592, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 673\n",
      "MEAN POLICY LOSS tensor(4.9981, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 674\n",
      "MEAN POLICY LOSS tensor(6.3493, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 675\n",
      "MEAN POLICY LOSS tensor(3.9525, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 676\n",
      "MEAN POLICY LOSS tensor(5.8151, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 677\n",
      "MEAN POLICY LOSS tensor(6.2092, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 678\n",
      "MEAN POLICY LOSS tensor(5.8614, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 679\n",
      "MEAN POLICY LOSS tensor(5.9031, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 680\n",
      "MEAN POLICY LOSS tensor(3.7998, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 681\n",
      "MEAN POLICY LOSS tensor(6.8360, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 682\n",
      "MEAN POLICY LOSS tensor(4.8870, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 683\n",
      "MEAN POLICY LOSS tensor(7.2799, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 684\n",
      "MEAN POLICY LOSS tensor(3.6118, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 685\n",
      "MEAN POLICY LOSS tensor(6.8186, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 686\n",
      "MEAN POLICY LOSS tensor(3.6164, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 687\n",
      "MEAN POLICY LOSS tensor(3.9306, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 688\n",
      "MEAN POLICY LOSS tensor(5.0016, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 689\n",
      "MEAN POLICY LOSS tensor(3.4995, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 690\n",
      "MEAN POLICY LOSS tensor(6.4315, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 691\n",
      "MEAN POLICY LOSS tensor(4.8407, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 692\n",
      "MEAN POLICY LOSS tensor(4.6544, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 693\n",
      "MEAN POLICY LOSS tensor(4.2696, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 694\n",
      "MEAN POLICY LOSS tensor(4.9934, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 695\n",
      "MEAN POLICY LOSS tensor(5.3204, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 696\n",
      "MEAN POLICY LOSS tensor(5.2648, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 697\n",
      "MEAN POLICY LOSS tensor(4.6451, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 698\n",
      "MEAN POLICY LOSS tensor(5.5102, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 699\n",
      "MEAN POLICY LOSS tensor(5.0554, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 700\n",
      "MEAN POLICY LOSS tensor(5.8267, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 701\n",
      "MEAN POLICY LOSS tensor(5.5802, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 702\n",
      "MEAN POLICY LOSS tensor(5.4095, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 703\n",
      "MEAN POLICY LOSS tensor(3.5181, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 704\n",
      "MEAN POLICY LOSS tensor(3.5771, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 705\n",
      "MEAN POLICY LOSS tensor(5.7422, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 706\n",
      "MEAN POLICY LOSS tensor(4.8951, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 707\n",
      "MEAN POLICY LOSS tensor(5.7908, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 708\n",
      "MEAN POLICY LOSS tensor(5.3354, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 709\n",
      "MEAN POLICY LOSS tensor(4.4038, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 710\n",
      "MEAN POLICY LOSS tensor(3.4128, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 711\n",
      "MEAN POLICY LOSS tensor(2.6867, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 712\n",
      "MEAN POLICY LOSS tensor(3.0564, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 713\n",
      "MEAN POLICY LOSS tensor(5.3493, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 714\n",
      "MEAN POLICY LOSS tensor(4.8557, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 715\n",
      "MEAN POLICY LOSS tensor(4.1631, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 716\n",
      "MEAN POLICY LOSS tensor(4.9067, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 717\n",
      "MEAN POLICY LOSS tensor(2.7932, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 718\n",
      "MEAN POLICY LOSS tensor(4.2615, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 719\n",
      "MEAN POLICY LOSS tensor(3.9792, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 720\n",
      "MEAN POLICY LOSS tensor(4.6312, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 721\n",
      "MEAN POLICY LOSS tensor(4.6289, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 722\n",
      "MEAN POLICY LOSS tensor(4.1435, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 723\n",
      "MEAN POLICY LOSS tensor(3.7101, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 724\n",
      "MEAN POLICY LOSS tensor(4.6775, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 725\n",
      "MEAN POLICY LOSS tensor(4.2650, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 726\n",
      "MEAN POLICY LOSS tensor(3.1813, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 727\n",
      "MEAN POLICY LOSS tensor(4.1007, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 728\n",
      "MEAN POLICY LOSS tensor(3.8912, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 729\n",
      "MEAN POLICY LOSS tensor(4.5167, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 730\n",
      "MEAN POLICY LOSS tensor(2.7932, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 731\n",
      "MEAN POLICY LOSS tensor(4.7180, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 732\n",
      "MEAN POLICY LOSS tensor(3.7940, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 733\n",
      "MEAN POLICY LOSS tensor(3.7078, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 734\n",
      "MEAN POLICY LOSS tensor(4.4530, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 735\n",
      "MEAN POLICY LOSS tensor(3.8889, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 736\n",
      "MEAN POLICY LOSS tensor(3.9792, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 737\n",
      "MEAN POLICY LOSS tensor(1.8595, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 738\n",
      "MEAN POLICY LOSS tensor(4.7030, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 739\n",
      "MEAN POLICY LOSS tensor(4.2395, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 740\n",
      "MEAN POLICY LOSS tensor(4.4380, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 741\n",
      "MEAN POLICY LOSS tensor(3.4070, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 742\n",
      "MEAN POLICY LOSS tensor(2.1765, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 743\n",
      "MEAN POLICY LOSS tensor(4.4096, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 744\n",
      "MEAN POLICY LOSS tensor(3.7009, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 745\n",
      "MEAN POLICY LOSS tensor(4.6590, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 746\n",
      "MEAN POLICY LOSS tensor(4.0393, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 747\n",
      "MEAN POLICY LOSS tensor(4.0370, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 748\n",
      "MEAN POLICY LOSS tensor(4.9888, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 749\n",
      "MEAN POLICY LOSS tensor(4.2222, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 750\n",
      "MEAN POLICY LOSS tensor(4.1122, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 751\n",
      "MEAN POLICY LOSS tensor(3.9063, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 752\n",
      "MEAN POLICY LOSS tensor(4.1828, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 753\n",
      "MEAN POLICY LOSS tensor(4.6370, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 754\n",
      "MEAN POLICY LOSS tensor(3.2728, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 755\n",
      "MEAN POLICY LOSS tensor(3.3850, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 756\n",
      "MEAN POLICY LOSS tensor(3.3931, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 757\n",
      "MEAN POLICY LOSS tensor(3.4313, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 758\n",
      "MEAN POLICY LOSS tensor(2.7631, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 759\n",
      "MEAN POLICY LOSS tensor(2.8510, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 760\n",
      "MEAN POLICY LOSS tensor(4.9784, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 761\n",
      "MEAN POLICY LOSS tensor(3.9514, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 762\n",
      "MEAN POLICY LOSS tensor(3.8889, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 763\n",
      "MEAN POLICY LOSS tensor(4.7470, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 764\n",
      "MEAN POLICY LOSS tensor(4.9830, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 765\n",
      "MEAN POLICY LOSS tensor(3.5539, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 766\n",
      "MEAN POLICY LOSS tensor(3.4347, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 767\n",
      "MEAN POLICY LOSS tensor(3.1189, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 768\n",
      "MEAN POLICY LOSS tensor(3.8993, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 769\n",
      "MEAN POLICY LOSS tensor(3.8415, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 770\n",
      "MEAN POLICY LOSS tensor(3.9352, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 771\n",
      "MEAN POLICY LOSS tensor(3.7859, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 772\n",
      "MEAN POLICY LOSS tensor(3.4452, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 773\n",
      "MEAN POLICY LOSS tensor(3.8577, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 774\n",
      "MEAN POLICY LOSS tensor(3.8970, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 775\n",
      "MEAN POLICY LOSS tensor(3.7790, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 776\n",
      "MEAN POLICY LOSS tensor(3.6673, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 777\n",
      "MEAN POLICY LOSS tensor(2.6474, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 778\n",
      "MEAN POLICY LOSS tensor(4.1018, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 779\n",
      "MEAN POLICY LOSS tensor(2.4345, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 780\n",
      "MEAN POLICY LOSS tensor(3.4428, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 781\n",
      "MEAN POLICY LOSS tensor(2.7862, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 782\n",
      "MEAN POLICY LOSS tensor(4.6405, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 783\n",
      "MEAN POLICY LOSS tensor(3.2890, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 784\n",
      "MEAN POLICY LOSS tensor(3.5088, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 785\n",
      "MEAN POLICY LOSS tensor(3.6511, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 786\n",
      "MEAN POLICY LOSS tensor(3.2265, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 787\n",
      "MEAN POLICY LOSS tensor(3.3989, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 788\n",
      "MEAN POLICY LOSS tensor(3.5319, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 789\n",
      "MEAN POLICY LOSS tensor(3.5424, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 790\n",
      "MEAN POLICY LOSS tensor(4.0173, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 791\n",
      "MEAN POLICY LOSS tensor(3.6523, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 792\n",
      "MEAN POLICY LOSS tensor(3.6731, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 793\n",
      "MEAN POLICY LOSS tensor(3.6581, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 794\n",
      "MEAN POLICY LOSS tensor(6.1548, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 795\n",
      "MEAN POLICY LOSS tensor(2.3581, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 796\n",
      "MEAN POLICY LOSS tensor(4.0359, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 797\n",
      "MEAN POLICY LOSS tensor(3.7333, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 798\n",
      "MEAN POLICY LOSS tensor(3.1837, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 799\n",
      "MEAN POLICY LOSS tensor(2.9904, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 800\n",
      "MEAN POLICY LOSS tensor(2.7446, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 801\n",
      "MEAN POLICY LOSS tensor(3.8195, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 802\n",
      "MEAN POLICY LOSS tensor(3.6812, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 803\n",
      "MEAN POLICY LOSS tensor(2.8823, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 804\n",
      "MEAN POLICY LOSS tensor(3.6858, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 805\n",
      "MEAN POLICY LOSS tensor(3.9282, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 806\n",
      "MEAN POLICY LOSS tensor(3.0309, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 807\n",
      "MEAN POLICY LOSS tensor(4.2939, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 808\n",
      "MEAN POLICY LOSS tensor(3.3873, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 809\n",
      "MEAN POLICY LOSS tensor(2.6694, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 810\n",
      "MEAN POLICY LOSS tensor(4.0347, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 811\n",
      "MEAN POLICY LOSS tensor(2.8082, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 812\n",
      "MEAN POLICY LOSS tensor(3.1663, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 813\n",
      "MEAN POLICY LOSS tensor(4.0868, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 814\n",
      "MEAN POLICY LOSS tensor(3.8947, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 815\n",
      "MEAN POLICY LOSS tensor(3.0784, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 816\n",
      "MEAN POLICY LOSS tensor(3.6592, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 817\n",
      "MEAN POLICY LOSS tensor(3.3838, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 818\n",
      "MEAN POLICY LOSS tensor(3.5690, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 819\n",
      "MEAN POLICY LOSS tensor(2.5166, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 820\n",
      "MEAN POLICY LOSS tensor(3.3109, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 821\n",
      "MEAN POLICY LOSS tensor(3.8334, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 822\n",
      "MEAN POLICY LOSS tensor(3.7547, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 823\n",
      "MEAN POLICY LOSS tensor(3.1628, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 824\n",
      "MEAN POLICY LOSS tensor(4.2997, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 825\n",
      "MEAN POLICY LOSS tensor(2.6809, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 826\n",
      "MEAN POLICY LOSS tensor(4.1550, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 827\n",
      "MEAN POLICY LOSS tensor(3.3653, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 828\n",
      "MEAN POLICY LOSS tensor(3.0934, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 829\n",
      "MEAN POLICY LOSS tensor(3.3966, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 830\n",
      "MEAN POLICY LOSS tensor(4.0926, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 831\n",
      "MEAN POLICY LOSS tensor(3.3179, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 832\n",
      "MEAN POLICY LOSS tensor(3.2623, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 833\n",
      "MEAN POLICY LOSS tensor(3.0518, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 834\n",
      "MEAN POLICY LOSS tensor(4.6486, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 835\n",
      "MEAN POLICY LOSS tensor(3.7101, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 836\n",
      "MEAN POLICY LOSS tensor(2.4692, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 837\n",
      "MEAN POLICY LOSS tensor(3.2149, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 838\n",
      "MEAN POLICY LOSS tensor(3.1154, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 839\n",
      "MEAN POLICY LOSS tensor(3.3792, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 840\n",
      "MEAN POLICY LOSS tensor(2.2540, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 841\n",
      "MEAN POLICY LOSS tensor(3.0159, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 842\n",
      "MEAN POLICY LOSS tensor(2.4877, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 843\n",
      "MEAN POLICY LOSS tensor(3.5875, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 844\n",
      "MEAN POLICY LOSS tensor(3.2762, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 845\n",
      "MEAN POLICY LOSS tensor(2.8094, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 846\n",
      "MEAN POLICY LOSS tensor(3.0911, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 847\n",
      "MEAN POLICY LOSS tensor(2.5930, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 848\n",
      "MEAN POLICY LOSS tensor(2.9673, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 849\n",
      "MEAN POLICY LOSS tensor(3.3132, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 850\n",
      "MEAN POLICY LOSS tensor(3.1698, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 851\n",
      "MEAN POLICY LOSS tensor(3.3364, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 852\n",
      "MEAN POLICY LOSS tensor(3.1871, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 853\n",
      "MEAN POLICY LOSS tensor(3.2832, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 854\n",
      "MEAN POLICY LOSS tensor(2.2413, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 855\n",
      "MEAN POLICY LOSS tensor(2.9580, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 856\n",
      "MEAN POLICY LOSS tensor(2.4530, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 857\n",
      "MEAN POLICY LOSS tensor(3.0390, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 858\n",
      "MEAN POLICY LOSS tensor(2.3801, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 859\n",
      "MEAN POLICY LOSS tensor(2.6717, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 860\n",
      "MEAN POLICY LOSS tensor(2.6231, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 861\n",
      "MEAN POLICY LOSS tensor(2.3373, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 862\n",
      "MEAN POLICY LOSS tensor(3.8739, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 863\n",
      "MEAN POLICY LOSS tensor(3.9387, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 864\n",
      "MEAN POLICY LOSS tensor(3.4185, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 865\n",
      "MEAN POLICY LOSS tensor(1.6837, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 866\n",
      "MEAN POLICY LOSS tensor(2.2505, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 867\n",
      "MEAN POLICY LOSS tensor(2.8776, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 868\n",
      "MEAN POLICY LOSS tensor(2.5525, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 869\n",
      "MEAN POLICY LOSS tensor(3.5632, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 870\n",
      "MEAN POLICY LOSS tensor(3.1351, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 871\n",
      "MEAN POLICY LOSS tensor(3.1918, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 872\n",
      "MEAN POLICY LOSS tensor(3.3109, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 873\n",
      "MEAN POLICY LOSS tensor(2.5872, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 874\n",
      "MEAN POLICY LOSS tensor(1.9613, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 875\n",
      "MEAN POLICY LOSS tensor(2.4345, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 876\n",
      "MEAN POLICY LOSS tensor(2.8811, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 877\n",
      "MEAN POLICY LOSS tensor(3.2172, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 878\n",
      "MEAN POLICY LOSS tensor(2.8533, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 879\n",
      "MEAN POLICY LOSS tensor(3.1605, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 880\n",
      "MEAN POLICY LOSS tensor(2.8163, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 881\n",
      "MEAN POLICY LOSS tensor(2.7295, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 882\n",
      "MEAN POLICY LOSS tensor(2.2679, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 883\n",
      "MEAN POLICY LOSS tensor(2.9823, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 884\n",
      "MEAN POLICY LOSS tensor(2.8510, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 885\n",
      "MEAN POLICY LOSS tensor(2.6104, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 886\n",
      "MEAN POLICY LOSS tensor(1.8896, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 887\n",
      "MEAN POLICY LOSS tensor(2.8834, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 888\n",
      "MEAN POLICY LOSS tensor(2.9418, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 889\n",
      "MEAN POLICY LOSS tensor(2.3269, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 890\n",
      "MEAN POLICY LOSS tensor(2.0319, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 891\n",
      "MEAN POLICY LOSS tensor(2.7689, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 892\n",
      "MEAN POLICY LOSS tensor(3.0888, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 893\n",
      "MEAN POLICY LOSS tensor(2.7307, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 894\n",
      "MEAN POLICY LOSS tensor(3.5794, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 895\n",
      "MEAN POLICY LOSS tensor(2.5143, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 896\n",
      "MEAN POLICY LOSS tensor(2.5791, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 897\n",
      "MEAN POLICY LOSS tensor(3.8010, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 898\n",
      "MEAN POLICY LOSS tensor(2.8996, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 899\n",
      "MEAN POLICY LOSS tensor(2.6960, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 900\n",
      "MEAN POLICY LOSS tensor(2.1244, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 901\n",
      "MEAN POLICY LOSS tensor(1.7727, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 902\n",
      "MEAN POLICY LOSS tensor(3.3005, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 903\n",
      "MEAN POLICY LOSS tensor(3.0286, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 904\n",
      "MEAN POLICY LOSS tensor(2.8198, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 905\n",
      "MEAN POLICY LOSS tensor(2.7087, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 906\n",
      "MEAN POLICY LOSS tensor(3.5956, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 907\n",
      "MEAN POLICY LOSS tensor(3.4116, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 908\n",
      "MEAN POLICY LOSS tensor(3.2473, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 909\n",
      "MEAN POLICY LOSS tensor(2.9835, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 910\n",
      "MEAN POLICY LOSS tensor(2.1406, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 911\n",
      "MEAN POLICY LOSS tensor(2.3767, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 912\n",
      "MEAN POLICY LOSS tensor(2.5537, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 913\n",
      "MEAN POLICY LOSS tensor(2.1557, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 914\n",
      "MEAN POLICY LOSS tensor(1.7832, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 915\n",
      "MEAN POLICY LOSS tensor(2.4519, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 916\n",
      "MEAN POLICY LOSS tensor(2.2760, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 917\n",
      "MEAN POLICY LOSS tensor(2.2795, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 918\n",
      "MEAN POLICY LOSS tensor(2.5999, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 919\n",
      "MEAN POLICY LOSS tensor(2.9008, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 920\n",
      "MEAN POLICY LOSS tensor(2.2899, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 921\n",
      "MEAN POLICY LOSS tensor(1.9463, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 922\n",
      "MEAN POLICY LOSS tensor(2.7272, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 923\n",
      "MEAN POLICY LOSS tensor(2.8961, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 924\n",
      "MEAN POLICY LOSS tensor(2.7735, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 925\n",
      "MEAN POLICY LOSS tensor(3.0622, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 926\n",
      "MEAN POLICY LOSS tensor(2.4576, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 927\n",
      "MEAN POLICY LOSS tensor(2.7828, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 928\n",
      "MEAN POLICY LOSS tensor(2.5606, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 929\n",
      "MEAN POLICY LOSS tensor(2.2610, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 930\n",
      "MEAN POLICY LOSS tensor(2.3466, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 931\n",
      "MEAN POLICY LOSS tensor(2.4750, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 932\n",
      "MEAN POLICY LOSS tensor(2.3524, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 933\n",
      "MEAN POLICY LOSS tensor(2.7099, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 934\n",
      "MEAN POLICY LOSS tensor(2.9962, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 935\n",
      "MEAN POLICY LOSS tensor(2.0978, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 936\n",
      "MEAN POLICY LOSS tensor(2.8325, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 937\n",
      "MEAN POLICY LOSS tensor(2.4172, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 938\n",
      "MEAN POLICY LOSS tensor(2.7191, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 939\n",
      "MEAN POLICY LOSS tensor(2.5884, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 940\n",
      "MEAN POLICY LOSS tensor(2.4414, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 941\n",
      "MEAN POLICY LOSS tensor(2.4854, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 942\n",
      "MEAN POLICY LOSS tensor(2.1499, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 943\n",
      "MEAN POLICY LOSS tensor(2.5618, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 944\n",
      "MEAN POLICY LOSS tensor(2.2367, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 945\n",
      "MEAN POLICY LOSS tensor(2.7388, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 946\n",
      "MEAN POLICY LOSS tensor(2.4218, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 947\n",
      "MEAN POLICY LOSS tensor(2.2020, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 948\n",
      "MEAN POLICY LOSS tensor(2.0296, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 949\n",
      "MEAN POLICY LOSS tensor(1.7739, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 950\n",
      "MEAN POLICY LOSS tensor(2.8267, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 951\n",
      "MEAN POLICY LOSS tensor(1.9521, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 952\n",
      "MEAN POLICY LOSS tensor(2.2957, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 953\n",
      "MEAN POLICY LOSS tensor(1.9833, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 954\n",
      "MEAN POLICY LOSS tensor(2.0377, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 955\n",
      "MEAN POLICY LOSS tensor(2.2436, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 956\n",
      "MEAN POLICY LOSS tensor(2.2390, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 957\n",
      "MEAN POLICY LOSS tensor(3.0032, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 958\n",
      "MEAN POLICY LOSS tensor(2.0145, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 959\n",
      "MEAN POLICY LOSS tensor(1.6600, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 960\n",
      "MEAN POLICY LOSS tensor(2.3084, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 961\n",
      "MEAN POLICY LOSS tensor(2.7631, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 962\n",
      "MEAN POLICY LOSS tensor(2.9407, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 963\n",
      "MEAN POLICY LOSS tensor(1.7982, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 964\n",
      "MEAN POLICY LOSS tensor(2.3743, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 965\n",
      "MEAN POLICY LOSS tensor(2.4727, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 966\n",
      "MEAN POLICY LOSS tensor(2.3593, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 967\n",
      "MEAN POLICY LOSS tensor(2.4021, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 968\n",
      "MEAN POLICY LOSS tensor(2.7376, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 969\n",
      "MEAN POLICY LOSS tensor(2.5803, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 970\n",
      "MEAN POLICY LOSS tensor(1.9046, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 971\n",
      "MEAN POLICY LOSS tensor(2.7885, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 972\n",
      "MEAN POLICY LOSS tensor(2.2066, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 973\n",
      "MEAN POLICY LOSS tensor(2.7480, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 974\n",
      "MEAN POLICY LOSS tensor(2.4866, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 975\n",
      "MEAN POLICY LOSS tensor(2.1406, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 976\n",
      "MEAN POLICY LOSS tensor(2.4021, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 977\n",
      "MEAN POLICY LOSS tensor(3.0946, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 978\n",
      "MEAN POLICY LOSS tensor(1.6883, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 979\n",
      "MEAN POLICY LOSS tensor(2.0388, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 980\n",
      "MEAN POLICY LOSS tensor(2.1129, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 981\n",
      "MEAN POLICY LOSS tensor(2.7249, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 982\n",
      "MEAN POLICY LOSS tensor(1.4621, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 983\n",
      "MEAN POLICY LOSS tensor(2.6335, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 984\n",
      "MEAN POLICY LOSS tensor(1.9243, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 985\n",
      "MEAN POLICY LOSS tensor(2.3662, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 986\n",
      "MEAN POLICY LOSS tensor(2.7862, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 987\n",
      "MEAN POLICY LOSS tensor(2.6613, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 988\n",
      "MEAN POLICY LOSS tensor(1.8942, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 989\n",
      "MEAN POLICY LOSS tensor(1.5269, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 990\n",
      "MEAN POLICY LOSS tensor(2.0747, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 991\n",
      "MEAN POLICY LOSS tensor(2.0562, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 992\n",
      "MEAN POLICY LOSS tensor(1.6125, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 993\n",
      "MEAN POLICY LOSS tensor(2.9210, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 994\n",
      "MEAN POLICY LOSS tensor(1.8144, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 995\n",
      "MEAN POLICY LOSS tensor(1.6391, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 996\n",
      "MEAN POLICY LOSS tensor(2.2968, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 997\n",
      "MEAN POLICY LOSS tensor(2.0342, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 998\n",
      "MEAN POLICY LOSS tensor(2.7966, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 999\n",
      "MEAN POLICY LOSS tensor(2.3107, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "Run 5/10\n",
      "EPSIODE# 0\n",
      "MEAN POLICY LOSS tensor(10347.6162, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 1\n",
      "MEAN POLICY LOSS tensor(11664.4092, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 2\n",
      "MEAN POLICY LOSS tensor(10149.8271, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 3\n",
      "MEAN POLICY LOSS tensor(11146.2715, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 4\n",
      "MEAN POLICY LOSS tensor(13564.0420, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 5\n",
      "MEAN POLICY LOSS tensor(12942.5186, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 6\n",
      "MEAN POLICY LOSS tensor(10356.1406, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 7\n",
      "MEAN POLICY LOSS tensor(9549.0361, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 8\n",
      "MEAN POLICY LOSS tensor(12759.9824, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 9\n",
      "MEAN POLICY LOSS tensor(9823.6787, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 10\n",
      "MEAN POLICY LOSS tensor(11566.0742, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 11\n",
      "MEAN POLICY LOSS tensor(5706.5508, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 12\n",
      "MEAN POLICY LOSS tensor(9368.4717, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 13\n",
      "MEAN POLICY LOSS tensor(11866.8760, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 14\n",
      "MEAN POLICY LOSS tensor(15368.9717, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 15\n",
      "MEAN POLICY LOSS tensor(6464.3774, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 16\n",
      "MEAN POLICY LOSS tensor(6881.5522, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 17\n",
      "MEAN POLICY LOSS tensor(5414.8521, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 18\n",
      "MEAN POLICY LOSS tensor(10046.8457, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 19\n",
      "MEAN POLICY LOSS tensor(7483.6548, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 20\n",
      "MEAN POLICY LOSS tensor(12112.5273, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 21\n",
      "MEAN POLICY LOSS tensor(7577.6826, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 22\n",
      "MEAN POLICY LOSS tensor(10803.4053, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 23\n",
      "MEAN POLICY LOSS tensor(2623.0789, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 24\n",
      "MEAN POLICY LOSS tensor(10787.8213, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 25\n",
      "MEAN POLICY LOSS tensor(13549.5488, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 26\n",
      "MEAN POLICY LOSS tensor(12394.0107, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 27\n",
      "MEAN POLICY LOSS tensor(18281.5039, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 28\n",
      "MEAN POLICY LOSS tensor(15020.3574, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 29\n",
      "MEAN POLICY LOSS tensor(7894.3027, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 30\n",
      "MEAN POLICY LOSS tensor(7115.3643, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 31\n",
      "MEAN POLICY LOSS tensor(14512.0430, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 32\n",
      "MEAN POLICY LOSS tensor(5846.2808, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 33\n",
      "MEAN POLICY LOSS tensor(15711.0664, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 34\n",
      "MEAN POLICY LOSS tensor(13202.4307, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 35\n",
      "MEAN POLICY LOSS tensor(12029.5586, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 36\n",
      "MEAN POLICY LOSS tensor(7631.4546, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 37\n",
      "MEAN POLICY LOSS tensor(5400.5869, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 38\n",
      "MEAN POLICY LOSS tensor(3697.6978, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 39\n",
      "MEAN POLICY LOSS tensor(6044.0498, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 40\n",
      "MEAN POLICY LOSS tensor(5143.3433, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 41\n",
      "MEAN POLICY LOSS tensor(11380.9043, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 42\n",
      "MEAN POLICY LOSS tensor(5390.2686, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 43\n",
      "MEAN POLICY LOSS tensor(4318.2480, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 44\n",
      "MEAN POLICY LOSS tensor(14446.3633, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 45\n",
      "MEAN POLICY LOSS tensor(1586.7136, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 46\n",
      "MEAN POLICY LOSS tensor(6398.2900, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 47\n",
      "MEAN POLICY LOSS tensor(17255.4590, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 48\n",
      "MEAN POLICY LOSS tensor(4979.3389, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 49\n",
      "MEAN POLICY LOSS tensor(3449.7141, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 50\n",
      "MEAN POLICY LOSS tensor(4380.0518, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 51\n",
      "MEAN POLICY LOSS tensor(8073.2900, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 52\n",
      "MEAN POLICY LOSS tensor(4774.5352, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 53\n",
      "MEAN POLICY LOSS tensor(12000.3877, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 54\n",
      "MEAN POLICY LOSS tensor(2193.1299, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 55\n",
      "MEAN POLICY LOSS tensor(2336.0098, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 56\n",
      "MEAN POLICY LOSS tensor(2331.7119, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 57\n",
      "MEAN POLICY LOSS tensor(1661.1331, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 58\n",
      "MEAN POLICY LOSS tensor(3113.0757, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 59\n",
      "MEAN POLICY LOSS tensor(1513.3241, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 60\n",
      "MEAN POLICY LOSS tensor(2148.0625, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 61\n",
      "MEAN POLICY LOSS tensor(3004.2275, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 62\n",
      "MEAN POLICY LOSS tensor(28291.1562, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 63\n",
      "MEAN POLICY LOSS tensor(1650.4673, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 64\n",
      "MEAN POLICY LOSS tensor(2989.4749, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 65\n",
      "MEAN POLICY LOSS tensor(17217.1660, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 66\n",
      "MEAN POLICY LOSS tensor(2141.0852, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 67\n",
      "MEAN POLICY LOSS tensor(1561.5720, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 68\n",
      "MEAN POLICY LOSS tensor(26512.4121, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 69\n",
      "MEAN POLICY LOSS tensor(1394.5323, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 70\n",
      "MEAN POLICY LOSS tensor(2082.0823, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 71\n",
      "MEAN POLICY LOSS tensor(995.8634, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 72\n",
      "MEAN POLICY LOSS tensor(2545.5083, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 73\n",
      "MEAN POLICY LOSS tensor(2501.0564, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 74\n",
      "MEAN POLICY LOSS tensor(1160.6907, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 75\n",
      "MEAN POLICY LOSS tensor(21358.8359, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 76\n",
      "MEAN POLICY LOSS tensor(616.4352, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 77\n",
      "MEAN POLICY LOSS tensor(21762.7129, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 78\n",
      "MEAN POLICY LOSS tensor(388.7404, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 79\n",
      "MEAN POLICY LOSS tensor(16270.1631, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 80\n",
      "MEAN POLICY LOSS tensor(3439.1768, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 81\n",
      "MEAN POLICY LOSS tensor(3073.2754, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 82\n",
      "MEAN POLICY LOSS tensor(34149.1367, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 83\n",
      "MEAN POLICY LOSS tensor(2629.6125, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 84\n",
      "MEAN POLICY LOSS tensor(4065.4390, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 85\n",
      "MEAN POLICY LOSS tensor(2449.2410, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 86\n",
      "MEAN POLICY LOSS tensor(10437.9160, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 87\n",
      "MEAN POLICY LOSS tensor(6051.4644, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 88\n",
      "MEAN POLICY LOSS tensor(4939.6777, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 89\n",
      "MEAN POLICY LOSS tensor(3801.5476, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 90\n",
      "MEAN POLICY LOSS tensor(3720.6550, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 91\n",
      "MEAN POLICY LOSS tensor(14051.2324, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 92\n",
      "MEAN POLICY LOSS tensor(3814.3198, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 93\n",
      "MEAN POLICY LOSS tensor(6219.7793, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 94\n",
      "MEAN POLICY LOSS tensor(5576.7549, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 95\n",
      "MEAN POLICY LOSS tensor(3195.7708, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 96\n",
      "MEAN POLICY LOSS tensor(4549.0630, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 97\n",
      "MEAN POLICY LOSS tensor(876.1263, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 98\n",
      "MEAN POLICY LOSS tensor(15289.2686, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 99\n",
      "MEAN POLICY LOSS tensor(1688.7662, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 100\n",
      "MEAN POLICY LOSS tensor(2762.7922, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 101\n",
      "MEAN POLICY LOSS tensor(23531.4297, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 102\n",
      "MEAN POLICY LOSS tensor(4673.6606, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 103\n",
      "MEAN POLICY LOSS tensor(4208.5332, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 104\n",
      "MEAN POLICY LOSS tensor(5221.8003, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 105\n",
      "MEAN POLICY LOSS tensor(3144.9407, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 106\n",
      "MEAN POLICY LOSS tensor(3829.8281, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 107\n",
      "MEAN POLICY LOSS tensor(897.0094, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 108\n",
      "MEAN POLICY LOSS tensor(4422.8428, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 109\n",
      "MEAN POLICY LOSS tensor(2238.2610, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 110\n",
      "MEAN POLICY LOSS tensor(17002.6582, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 111\n",
      "MEAN POLICY LOSS tensor(23196.3281, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 112\n",
      "MEAN POLICY LOSS tensor(27407.3438, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 113\n",
      "MEAN POLICY LOSS tensor(978.5663, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 114\n",
      "MEAN POLICY LOSS tensor(1437.0455, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 115\n",
      "MEAN POLICY LOSS tensor(1711.9926, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 116\n",
      "MEAN POLICY LOSS tensor(2073.8469, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 117\n",
      "MEAN POLICY LOSS tensor(3188.1162, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 118\n",
      "MEAN POLICY LOSS tensor(788.3067, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 119\n",
      "MEAN POLICY LOSS tensor(2250.3984, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 120\n",
      "MEAN POLICY LOSS tensor(1147.2843, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 121\n",
      "MEAN POLICY LOSS tensor(1584.6837, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 122\n",
      "MEAN POLICY LOSS tensor(3330.8726, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 123\n",
      "MEAN POLICY LOSS tensor(3556.4524, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 124\n",
      "MEAN POLICY LOSS tensor(1501.1089, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 125\n",
      "MEAN POLICY LOSS tensor(2108.2947, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 126\n",
      "MEAN POLICY LOSS tensor(1441.7618, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 127\n",
      "MEAN POLICY LOSS tensor(22811.3438, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 128\n",
      "MEAN POLICY LOSS tensor(692.3118, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 129\n",
      "MEAN POLICY LOSS tensor(1002.8710, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 130\n",
      "MEAN POLICY LOSS tensor(3373.7830, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 131\n",
      "MEAN POLICY LOSS tensor(732.7792, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 132\n",
      "MEAN POLICY LOSS tensor(210.0433, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 133\n",
      "MEAN POLICY LOSS tensor(1066.4025, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 134\n",
      "MEAN POLICY LOSS tensor(1399.4341, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 135\n",
      "MEAN POLICY LOSS tensor(1535.2365, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 136\n",
      "MEAN POLICY LOSS tensor(1433.9022, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 137\n",
      "MEAN POLICY LOSS tensor(586.5146, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 138\n",
      "MEAN POLICY LOSS tensor(1065.2292, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 139\n",
      "MEAN POLICY LOSS tensor(1408.5660, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 140\n",
      "MEAN POLICY LOSS tensor(1249.2885, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 141\n",
      "MEAN POLICY LOSS tensor(232.4527, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 142\n",
      "MEAN POLICY LOSS tensor(396.6045, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 143\n",
      "MEAN POLICY LOSS tensor(1045.5328, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 144\n",
      "MEAN POLICY LOSS tensor(32660.0039, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 145\n",
      "MEAN POLICY LOSS tensor(971.1084, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 146\n",
      "MEAN POLICY LOSS tensor(1447.3142, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 147\n",
      "MEAN POLICY LOSS tensor(797.3578, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 148\n",
      "MEAN POLICY LOSS tensor(390.1937, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 149\n",
      "MEAN POLICY LOSS tensor(262.8166, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 150\n",
      "MEAN POLICY LOSS tensor(714.2329, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 151\n",
      "MEAN POLICY LOSS tensor(619.4129, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 152\n",
      "MEAN POLICY LOSS tensor(164.3618, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 153\n",
      "MEAN POLICY LOSS tensor(931.7928, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 154\n",
      "MEAN POLICY LOSS tensor(169.1698, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 155\n",
      "MEAN POLICY LOSS tensor(706.2450, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 156\n",
      "MEAN POLICY LOSS tensor(27525.6074, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 157\n",
      "MEAN POLICY LOSS tensor(657.9570, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 158\n",
      "MEAN POLICY LOSS tensor(38311.7812, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 159\n",
      "MEAN POLICY LOSS tensor(1327.7526, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 160\n",
      "MEAN POLICY LOSS tensor(223.9601, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 161\n",
      "MEAN POLICY LOSS tensor(580.7642, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 162\n",
      "MEAN POLICY LOSS tensor(1138.6168, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 163\n",
      "MEAN POLICY LOSS tensor(1040.0255, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 164\n",
      "MEAN POLICY LOSS tensor(1594.3943, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 165\n",
      "MEAN POLICY LOSS tensor(22271.0117, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 166\n",
      "MEAN POLICY LOSS tensor(1418.3372, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 167\n",
      "MEAN POLICY LOSS tensor(507.8706, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 168\n",
      "MEAN POLICY LOSS tensor(600.5517, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 169\n",
      "MEAN POLICY LOSS tensor(32677.8008, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 170\n",
      "MEAN POLICY LOSS tensor(392.9522, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 171\n",
      "MEAN POLICY LOSS tensor(2221.8049, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 172\n",
      "MEAN POLICY LOSS tensor(2146.2295, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 173\n",
      "MEAN POLICY LOSS tensor(2187.6484, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 174\n",
      "MEAN POLICY LOSS tensor(23750.3125, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 175\n",
      "MEAN POLICY LOSS tensor(2315.5562, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 176\n",
      "MEAN POLICY LOSS tensor(347.5069, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 177\n",
      "MEAN POLICY LOSS tensor(812.8351, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 178\n",
      "MEAN POLICY LOSS tensor(1018.7403, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 179\n",
      "MEAN POLICY LOSS tensor(1340.6729, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 180\n",
      "MEAN POLICY LOSS tensor(2575.4797, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 181\n",
      "MEAN POLICY LOSS tensor(3435.0522, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 182\n",
      "MEAN POLICY LOSS tensor(953.1974, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 183\n",
      "MEAN POLICY LOSS tensor(1958.0791, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 184\n",
      "MEAN POLICY LOSS tensor(2851.9543, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 185\n",
      "MEAN POLICY LOSS tensor(2794.4526, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 186\n",
      "MEAN POLICY LOSS tensor(751.2891, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 187\n",
      "MEAN POLICY LOSS tensor(18397.0039, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 188\n",
      "MEAN POLICY LOSS tensor(1465.3578, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 189\n",
      "MEAN POLICY LOSS tensor(1770.2843, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 190\n",
      "MEAN POLICY LOSS tensor(1383.4576, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 191\n",
      "MEAN POLICY LOSS tensor(2296.1787, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 192\n",
      "MEAN POLICY LOSS tensor(2004.3135, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 193\n",
      "MEAN POLICY LOSS tensor(475.0872, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 194\n",
      "MEAN POLICY LOSS tensor(561.9112, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 195\n",
      "MEAN POLICY LOSS tensor(1600.1926, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 196\n",
      "MEAN POLICY LOSS tensor(266.6379, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 197\n",
      "MEAN POLICY LOSS tensor(307.2799, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 198\n",
      "MEAN POLICY LOSS tensor(1345.7227, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 199\n",
      "MEAN POLICY LOSS tensor(1717.3447, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 200\n",
      "MEAN POLICY LOSS tensor(228.7794, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 201\n",
      "MEAN POLICY LOSS tensor(1110.3842, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 202\n",
      "MEAN POLICY LOSS tensor(1767.6610, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 203\n",
      "MEAN POLICY LOSS tensor(1452.3195, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 204\n",
      "MEAN POLICY LOSS tensor(478.4768, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 205\n",
      "MEAN POLICY LOSS tensor(785.0408, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 206\n",
      "MEAN POLICY LOSS tensor(609.8424, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 207\n",
      "MEAN POLICY LOSS tensor(589.3389, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 208\n",
      "MEAN POLICY LOSS tensor(584.3430, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 209\n",
      "MEAN POLICY LOSS tensor(868.2686, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 210\n",
      "MEAN POLICY LOSS tensor(389.5771, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 211\n",
      "MEAN POLICY LOSS tensor(689.2894, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 212\n",
      "MEAN POLICY LOSS tensor(251.6081, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 213\n",
      "MEAN POLICY LOSS tensor(263.4763, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 214\n",
      "MEAN POLICY LOSS tensor(247.9384, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 215\n",
      "MEAN POLICY LOSS tensor(478.6817, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 216\n",
      "MEAN POLICY LOSS tensor(278.1618, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 217\n",
      "MEAN POLICY LOSS tensor(91.9571, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 218\n",
      "MEAN POLICY LOSS tensor(305.2276, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 219\n",
      "MEAN POLICY LOSS tensor(248.1633, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 220\n",
      "MEAN POLICY LOSS tensor(95.8667, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 221\n",
      "MEAN POLICY LOSS tensor(142.5900, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 222\n",
      "MEAN POLICY LOSS tensor(279.7712, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 223\n",
      "MEAN POLICY LOSS tensor(106.2379, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 224\n",
      "MEAN POLICY LOSS tensor(231.2595, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 225\n",
      "MEAN POLICY LOSS tensor(205.4493, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 226\n",
      "MEAN POLICY LOSS tensor(320.6877, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 227\n",
      "MEAN POLICY LOSS tensor(141.0723, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 228\n",
      "MEAN POLICY LOSS tensor(28.1367, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 229\n",
      "MEAN POLICY LOSS tensor(250.0693, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 230\n",
      "MEAN POLICY LOSS tensor(111.4372, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 231\n",
      "MEAN POLICY LOSS tensor(19.5696, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 232\n",
      "MEAN POLICY LOSS tensor(45022.2773, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 233\n",
      "MEAN POLICY LOSS tensor(265.6674, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 234\n",
      "MEAN POLICY LOSS tensor(163.0742, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 235\n",
      "MEAN POLICY LOSS tensor(72.6286, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 236\n",
      "MEAN POLICY LOSS tensor(251.0818, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 237\n",
      "MEAN POLICY LOSS tensor(44.4342, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 238\n",
      "MEAN POLICY LOSS tensor(86.4787, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 239\n",
      "MEAN POLICY LOSS tensor(435.1741, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 240\n",
      "MEAN POLICY LOSS tensor(723.7216, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 241\n",
      "MEAN POLICY LOSS tensor(319.6112, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 242\n",
      "MEAN POLICY LOSS tensor(397.4097, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 243\n",
      "MEAN POLICY LOSS tensor(832.2034, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 244\n",
      "MEAN POLICY LOSS tensor(30144.4414, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 245\n",
      "MEAN POLICY LOSS tensor(403.0118, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 246\n",
      "MEAN POLICY LOSS tensor(593.4947, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 247\n",
      "MEAN POLICY LOSS tensor(211.0952, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 248\n",
      "MEAN POLICY LOSS tensor(221.9119, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 249\n",
      "MEAN POLICY LOSS tensor(788.0102, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 250\n",
      "MEAN POLICY LOSS tensor(591.0682, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 251\n",
      "MEAN POLICY LOSS tensor(159.4815, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 252\n",
      "MEAN POLICY LOSS tensor(305.1964, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 253\n",
      "MEAN POLICY LOSS tensor(555.2068, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 254\n",
      "MEAN POLICY LOSS tensor(742.9240, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 255\n",
      "MEAN POLICY LOSS tensor(27380.2090, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 256\n",
      "MEAN POLICY LOSS tensor(386.1719, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 257\n",
      "MEAN POLICY LOSS tensor(364.8360, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 258\n",
      "MEAN POLICY LOSS tensor(142.4062, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 259\n",
      "MEAN POLICY LOSS tensor(1001.3310, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 260\n",
      "MEAN POLICY LOSS tensor(343.2676, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 261\n",
      "MEAN POLICY LOSS tensor(869.4460, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 262\n",
      "MEAN POLICY LOSS tensor(253.7095, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 263\n",
      "MEAN POLICY LOSS tensor(113.4370, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 264\n",
      "MEAN POLICY LOSS tensor(1808.0104, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 265\n",
      "MEAN POLICY LOSS tensor(1237.5494, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 266\n",
      "MEAN POLICY LOSS tensor(1066.6910, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 267\n",
      "MEAN POLICY LOSS tensor(798.0477, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 268\n",
      "MEAN POLICY LOSS tensor(515.9424, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 269\n",
      "MEAN POLICY LOSS tensor(30088.2598, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 270\n",
      "MEAN POLICY LOSS tensor(161.5146, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 271\n",
      "MEAN POLICY LOSS tensor(21233.0020, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 272\n",
      "MEAN POLICY LOSS tensor(1394.3761, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 273\n",
      "MEAN POLICY LOSS tensor(670.8826, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 274\n",
      "MEAN POLICY LOSS tensor(1911.3048, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 275\n",
      "MEAN POLICY LOSS tensor(443.4486, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 276\n",
      "MEAN POLICY LOSS tensor(15916.1553, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 277\n",
      "MEAN POLICY LOSS tensor(1374.9329, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 278\n",
      "MEAN POLICY LOSS tensor(2631.1904, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 279\n",
      "MEAN POLICY LOSS tensor(2309.4045, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 280\n",
      "MEAN POLICY LOSS tensor(2821.4424, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 281\n",
      "MEAN POLICY LOSS tensor(15624.4131, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 282\n",
      "MEAN POLICY LOSS tensor(807.4222, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 283\n",
      "MEAN POLICY LOSS tensor(2402.9866, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 284\n",
      "MEAN POLICY LOSS tensor(515.5414, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 285\n",
      "MEAN POLICY LOSS tensor(14178.5342, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 286\n",
      "MEAN POLICY LOSS tensor(2632.7532, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 287\n",
      "MEAN POLICY LOSS tensor(2835.5408, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 288\n",
      "MEAN POLICY LOSS tensor(4127.5098, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 289\n",
      "MEAN POLICY LOSS tensor(2664.8865, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 290\n",
      "MEAN POLICY LOSS tensor(1318.1051, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 291\n",
      "MEAN POLICY LOSS tensor(1070.4546, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 292\n",
      "MEAN POLICY LOSS tensor(1689.2107, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 293\n",
      "MEAN POLICY LOSS tensor(1571.1489, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 294\n",
      "MEAN POLICY LOSS tensor(25977.1934, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 295\n",
      "MEAN POLICY LOSS tensor(1145.5261, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 296\n",
      "MEAN POLICY LOSS tensor(692.3893, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 297\n",
      "MEAN POLICY LOSS tensor(14436.2686, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 298\n",
      "MEAN POLICY LOSS tensor(732.3484, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 299\n",
      "MEAN POLICY LOSS tensor(2482.7417, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 300\n",
      "MEAN POLICY LOSS tensor(434.0141, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 301\n",
      "MEAN POLICY LOSS tensor(1141.4447, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 302\n",
      "MEAN POLICY LOSS tensor(2506.4382, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 303\n",
      "MEAN POLICY LOSS tensor(12846.6895, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 304\n",
      "MEAN POLICY LOSS tensor(1290.1483, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 305\n",
      "MEAN POLICY LOSS tensor(11261.4766, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 306\n",
      "MEAN POLICY LOSS tensor(1449.9819, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 307\n",
      "MEAN POLICY LOSS tensor(37998.7422, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 308\n",
      "MEAN POLICY LOSS tensor(180.2104, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 309\n",
      "MEAN POLICY LOSS tensor(2361.6711, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 310\n",
      "MEAN POLICY LOSS tensor(3806.0447, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 311\n",
      "MEAN POLICY LOSS tensor(9479.9463, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 312\n",
      "MEAN POLICY LOSS tensor(12397.6514, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 313\n",
      "MEAN POLICY LOSS tensor(2030.4153, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 314\n",
      "MEAN POLICY LOSS tensor(2810.6042, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 315\n",
      "MEAN POLICY LOSS tensor(5743.1543, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 316\n",
      "MEAN POLICY LOSS tensor(7013.1846, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 317\n",
      "MEAN POLICY LOSS tensor(3575.2173, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 318\n",
      "MEAN POLICY LOSS tensor(8007.9824, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 319\n",
      "MEAN POLICY LOSS tensor(17218.1348, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 320\n",
      "MEAN POLICY LOSS tensor(2710.6704, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 321\n",
      "MEAN POLICY LOSS tensor(5552.6865, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 322\n",
      "MEAN POLICY LOSS tensor(1994.8582, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 323\n",
      "MEAN POLICY LOSS tensor(4051.4458, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 324\n",
      "MEAN POLICY LOSS tensor(1061.7772, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 325\n",
      "MEAN POLICY LOSS tensor(7579.4146, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 326\n",
      "MEAN POLICY LOSS tensor(12058.9189, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 327\n",
      "MEAN POLICY LOSS tensor(12902.7188, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 328\n",
      "MEAN POLICY LOSS tensor(6897.4038, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 329\n",
      "MEAN POLICY LOSS tensor(1161.6301, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 330\n",
      "MEAN POLICY LOSS tensor(2116.3601, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 331\n",
      "MEAN POLICY LOSS tensor(28130.0898, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 332\n",
      "MEAN POLICY LOSS tensor(2967.9631, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 333\n",
      "MEAN POLICY LOSS tensor(4402.3740, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 334\n",
      "MEAN POLICY LOSS tensor(6849.0674, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 335\n",
      "MEAN POLICY LOSS tensor(641.0894, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 336\n",
      "MEAN POLICY LOSS tensor(10098.4307, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 337\n",
      "MEAN POLICY LOSS tensor(4162.7886, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 338\n",
      "MEAN POLICY LOSS tensor(5647.6680, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 339\n",
      "MEAN POLICY LOSS tensor(1156.8077, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 340\n",
      "MEAN POLICY LOSS tensor(2779.8459, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 341\n",
      "MEAN POLICY LOSS tensor(1705.4154, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 342\n",
      "MEAN POLICY LOSS tensor(10666.3125, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 343\n",
      "MEAN POLICY LOSS tensor(8046.9517, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 344\n",
      "MEAN POLICY LOSS tensor(9125.3418, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 345\n",
      "MEAN POLICY LOSS tensor(1438.8334, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 346\n",
      "MEAN POLICY LOSS tensor(4855.7236, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 347\n",
      "MEAN POLICY LOSS tensor(35398.9922, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 348\n",
      "MEAN POLICY LOSS tensor(3512.0334, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 349\n",
      "MEAN POLICY LOSS tensor(3761.6882, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 350\n",
      "MEAN POLICY LOSS tensor(6829.0127, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 351\n",
      "MEAN POLICY LOSS tensor(4065.3762, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 352\n",
      "MEAN POLICY LOSS tensor(502.0322, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 353\n",
      "MEAN POLICY LOSS tensor(1999.9917, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 354\n",
      "MEAN POLICY LOSS tensor(1713.8635, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 355\n",
      "MEAN POLICY LOSS tensor(905.0162, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 356\n",
      "MEAN POLICY LOSS tensor(3335.1067, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 357\n",
      "MEAN POLICY LOSS tensor(3862.4204, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 358\n",
      "MEAN POLICY LOSS tensor(4015.7827, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 359\n",
      "MEAN POLICY LOSS tensor(1212.7632, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 360\n",
      "MEAN POLICY LOSS tensor(2297.2161, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 361\n",
      "MEAN POLICY LOSS tensor(798.7368, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 362\n",
      "MEAN POLICY LOSS tensor(562.6237, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 363\n",
      "MEAN POLICY LOSS tensor(792.7003, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 364\n",
      "MEAN POLICY LOSS tensor(716.9810, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 365\n",
      "MEAN POLICY LOSS tensor(184.1858, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 366\n",
      "MEAN POLICY LOSS tensor(223.9523, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 367\n",
      "MEAN POLICY LOSS tensor(31737.8535, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 368\n",
      "MEAN POLICY LOSS tensor(145.7191, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 369\n",
      "MEAN POLICY LOSS tensor(326.5651, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 370\n",
      "MEAN POLICY LOSS tensor(20161.9707, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 371\n",
      "MEAN POLICY LOSS tensor(736.2487, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 372\n",
      "MEAN POLICY LOSS tensor(992.0703, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 373\n",
      "MEAN POLICY LOSS tensor(19427.4707, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 374\n",
      "MEAN POLICY LOSS tensor(454.2150, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 375\n",
      "MEAN POLICY LOSS tensor(5642.0620, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 376\n",
      "MEAN POLICY LOSS tensor(173.0088, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 377\n",
      "MEAN POLICY LOSS tensor(4659.5410, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 378\n",
      "MEAN POLICY LOSS tensor(326.1873, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 379\n",
      "MEAN POLICY LOSS tensor(2005.9071, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 380\n",
      "MEAN POLICY LOSS tensor(1027.1820, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 381\n",
      "MEAN POLICY LOSS tensor(569.0755, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 382\n",
      "MEAN POLICY LOSS tensor(5143.5205, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 383\n",
      "MEAN POLICY LOSS tensor(43.3786, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 384\n",
      "MEAN POLICY LOSS tensor(12883.6514, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 385\n",
      "MEAN POLICY LOSS tensor(890.6656, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 386\n",
      "MEAN POLICY LOSS tensor(1531.4584, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 387\n",
      "MEAN POLICY LOSS tensor(1380.3071, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 388\n",
      "MEAN POLICY LOSS tensor(240.5541, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 389\n",
      "MEAN POLICY LOSS tensor(1584.0638, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 390\n",
      "MEAN POLICY LOSS tensor(1005.5892, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 391\n",
      "MEAN POLICY LOSS tensor(710.9252, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 392\n",
      "MEAN POLICY LOSS tensor(589.0764, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 393\n",
      "MEAN POLICY LOSS tensor(3957.4434, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 394\n",
      "MEAN POLICY LOSS tensor(3368.0415, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 395\n",
      "MEAN POLICY LOSS tensor(56017.3125, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 396\n",
      "MEAN POLICY LOSS tensor(1762.7986, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 397\n",
      "MEAN POLICY LOSS tensor(1800.9906, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 398\n",
      "MEAN POLICY LOSS tensor(1759.4458, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 399\n",
      "MEAN POLICY LOSS tensor(532.9739, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 400\n",
      "MEAN POLICY LOSS tensor(2067.1870, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 401\n",
      "MEAN POLICY LOSS tensor(1365.2902, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 402\n",
      "MEAN POLICY LOSS tensor(1173.2693, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 403\n",
      "MEAN POLICY LOSS tensor(1016.4045, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 404\n",
      "MEAN POLICY LOSS tensor(542.9279, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 405\n",
      "MEAN POLICY LOSS tensor(277.7845, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 406\n",
      "MEAN POLICY LOSS tensor(196.6936, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 407\n",
      "MEAN POLICY LOSS tensor(1090.1498, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 408\n",
      "MEAN POLICY LOSS tensor(243.9422, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 409\n",
      "MEAN POLICY LOSS tensor(495.2408, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 410\n",
      "MEAN POLICY LOSS tensor(30179.4512, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 411\n",
      "MEAN POLICY LOSS tensor(30151.9434, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 412\n",
      "MEAN POLICY LOSS tensor(1273.3456, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 413\n",
      "MEAN POLICY LOSS tensor(986.3024, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 414\n",
      "MEAN POLICY LOSS tensor(586.4974, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 415\n",
      "MEAN POLICY LOSS tensor(1099.9779, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 416\n",
      "MEAN POLICY LOSS tensor(149.7678, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 417\n",
      "MEAN POLICY LOSS tensor(2816.1797, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 418\n",
      "MEAN POLICY LOSS tensor(1450.6364, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 419\n",
      "MEAN POLICY LOSS tensor(720.6848, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 420\n",
      "MEAN POLICY LOSS tensor(2570.4700, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 421\n",
      "MEAN POLICY LOSS tensor(427.9635, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 422\n",
      "MEAN POLICY LOSS tensor(1081.8850, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 423\n",
      "MEAN POLICY LOSS tensor(4776.9287, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 424\n",
      "MEAN POLICY LOSS tensor(946.9080, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 425\n",
      "MEAN POLICY LOSS tensor(331.8664, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 426\n",
      "MEAN POLICY LOSS tensor(2490.8718, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 427\n",
      "MEAN POLICY LOSS tensor(921.7515, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 428\n",
      "MEAN POLICY LOSS tensor(232.3094, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 429\n",
      "MEAN POLICY LOSS tensor(88.9012, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 430\n",
      "MEAN POLICY LOSS tensor(870.2310, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 431\n",
      "MEAN POLICY LOSS tensor(981.8379, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 432\n",
      "MEAN POLICY LOSS tensor(69.2880, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 433\n",
      "MEAN POLICY LOSS tensor(1645.6447, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 434\n",
      "MEAN POLICY LOSS tensor(385.9164, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 435\n",
      "MEAN POLICY LOSS tensor(552.9834, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 436\n",
      "MEAN POLICY LOSS tensor(720.6993, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 437\n",
      "MEAN POLICY LOSS tensor(251.2087, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 438\n",
      "MEAN POLICY LOSS tensor(320.2089, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 439\n",
      "MEAN POLICY LOSS tensor(684.9813, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 440\n",
      "MEAN POLICY LOSS tensor(440.9647, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 441\n",
      "MEAN POLICY LOSS tensor(264.3590, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 442\n",
      "MEAN POLICY LOSS tensor(354.2942, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 443\n",
      "MEAN POLICY LOSS tensor(72.8407, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 444\n",
      "MEAN POLICY LOSS tensor(455.4936, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 445\n",
      "MEAN POLICY LOSS tensor(92.3045, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 446\n",
      "MEAN POLICY LOSS tensor(30871.7070, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 447\n",
      "MEAN POLICY LOSS tensor(233.3786, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 448\n",
      "MEAN POLICY LOSS tensor(46834.1836, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 449\n",
      "MEAN POLICY LOSS tensor(627.3889, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 450\n",
      "MEAN POLICY LOSS tensor(401.2316, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 451\n",
      "MEAN POLICY LOSS tensor(384.6344, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 452\n",
      "MEAN POLICY LOSS tensor(355.4560, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 453\n",
      "MEAN POLICY LOSS tensor(317.7899, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 454\n",
      "MEAN POLICY LOSS tensor(502.8122, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 455\n",
      "MEAN POLICY LOSS tensor(989.7704, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 456\n",
      "MEAN POLICY LOSS tensor(52.8842, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 457\n",
      "MEAN POLICY LOSS tensor(931.5912, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 458\n",
      "MEAN POLICY LOSS tensor(803.2712, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 459\n",
      "MEAN POLICY LOSS tensor(717.9766, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 460\n",
      "MEAN POLICY LOSS tensor(366.4714, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 461\n",
      "MEAN POLICY LOSS tensor(664.6311, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 462\n",
      "MEAN POLICY LOSS tensor(696.7788, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 463\n",
      "MEAN POLICY LOSS tensor(560.4257, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 464\n",
      "MEAN POLICY LOSS tensor(768.8921, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 465\n",
      "MEAN POLICY LOSS tensor(40950.0195, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 466\n",
      "MEAN POLICY LOSS tensor(505.0320, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 467\n",
      "MEAN POLICY LOSS tensor(794.8555, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 468\n",
      "MEAN POLICY LOSS tensor(496.2008, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 469\n",
      "MEAN POLICY LOSS tensor(28414.0742, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 470\n",
      "MEAN POLICY LOSS tensor(726.4705, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 471\n",
      "MEAN POLICY LOSS tensor(1825.4406, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 472\n",
      "MEAN POLICY LOSS tensor(1719.0696, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 473\n",
      "MEAN POLICY LOSS tensor(23448.1484, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 474\n",
      "MEAN POLICY LOSS tensor(32362.7031, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 475\n",
      "MEAN POLICY LOSS tensor(791.2823, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 476\n",
      "MEAN POLICY LOSS tensor(592.5212, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 477\n",
      "MEAN POLICY LOSS tensor(3686.6270, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 478\n",
      "MEAN POLICY LOSS tensor(547.0150, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 479\n",
      "MEAN POLICY LOSS tensor(1705.6514, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 480\n",
      "MEAN POLICY LOSS tensor(1015.0237, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 481\n",
      "MEAN POLICY LOSS tensor(18735.8555, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 482\n",
      "MEAN POLICY LOSS tensor(3362.9124, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 483\n",
      "MEAN POLICY LOSS tensor(2790.2371, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 484\n",
      "MEAN POLICY LOSS tensor(698.6250, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 485\n",
      "MEAN POLICY LOSS tensor(5529.3115, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 486\n",
      "MEAN POLICY LOSS tensor(15685.0068, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 487\n",
      "MEAN POLICY LOSS tensor(2588.2659, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 488\n",
      "MEAN POLICY LOSS tensor(25047.6426, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 489\n",
      "MEAN POLICY LOSS tensor(2415.1130, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 490\n",
      "MEAN POLICY LOSS tensor(370.2970, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 491\n",
      "MEAN POLICY LOSS tensor(1167.3419, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 492\n",
      "MEAN POLICY LOSS tensor(20728.7188, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 493\n",
      "MEAN POLICY LOSS tensor(3885.5532, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 494\n",
      "MEAN POLICY LOSS tensor(2246.6829, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 495\n",
      "MEAN POLICY LOSS tensor(841.7625, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 496\n",
      "MEAN POLICY LOSS tensor(5479.0186, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 497\n",
      "MEAN POLICY LOSS tensor(14676.1689, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 498\n",
      "MEAN POLICY LOSS tensor(1846.6798, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 499\n",
      "MEAN POLICY LOSS tensor(20029.0410, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 500\n",
      "MEAN POLICY LOSS tensor(5276.0347, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 501\n",
      "MEAN POLICY LOSS tensor(2611.9150, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 502\n",
      "MEAN POLICY LOSS tensor(2075.5513, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 503\n",
      "MEAN POLICY LOSS tensor(1341.8615, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 504\n",
      "MEAN POLICY LOSS tensor(3262.0515, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 505\n",
      "MEAN POLICY LOSS tensor(1098.2380, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 506\n",
      "MEAN POLICY LOSS tensor(3246.8481, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 507\n",
      "MEAN POLICY LOSS tensor(22470.0859, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 508\n",
      "MEAN POLICY LOSS tensor(2447.1675, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 509\n",
      "MEAN POLICY LOSS tensor(11379.5234, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 510\n",
      "MEAN POLICY LOSS tensor(22646.7852, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 511\n",
      "MEAN POLICY LOSS tensor(1721.0833, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 512\n",
      "MEAN POLICY LOSS tensor(2538.5835, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 513\n",
      "MEAN POLICY LOSS tensor(13702.3281, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 514\n",
      "MEAN POLICY LOSS tensor(3582.0347, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 515\n",
      "MEAN POLICY LOSS tensor(10776.4990, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 516\n",
      "MEAN POLICY LOSS tensor(803.0788, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 517\n",
      "MEAN POLICY LOSS tensor(24096.6719, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 518\n",
      "MEAN POLICY LOSS tensor(7112.3179, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 519\n",
      "MEAN POLICY LOSS tensor(18327.3438, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 520\n",
      "MEAN POLICY LOSS tensor(23108.8223, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 521\n",
      "MEAN POLICY LOSS tensor(22022.8945, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 522\n",
      "MEAN POLICY LOSS tensor(8600.5225, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 523\n",
      "MEAN POLICY LOSS tensor(13019.9980, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 524\n",
      "MEAN POLICY LOSS tensor(17780.1758, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 525\n",
      "MEAN POLICY LOSS tensor(1760.9398, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 526\n",
      "MEAN POLICY LOSS tensor(13194.1758, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 527\n",
      "MEAN POLICY LOSS tensor(14922.5156, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 528\n",
      "MEAN POLICY LOSS tensor(17484.4023, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 529\n",
      "MEAN POLICY LOSS tensor(6751.5103, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 530\n",
      "MEAN POLICY LOSS tensor(9157.6621, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 531\n",
      "MEAN POLICY LOSS tensor(12553.1836, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 532\n",
      "MEAN POLICY LOSS tensor(10984.3672, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 533\n",
      "MEAN POLICY LOSS tensor(2440.4192, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 534\n",
      "MEAN POLICY LOSS tensor(9619.0928, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 535\n",
      "MEAN POLICY LOSS tensor(10872.6475, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 536\n",
      "MEAN POLICY LOSS tensor(10136.2568, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 537\n",
      "MEAN POLICY LOSS tensor(9939.9482, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 538\n",
      "MEAN POLICY LOSS tensor(5144.2764, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 539\n",
      "MEAN POLICY LOSS tensor(10200.8955, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 540\n",
      "MEAN POLICY LOSS tensor(8851.2061, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 541\n",
      "MEAN POLICY LOSS tensor(10835.1426, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 542\n",
      "MEAN POLICY LOSS tensor(11224.2520, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 543\n",
      "MEAN POLICY LOSS tensor(10098.1807, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 544\n",
      "MEAN POLICY LOSS tensor(9798.9648, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 545\n",
      "MEAN POLICY LOSS tensor(11218.9297, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 546\n",
      "MEAN POLICY LOSS tensor(11161.9258, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 547\n",
      "MEAN POLICY LOSS tensor(13355.6387, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 548\n",
      "MEAN POLICY LOSS tensor(10779.2129, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 549\n",
      "MEAN POLICY LOSS tensor(9717.1445, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 550\n",
      "MEAN POLICY LOSS tensor(10657.2842, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 551\n",
      "MEAN POLICY LOSS tensor(10718.6123, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 552\n",
      "MEAN POLICY LOSS tensor(9902.5576, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 553\n",
      "MEAN POLICY LOSS tensor(8178.4917, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 554\n",
      "MEAN POLICY LOSS tensor(9825.9727, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 555\n",
      "MEAN POLICY LOSS tensor(7150.9141, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 556\n",
      "MEAN POLICY LOSS tensor(8657.2930, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 557\n",
      "MEAN POLICY LOSS tensor(9331.3379, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 558\n",
      "MEAN POLICY LOSS tensor(9996.8486, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 559\n",
      "MEAN POLICY LOSS tensor(9375.8232, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 560\n",
      "MEAN POLICY LOSS tensor(9434.1338, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 561\n",
      "MEAN POLICY LOSS tensor(9088.9590, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 562\n",
      "MEAN POLICY LOSS tensor(8778.6201, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 563\n",
      "MEAN POLICY LOSS tensor(8256.7090, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 564\n",
      "MEAN POLICY LOSS tensor(8160.1733, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 565\n",
      "MEAN POLICY LOSS tensor(8588.8193, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 566\n",
      "MEAN POLICY LOSS tensor(4987.5200, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 567\n",
      "MEAN POLICY LOSS tensor(7835.8989, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 568\n",
      "MEAN POLICY LOSS tensor(7411.0728, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 569\n",
      "MEAN POLICY LOSS tensor(7258.8579, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 570\n",
      "MEAN POLICY LOSS tensor(9017.8652, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 571\n",
      "MEAN POLICY LOSS tensor(7198.5596, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 572\n",
      "MEAN POLICY LOSS tensor(9745.2793, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 573\n",
      "MEAN POLICY LOSS tensor(8262.1846, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 574\n",
      "MEAN POLICY LOSS tensor(28049.2051, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 575\n",
      "MEAN POLICY LOSS tensor(6105.4482, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 576\n",
      "MEAN POLICY LOSS tensor(4956.3911, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 577\n",
      "MEAN POLICY LOSS tensor(6095.3677, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 578\n",
      "MEAN POLICY LOSS tensor(7230.2246, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 579\n",
      "MEAN POLICY LOSS tensor(9597.4482, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 580\n",
      "MEAN POLICY LOSS tensor(6529.2036, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 581\n",
      "MEAN POLICY LOSS tensor(8861.5605, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 582\n",
      "MEAN POLICY LOSS tensor(5003.2666, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 583\n",
      "MEAN POLICY LOSS tensor(7735.1562, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 584\n",
      "MEAN POLICY LOSS tensor(23842.7832, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 585\n",
      "MEAN POLICY LOSS tensor(9087.1719, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 586\n",
      "MEAN POLICY LOSS tensor(2519.3889, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 587\n",
      "MEAN POLICY LOSS tensor(21414.7109, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 588\n",
      "MEAN POLICY LOSS tensor(8138.2705, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 589\n",
      "MEAN POLICY LOSS tensor(21261.5996, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 590\n",
      "MEAN POLICY LOSS tensor(7551.6548, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 591\n",
      "MEAN POLICY LOSS tensor(4879.7612, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 592\n",
      "MEAN POLICY LOSS tensor(6372.8057, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 593\n",
      "MEAN POLICY LOSS tensor(8122.7524, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 594\n",
      "MEAN POLICY LOSS tensor(5164.3716, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 595\n",
      "MEAN POLICY LOSS tensor(7962.4839, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 596\n",
      "MEAN POLICY LOSS tensor(9242.6084, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 597\n",
      "MEAN POLICY LOSS tensor(10196.5791, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 598\n",
      "MEAN POLICY LOSS tensor(6609.1758, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 599\n",
      "MEAN POLICY LOSS tensor(15450.6211, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 600\n",
      "MEAN POLICY LOSS tensor(7839.6357, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 601\n",
      "MEAN POLICY LOSS tensor(1541.9321, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 602\n",
      "MEAN POLICY LOSS tensor(15680.2188, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 603\n",
      "MEAN POLICY LOSS tensor(3334.9441, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 604\n",
      "MEAN POLICY LOSS tensor(5133.7417, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 605\n",
      "MEAN POLICY LOSS tensor(3875.0276, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 606\n",
      "MEAN POLICY LOSS tensor(15355.2158, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 607\n",
      "MEAN POLICY LOSS tensor(3450.6267, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 608\n",
      "MEAN POLICY LOSS tensor(12574.0820, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 609\n",
      "MEAN POLICY LOSS tensor(11500.3545, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 610\n",
      "MEAN POLICY LOSS tensor(4548.6357, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 611\n",
      "MEAN POLICY LOSS tensor(11813.6035, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 612\n",
      "MEAN POLICY LOSS tensor(4512.9268, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 613\n",
      "MEAN POLICY LOSS tensor(6365.8247, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 614\n",
      "MEAN POLICY LOSS tensor(16681.3262, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 615\n",
      "MEAN POLICY LOSS tensor(759.3961, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 616\n",
      "MEAN POLICY LOSS tensor(4594.6392, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 617\n",
      "MEAN POLICY LOSS tensor(7055.9248, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 618\n",
      "MEAN POLICY LOSS tensor(16068.1855, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 619\n",
      "MEAN POLICY LOSS tensor(11931.5352, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 620\n",
      "MEAN POLICY LOSS tensor(5726.4097, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 621\n",
      "MEAN POLICY LOSS tensor(18104.3398, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 622\n",
      "MEAN POLICY LOSS tensor(7100.9048, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 623\n",
      "MEAN POLICY LOSS tensor(3162.9153, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 624\n",
      "MEAN POLICY LOSS tensor(26769.3223, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 625\n",
      "MEAN POLICY LOSS tensor(5124.4966, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 626\n",
      "MEAN POLICY LOSS tensor(10971.3408, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 627\n",
      "MEAN POLICY LOSS tensor(1040.3280, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 628\n",
      "MEAN POLICY LOSS tensor(532.8325, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 629\n",
      "MEAN POLICY LOSS tensor(20444.3438, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 630\n",
      "MEAN POLICY LOSS tensor(13484.6963, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 631\n",
      "MEAN POLICY LOSS tensor(5914.9922, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 632\n",
      "MEAN POLICY LOSS tensor(835.1610, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 633\n",
      "MEAN POLICY LOSS tensor(19538.4434, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 634\n",
      "MEAN POLICY LOSS tensor(8065.2822, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 635\n",
      "MEAN POLICY LOSS tensor(10478.0127, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 636\n",
      "MEAN POLICY LOSS tensor(17230.1504, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 637\n",
      "MEAN POLICY LOSS tensor(2477.4968, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 638\n",
      "MEAN POLICY LOSS tensor(4473.5469, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 639\n",
      "MEAN POLICY LOSS tensor(2317.3918, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 640\n",
      "MEAN POLICY LOSS tensor(8758.6221, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 641\n",
      "MEAN POLICY LOSS tensor(8691.2158, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 642\n",
      "MEAN POLICY LOSS tensor(12784.0361, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 643\n",
      "MEAN POLICY LOSS tensor(15689.9492, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 644\n",
      "MEAN POLICY LOSS tensor(8675.5049, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 645\n",
      "MEAN POLICY LOSS tensor(3753.8162, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 646\n",
      "MEAN POLICY LOSS tensor(8110.5288, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 647\n",
      "MEAN POLICY LOSS tensor(5670.8906, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 648\n",
      "MEAN POLICY LOSS tensor(9033.1689, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 649\n",
      "MEAN POLICY LOSS tensor(21543.2266, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 650\n",
      "MEAN POLICY LOSS tensor(7508.0874, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 651\n",
      "MEAN POLICY LOSS tensor(8640.7451, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 652\n",
      "MEAN POLICY LOSS tensor(7676.8193, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 653\n",
      "MEAN POLICY LOSS tensor(8134.6592, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 654\n",
      "MEAN POLICY LOSS tensor(7836.3911, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 655\n",
      "MEAN POLICY LOSS tensor(7832.3467, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 656\n",
      "MEAN POLICY LOSS tensor(6000.6206, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 657\n",
      "MEAN POLICY LOSS tensor(7378.7090, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 658\n",
      "MEAN POLICY LOSS tensor(10193.1445, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 659\n",
      "MEAN POLICY LOSS tensor(7630.8101, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 660\n",
      "MEAN POLICY LOSS tensor(7656.5308, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 661\n",
      "MEAN POLICY LOSS tensor(11221.3047, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 662\n",
      "MEAN POLICY LOSS tensor(6747.8276, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 663\n",
      "MEAN POLICY LOSS tensor(5285.0938, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 664\n",
      "MEAN POLICY LOSS tensor(6527.6772, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 665\n",
      "MEAN POLICY LOSS tensor(21383.3047, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 666\n",
      "MEAN POLICY LOSS tensor(6688.8892, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 667\n",
      "MEAN POLICY LOSS tensor(18522.1289, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 668\n",
      "MEAN POLICY LOSS tensor(10905.0342, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 669\n",
      "MEAN POLICY LOSS tensor(10100.1797, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 670\n",
      "MEAN POLICY LOSS tensor(15977.4229, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 671\n",
      "MEAN POLICY LOSS tensor(10853.9775, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 672\n",
      "MEAN POLICY LOSS tensor(6723.5068, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 673\n",
      "MEAN POLICY LOSS tensor(11651.9834, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 674\n",
      "MEAN POLICY LOSS tensor(19873.4023, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 675\n",
      "MEAN POLICY LOSS tensor(7167.9507, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 676\n",
      "MEAN POLICY LOSS tensor(26493.2656, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 677\n",
      "MEAN POLICY LOSS tensor(12051.1357, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 678\n",
      "MEAN POLICY LOSS tensor(7483.5176, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 679\n",
      "MEAN POLICY LOSS tensor(11678.2031, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 680\n",
      "MEAN POLICY LOSS tensor(7809.7832, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 681\n",
      "MEAN POLICY LOSS tensor(7627.6357, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 682\n",
      "MEAN POLICY LOSS tensor(14228.0146, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 683\n",
      "MEAN POLICY LOSS tensor(7118.6455, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 684\n",
      "MEAN POLICY LOSS tensor(10110.9521, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 685\n",
      "MEAN POLICY LOSS tensor(8443.0205, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 686\n",
      "MEAN POLICY LOSS tensor(7877.1587, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 687\n",
      "MEAN POLICY LOSS tensor(11525.8223, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 688\n",
      "MEAN POLICY LOSS tensor(9413.7852, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 689\n",
      "MEAN POLICY LOSS tensor(8007.3115, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 690\n",
      "MEAN POLICY LOSS tensor(10109.8545, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 691\n",
      "MEAN POLICY LOSS tensor(17048.2148, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 692\n",
      "MEAN POLICY LOSS tensor(11197.6416, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 693\n",
      "MEAN POLICY LOSS tensor(11149.3535, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 694\n",
      "MEAN POLICY LOSS tensor(8349.2412, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 695\n",
      "MEAN POLICY LOSS tensor(7715.7290, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 696\n",
      "MEAN POLICY LOSS tensor(14920.6123, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 697\n",
      "MEAN POLICY LOSS tensor(4775.7793, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 698\n",
      "MEAN POLICY LOSS tensor(7846.0522, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 699\n",
      "MEAN POLICY LOSS tensor(10848.6035, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 700\n",
      "MEAN POLICY LOSS tensor(4372.6665, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 701\n",
      "MEAN POLICY LOSS tensor(15588.4697, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 702\n",
      "MEAN POLICY LOSS tensor(10601.2451, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 703\n",
      "MEAN POLICY LOSS tensor(7448.3682, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 704\n",
      "MEAN POLICY LOSS tensor(10378.2676, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 705\n",
      "MEAN POLICY LOSS tensor(4306.4429, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 706\n",
      "MEAN POLICY LOSS tensor(6350.5840, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 707\n",
      "MEAN POLICY LOSS tensor(8434.6104, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 708\n",
      "MEAN POLICY LOSS tensor(5349.2100, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 709\n",
      "MEAN POLICY LOSS tensor(8521.2764, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 710\n",
      "MEAN POLICY LOSS tensor(16743.2441, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 711\n",
      "MEAN POLICY LOSS tensor(9955.8652, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 712\n",
      "MEAN POLICY LOSS tensor(13900.6016, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 713\n",
      "MEAN POLICY LOSS tensor(8377.1943, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 714\n",
      "MEAN POLICY LOSS tensor(14045.8340, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 715\n",
      "MEAN POLICY LOSS tensor(9932.6455, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 716\n",
      "MEAN POLICY LOSS tensor(8703.2422, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 717\n",
      "MEAN POLICY LOSS tensor(14390.2314, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 718\n",
      "MEAN POLICY LOSS tensor(19074.8496, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 719\n",
      "MEAN POLICY LOSS tensor(9747.8369, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 720\n",
      "MEAN POLICY LOSS tensor(4218.9800, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 721\n",
      "MEAN POLICY LOSS tensor(12852.4883, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 722\n",
      "MEAN POLICY LOSS tensor(3086.2012, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 723\n",
      "MEAN POLICY LOSS tensor(9248.3535, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 724\n",
      "MEAN POLICY LOSS tensor(242.0270, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 725\n",
      "MEAN POLICY LOSS tensor(9855.3096, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 726\n",
      "MEAN POLICY LOSS tensor(8606.9336, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 727\n",
      "MEAN POLICY LOSS tensor(10013.2676, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 728\n",
      "MEAN POLICY LOSS tensor(1788.4115, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 729\n",
      "MEAN POLICY LOSS tensor(10015.7549, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 730\n",
      "MEAN POLICY LOSS tensor(9641.7861, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 731\n",
      "MEAN POLICY LOSS tensor(9531.6396, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 732\n",
      "MEAN POLICY LOSS tensor(13999.9902, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 733\n",
      "MEAN POLICY LOSS tensor(6617.4775, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 734\n",
      "MEAN POLICY LOSS tensor(8493.0908, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 735\n",
      "MEAN POLICY LOSS tensor(1687.9404, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 736\n",
      "MEAN POLICY LOSS tensor(9132.3525, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 737\n",
      "MEAN POLICY LOSS tensor(7897.2305, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 738\n",
      "MEAN POLICY LOSS tensor(10080.1982, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 739\n",
      "MEAN POLICY LOSS tensor(8606.1758, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 740\n",
      "MEAN POLICY LOSS tensor(8347.2480, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 741\n",
      "MEAN POLICY LOSS tensor(9294.9502, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 742\n",
      "MEAN POLICY LOSS tensor(10054.3740, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 743\n",
      "MEAN POLICY LOSS tensor(8877.9775, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 744\n",
      "MEAN POLICY LOSS tensor(14893.8428, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 745\n",
      "MEAN POLICY LOSS tensor(1589.0240, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 746\n",
      "MEAN POLICY LOSS tensor(8249.0264, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 747\n",
      "MEAN POLICY LOSS tensor(10789.8555, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 748\n",
      "MEAN POLICY LOSS tensor(8144.7139, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 749\n",
      "MEAN POLICY LOSS tensor(7283.9907, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 750\n",
      "MEAN POLICY LOSS tensor(5510.1353, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 751\n",
      "MEAN POLICY LOSS tensor(3768.7429, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 752\n",
      "MEAN POLICY LOSS tensor(5444.2759, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 753\n",
      "MEAN POLICY LOSS tensor(5756.3345, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 754\n",
      "MEAN POLICY LOSS tensor(4630.9648, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 755\n",
      "MEAN POLICY LOSS tensor(8285.7910, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 756\n",
      "MEAN POLICY LOSS tensor(2909.4536, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 757\n",
      "MEAN POLICY LOSS tensor(5157.9268, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 758\n",
      "MEAN POLICY LOSS tensor(3858.4087, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 759\n",
      "MEAN POLICY LOSS tensor(15563.4756, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 760\n",
      "MEAN POLICY LOSS tensor(4739.4189, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 761\n",
      "MEAN POLICY LOSS tensor(1797.7682, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 762\n",
      "MEAN POLICY LOSS tensor(2638.4031, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 763\n",
      "MEAN POLICY LOSS tensor(3023.6506, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 764\n",
      "MEAN POLICY LOSS tensor(11895.0889, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 765\n",
      "MEAN POLICY LOSS tensor(1607.1172, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 766\n",
      "MEAN POLICY LOSS tensor(2049.4951, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 767\n",
      "MEAN POLICY LOSS tensor(3310.7214, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 768\n",
      "MEAN POLICY LOSS tensor(1300.7260, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 769\n",
      "MEAN POLICY LOSS tensor(1136.5339, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 770\n",
      "MEAN POLICY LOSS tensor(915.1559, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 771\n",
      "MEAN POLICY LOSS tensor(472.3472, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 772\n",
      "MEAN POLICY LOSS tensor(794.8102, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 773\n",
      "MEAN POLICY LOSS tensor(319.5484, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 774\n",
      "MEAN POLICY LOSS tensor(773.7719, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 775\n",
      "MEAN POLICY LOSS tensor(287.4161, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 776\n",
      "MEAN POLICY LOSS tensor(348.8450, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 777\n",
      "MEAN POLICY LOSS tensor(328.8637, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 778\n",
      "MEAN POLICY LOSS tensor(331.2793, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 779\n",
      "MEAN POLICY LOSS tensor(223.7693, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 780\n",
      "MEAN POLICY LOSS tensor(183.7703, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 781\n",
      "MEAN POLICY LOSS tensor(143.6552, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 782\n",
      "MEAN POLICY LOSS tensor(72.5732, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 783\n",
      "MEAN POLICY LOSS tensor(124.0822, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 784\n",
      "MEAN POLICY LOSS tensor(112.5529, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 785\n",
      "MEAN POLICY LOSS tensor(100.4627, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 786\n",
      "MEAN POLICY LOSS tensor(124.4625, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 787\n",
      "MEAN POLICY LOSS tensor(86.8936, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 788\n",
      "MEAN POLICY LOSS tensor(88.7863, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 789\n",
      "MEAN POLICY LOSS tensor(64.4903, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 790\n",
      "MEAN POLICY LOSS tensor(63.2735, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 791\n",
      "MEAN POLICY LOSS tensor(66.3860, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 792\n",
      "MEAN POLICY LOSS tensor(62.0698, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 793\n",
      "MEAN POLICY LOSS tensor(68.5268, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 794\n",
      "MEAN POLICY LOSS tensor(69.1220, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 795\n",
      "MEAN POLICY LOSS tensor(53.3605, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 796\n",
      "MEAN POLICY LOSS tensor(46.2651, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 797\n",
      "MEAN POLICY LOSS tensor(56.7037, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 798\n",
      "MEAN POLICY LOSS tensor(59.5848, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 799\n",
      "MEAN POLICY LOSS tensor(44.5934, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 800\n",
      "MEAN POLICY LOSS tensor(39.0987, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 801\n",
      "MEAN POLICY LOSS tensor(32.6380, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 802\n",
      "MEAN POLICY LOSS tensor(40.8566, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 803\n",
      "MEAN POLICY LOSS tensor(40.7044, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 804\n",
      "MEAN POLICY LOSS tensor(22.7870, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 805\n",
      "MEAN POLICY LOSS tensor(32.2232, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 806\n",
      "MEAN POLICY LOSS tensor(60173.5352, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 807\n",
      "MEAN POLICY LOSS tensor(37.8747, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 808\n",
      "MEAN POLICY LOSS tensor(56.6298, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 809\n",
      "MEAN POLICY LOSS tensor(68.6165, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 810\n",
      "MEAN POLICY LOSS tensor(86.4368, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 811\n",
      "MEAN POLICY LOSS tensor(72.8308, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 812\n",
      "MEAN POLICY LOSS tensor(82.8611, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 813\n",
      "MEAN POLICY LOSS tensor(101.6945, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 814\n",
      "MEAN POLICY LOSS tensor(104.1507, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 815\n",
      "MEAN POLICY LOSS tensor(111.6373, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 816\n",
      "MEAN POLICY LOSS tensor(128.9449, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 817\n",
      "MEAN POLICY LOSS tensor(186.8699, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 818\n",
      "MEAN POLICY LOSS tensor(49.3931, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 819\n",
      "MEAN POLICY LOSS tensor(61.6699, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 820\n",
      "MEAN POLICY LOSS tensor(195.2755, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 821\n",
      "MEAN POLICY LOSS tensor(187.6345, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 822\n",
      "MEAN POLICY LOSS tensor(183.7420, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 823\n",
      "MEAN POLICY LOSS tensor(109.6718, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 824\n",
      "MEAN POLICY LOSS tensor(127.3913, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 825\n",
      "MEAN POLICY LOSS tensor(123.9931, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 826\n",
      "MEAN POLICY LOSS tensor(145.7327, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 827\n",
      "MEAN POLICY LOSS tensor(168.0897, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 828\n",
      "MEAN POLICY LOSS tensor(169.0467, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 829\n",
      "MEAN POLICY LOSS tensor(176.2620, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 830\n",
      "MEAN POLICY LOSS tensor(165.9306, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 831\n",
      "MEAN POLICY LOSS tensor(93.7398, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 832\n",
      "MEAN POLICY LOSS tensor(202.5813, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 833\n",
      "MEAN POLICY LOSS tensor(179.5954, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 834\n",
      "MEAN POLICY LOSS tensor(168.5613, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 835\n",
      "MEAN POLICY LOSS tensor(166.6347, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 836\n",
      "MEAN POLICY LOSS tensor(155.4016, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 837\n",
      "MEAN POLICY LOSS tensor(137.8824, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 838\n",
      "MEAN POLICY LOSS tensor(121.6153, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 839\n",
      "MEAN POLICY LOSS tensor(119.8228, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 840\n",
      "MEAN POLICY LOSS tensor(163.0171, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 841\n",
      "MEAN POLICY LOSS tensor(70.6389, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 842\n",
      "MEAN POLICY LOSS tensor(57.1923, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 843\n",
      "MEAN POLICY LOSS tensor(130.4787, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 844\n",
      "MEAN POLICY LOSS tensor(133.5308, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 845\n",
      "MEAN POLICY LOSS tensor(120.2743, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 846\n",
      "MEAN POLICY LOSS tensor(126.3320, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 847\n",
      "MEAN POLICY LOSS tensor(112.6880, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 848\n",
      "MEAN POLICY LOSS tensor(132.8876, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 849\n",
      "MEAN POLICY LOSS tensor(116.9472, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 850\n",
      "MEAN POLICY LOSS tensor(58.5514, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 851\n",
      "MEAN POLICY LOSS tensor(109.5442, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 852\n",
      "MEAN POLICY LOSS tensor(100.6736, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 853\n",
      "MEAN POLICY LOSS tensor(92.2987, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 854\n",
      "MEAN POLICY LOSS tensor(77.8355, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 855\n",
      "MEAN POLICY LOSS tensor(45.8513, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 856\n",
      "MEAN POLICY LOSS tensor(80.3965, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 857\n",
      "MEAN POLICY LOSS tensor(27.6095, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 858\n",
      "MEAN POLICY LOSS tensor(80.9055, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 859\n",
      "MEAN POLICY LOSS tensor(31.4967, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 860\n",
      "MEAN POLICY LOSS tensor(66.3895, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 861\n",
      "MEAN POLICY LOSS tensor(42.4492, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 862\n",
      "MEAN POLICY LOSS tensor(76.9968, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 863\n",
      "MEAN POLICY LOSS tensor(77.6957, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 864\n",
      "MEAN POLICY LOSS tensor(67.7289, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 865\n",
      "MEAN POLICY LOSS tensor(41.3102, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 866\n",
      "MEAN POLICY LOSS tensor(67.9502, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 867\n",
      "MEAN POLICY LOSS tensor(56.7281, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 868\n",
      "MEAN POLICY LOSS tensor(64.6533, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 869\n",
      "MEAN POLICY LOSS tensor(41.7766, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 870\n",
      "MEAN POLICY LOSS tensor(66.3872, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 871\n",
      "MEAN POLICY LOSS tensor(33.1765, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 872\n",
      "MEAN POLICY LOSS tensor(33.3919, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 873\n",
      "MEAN POLICY LOSS tensor(57.2278, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 874\n",
      "MEAN POLICY LOSS tensor(48.8844, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 875\n",
      "MEAN POLICY LOSS tensor(31.3737, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 876\n",
      "MEAN POLICY LOSS tensor(65.7111, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 877\n",
      "MEAN POLICY LOSS tensor(42.1878, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 878\n",
      "MEAN POLICY LOSS tensor(42.7704, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 879\n",
      "MEAN POLICY LOSS tensor(41.7649, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 880\n",
      "MEAN POLICY LOSS tensor(60.6323, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 881\n",
      "MEAN POLICY LOSS tensor(50.8859, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 882\n",
      "MEAN POLICY LOSS tensor(54.2654, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 883\n",
      "MEAN POLICY LOSS tensor(40.8577, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 884\n",
      "MEAN POLICY LOSS tensor(46.9322, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 885\n",
      "MEAN POLICY LOSS tensor(57706.5938, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 886\n",
      "MEAN POLICY LOSS tensor(57.2255, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 887\n",
      "MEAN POLICY LOSS tensor(52.2801, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 888\n",
      "MEAN POLICY LOSS tensor(73.7398, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 889\n",
      "MEAN POLICY LOSS tensor(57.2476, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 890\n",
      "MEAN POLICY LOSS tensor(35.0587, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 891\n",
      "MEAN POLICY LOSS tensor(102.4508, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 892\n",
      "MEAN POLICY LOSS tensor(90.7750, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 893\n",
      "MEAN POLICY LOSS tensor(114.4417, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 894\n",
      "MEAN POLICY LOSS tensor(43115.8750, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 895\n",
      "MEAN POLICY LOSS tensor(193.3508, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 896\n",
      "MEAN POLICY LOSS tensor(200.5736, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 897\n",
      "MEAN POLICY LOSS tensor(279.7886, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 898\n",
      "MEAN POLICY LOSS tensor(149.2575, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 899\n",
      "MEAN POLICY LOSS tensor(238.1823, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 900\n",
      "MEAN POLICY LOSS tensor(433.7299, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 901\n",
      "MEAN POLICY LOSS tensor(209.5261, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 902\n",
      "MEAN POLICY LOSS tensor(322.6580, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 903\n",
      "MEAN POLICY LOSS tensor(215.6738, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 904\n",
      "MEAN POLICY LOSS tensor(445.4845, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 905\n",
      "MEAN POLICY LOSS tensor(404.9042, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 906\n",
      "MEAN POLICY LOSS tensor(418.8887, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 907\n",
      "MEAN POLICY LOSS tensor(362.6078, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 908\n",
      "MEAN POLICY LOSS tensor(583.6317, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 909\n",
      "MEAN POLICY LOSS tensor(519.5366, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 910\n",
      "MEAN POLICY LOSS tensor(578.7531, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 911\n",
      "MEAN POLICY LOSS tensor(225.9377, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 912\n",
      "MEAN POLICY LOSS tensor(388.6471, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 913\n",
      "MEAN POLICY LOSS tensor(400.9307, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 914\n",
      "MEAN POLICY LOSS tensor(139.0943, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 915\n",
      "MEAN POLICY LOSS tensor(135.4747, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 916\n",
      "MEAN POLICY LOSS tensor(402.3570, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 917\n",
      "MEAN POLICY LOSS tensor(465.9894, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 918\n",
      "MEAN POLICY LOSS tensor(471.8148, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 919\n",
      "MEAN POLICY LOSS tensor(308.7545, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 920\n",
      "MEAN POLICY LOSS tensor(392.0055, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 921\n",
      "MEAN POLICY LOSS tensor(438.6058, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 922\n",
      "MEAN POLICY LOSS tensor(339.1697, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 923\n",
      "MEAN POLICY LOSS tensor(246.6950, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 924\n",
      "MEAN POLICY LOSS tensor(235.9591, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 925\n",
      "MEAN POLICY LOSS tensor(431.7936, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 926\n",
      "MEAN POLICY LOSS tensor(248.5319, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 927\n",
      "MEAN POLICY LOSS tensor(409.5072, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 928\n",
      "MEAN POLICY LOSS tensor(212.7733, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 929\n",
      "MEAN POLICY LOSS tensor(152.1546, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 930\n",
      "MEAN POLICY LOSS tensor(344.8974, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 931\n",
      "MEAN POLICY LOSS tensor(247.3600, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 932\n",
      "MEAN POLICY LOSS tensor(45.3278, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 933\n",
      "MEAN POLICY LOSS tensor(240.1663, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 934\n",
      "MEAN POLICY LOSS tensor(242.1224, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 935\n",
      "MEAN POLICY LOSS tensor(242.1211, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 936\n",
      "MEAN POLICY LOSS tensor(131.1658, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 937\n",
      "MEAN POLICY LOSS tensor(147.2844, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 938\n",
      "MEAN POLICY LOSS tensor(216.7632, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 939\n",
      "MEAN POLICY LOSS tensor(192.7409, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 940\n",
      "MEAN POLICY LOSS tensor(255.4739, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 941\n",
      "MEAN POLICY LOSS tensor(179.2306, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 942\n",
      "MEAN POLICY LOSS tensor(137.5274, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 943\n",
      "MEAN POLICY LOSS tensor(137.4718, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 944\n",
      "MEAN POLICY LOSS tensor(105.3567, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 945\n",
      "MEAN POLICY LOSS tensor(148.3084, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 946\n",
      "MEAN POLICY LOSS tensor(160.2162, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 947\n",
      "MEAN POLICY LOSS tensor(172.8197, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 948\n",
      "MEAN POLICY LOSS tensor(194.4979, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 949\n",
      "MEAN POLICY LOSS tensor(204.6867, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 950\n",
      "MEAN POLICY LOSS tensor(99.2438, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 951\n",
      "MEAN POLICY LOSS tensor(61.9341, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 952\n",
      "MEAN POLICY LOSS tensor(161.0092, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 953\n",
      "MEAN POLICY LOSS tensor(140.5629, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 954\n",
      "MEAN POLICY LOSS tensor(105.0545, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 955\n",
      "MEAN POLICY LOSS tensor(102.4807, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 956\n",
      "MEAN POLICY LOSS tensor(117.5080, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 957\n",
      "MEAN POLICY LOSS tensor(142.2900, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 958\n",
      "MEAN POLICY LOSS tensor(160.4315, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 959\n",
      "MEAN POLICY LOSS tensor(175.4159, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 960\n",
      "MEAN POLICY LOSS tensor(68.5811, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 961\n",
      "MEAN POLICY LOSS tensor(98.1250, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 962\n",
      "MEAN POLICY LOSS tensor(131.8153, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 963\n",
      "MEAN POLICY LOSS tensor(110.2111, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 964\n",
      "MEAN POLICY LOSS tensor(119.1835, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 965\n",
      "MEAN POLICY LOSS tensor(93.8590, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 966\n",
      "MEAN POLICY LOSS tensor(58.2465, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 967\n",
      "MEAN POLICY LOSS tensor(63.6961, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 968\n",
      "MEAN POLICY LOSS tensor(116.7605, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 969\n",
      "MEAN POLICY LOSS tensor(91.9366, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 970\n",
      "MEAN POLICY LOSS tensor(104.2694, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 971\n",
      "MEAN POLICY LOSS tensor(121.0555, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 972\n",
      "MEAN POLICY LOSS tensor(110.8229, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 973\n",
      "MEAN POLICY LOSS tensor(68.6980, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 974\n",
      "MEAN POLICY LOSS tensor(75.5098, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 975\n",
      "MEAN POLICY LOSS tensor(72.7976, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 976\n",
      "MEAN POLICY LOSS tensor(29.6553, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 977\n",
      "MEAN POLICY LOSS tensor(87.9988, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 978\n",
      "MEAN POLICY LOSS tensor(32.4030, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 979\n",
      "MEAN POLICY LOSS tensor(101.9301, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 980\n",
      "MEAN POLICY LOSS tensor(64.7166, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 981\n",
      "MEAN POLICY LOSS tensor(73.7765, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 982\n",
      "MEAN POLICY LOSS tensor(68.9438, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 983\n",
      "MEAN POLICY LOSS tensor(81.6088, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 984\n",
      "MEAN POLICY LOSS tensor(67.2047, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 985\n",
      "MEAN POLICY LOSS tensor(81.8806, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 986\n",
      "MEAN POLICY LOSS tensor(81.7821, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 987\n",
      "MEAN POLICY LOSS tensor(89.4119, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 988\n",
      "MEAN POLICY LOSS tensor(63.5250, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 989\n",
      "MEAN POLICY LOSS tensor(75.4247, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 990\n",
      "MEAN POLICY LOSS tensor(75.4532, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 991\n",
      "MEAN POLICY LOSS tensor(86.6556, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 992\n",
      "MEAN POLICY LOSS tensor(64.5444, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 993\n",
      "MEAN POLICY LOSS tensor(56.9300, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 994\n",
      "MEAN POLICY LOSS tensor(76.4267, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 995\n",
      "MEAN POLICY LOSS tensor(34.7313, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 996\n",
      "MEAN POLICY LOSS tensor(70.5068, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 997\n",
      "MEAN POLICY LOSS tensor(63.0547, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 998\n",
      "MEAN POLICY LOSS tensor(51.3219, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 999\n",
      "MEAN POLICY LOSS tensor(67.2211, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "Run 6/10\n",
      "EPSIODE# 0\n",
      "MEAN POLICY LOSS tensor(10701.9619, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 1\n",
      "MEAN POLICY LOSS tensor(11094.5469, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 2\n",
      "MEAN POLICY LOSS tensor(10188.3330, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 3\n",
      "MEAN POLICY LOSS tensor(13973.3877, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 4\n",
      "MEAN POLICY LOSS tensor(6681.9526, grad_fn=<AddBackward0>)\n",
      "REWARD -320.0\n",
      "EPSIODE# 5\n",
      "MEAN POLICY LOSS tensor(8143.1187, grad_fn=<AddBackward0>)\n",
      "REWARD -315.0\n",
      "EPSIODE# 6\n",
      "MEAN POLICY LOSS tensor(5410.6738, grad_fn=<AddBackward0>)\n",
      "REWARD -230.0\n",
      "EPSIODE# 7\n",
      "MEAN POLICY LOSS tensor(13240.1436, grad_fn=<AddBackward0>)\n",
      "REWARD -287.0\n",
      "EPSIODE# 8\n",
      "MEAN POLICY LOSS tensor(4224.7441, grad_fn=<AddBackward0>)\n",
      "REWARD -230.0\n",
      "EPSIODE# 9\n",
      "MEAN POLICY LOSS tensor(12993.3086, grad_fn=<AddBackward0>)\n",
      "REWARD -255.0\n",
      "EPSIODE# 10\n",
      "MEAN POLICY LOSS tensor(12053.9990, grad_fn=<AddBackward0>)\n",
      "REWARD -310.0\n",
      "EPSIODE# 11\n",
      "MEAN POLICY LOSS tensor(12888.8066, grad_fn=<AddBackward0>)\n",
      "REWARD -252.0\n",
      "EPSIODE# 12\n",
      "MEAN POLICY LOSS tensor(7370.7710, grad_fn=<AddBackward0>)\n",
      "REWARD -262.0\n",
      "EPSIODE# 13\n",
      "MEAN POLICY LOSS tensor(3073.8203, grad_fn=<AddBackward0>)\n",
      "REWARD -255.0\n",
      "EPSIODE# 14\n",
      "MEAN POLICY LOSS tensor(12692.5918, grad_fn=<AddBackward0>)\n",
      "REWARD -456.0\n",
      "EPSIODE# 15\n",
      "MEAN POLICY LOSS tensor(2869.2754, grad_fn=<AddBackward0>)\n",
      "REWARD -183.0\n",
      "EPSIODE# 16\n",
      "MEAN POLICY LOSS tensor(2089.0278, grad_fn=<AddBackward0>)\n",
      "REWARD -201.0\n",
      "EPSIODE# 17\n",
      "MEAN POLICY LOSS tensor(3556.3264, grad_fn=<AddBackward0>)\n",
      "REWARD -271.0\n",
      "EPSIODE# 18\n",
      "MEAN POLICY LOSS tensor(1266.8607, grad_fn=<AddBackward0>)\n",
      "REWARD -252.0\n",
      "EPSIODE# 19\n",
      "MEAN POLICY LOSS tensor(7368.1489, grad_fn=<AddBackward0>)\n",
      "REWARD -382.0\n",
      "EPSIODE# 20\n",
      "MEAN POLICY LOSS tensor(12578.7637, grad_fn=<AddBackward0>)\n",
      "REWARD -315.0\n",
      "EPSIODE# 21\n",
      "MEAN POLICY LOSS tensor(4071.0759, grad_fn=<AddBackward0>)\n",
      "REWARD -358.0\n",
      "EPSIODE# 22\n",
      "MEAN POLICY LOSS tensor(2515.7036, grad_fn=<AddBackward0>)\n",
      "REWARD -177.0\n",
      "EPSIODE# 23\n",
      "MEAN POLICY LOSS tensor(1278.6770, grad_fn=<AddBackward0>)\n",
      "REWARD -267.0\n",
      "EPSIODE# 24\n",
      "MEAN POLICY LOSS tensor(180.5287, grad_fn=<AddBackward0>)\n",
      "REWARD -264.0\n",
      "EPSIODE# 25\n",
      "MEAN POLICY LOSS tensor(1693.1124, grad_fn=<AddBackward0>)\n",
      "REWARD -371.0\n",
      "EPSIODE# 26\n",
      "MEAN POLICY LOSS tensor(6300.8882, grad_fn=<AddBackward0>)\n",
      "REWARD -325.0\n",
      "EPSIODE# 27\n",
      "MEAN POLICY LOSS tensor(4566.9526, grad_fn=<AddBackward0>)\n",
      "REWARD -217.0\n",
      "EPSIODE# 28\n",
      "MEAN POLICY LOSS tensor(8734.3643, grad_fn=<AddBackward0>)\n",
      "REWARD -250.0\n",
      "EPSIODE# 29\n",
      "MEAN POLICY LOSS tensor(1296.2063, grad_fn=<AddBackward0>)\n",
      "REWARD -251.0\n",
      "EPSIODE# 30\n",
      "MEAN POLICY LOSS tensor(8918.6729, grad_fn=<AddBackward0>)\n",
      "REWARD -254.0\n",
      "EPSIODE# 31\n",
      "MEAN POLICY LOSS tensor(88.5071, grad_fn=<AddBackward0>)\n",
      "REWARD -274.0\n",
      "EPSIODE# 32\n",
      "MEAN POLICY LOSS tensor(6495.6333, grad_fn=<AddBackward0>)\n",
      "REWARD -249.0\n",
      "EPSIODE# 33\n",
      "MEAN POLICY LOSS tensor(3830.6755, grad_fn=<AddBackward0>)\n",
      "REWARD -213.0\n",
      "EPSIODE# 34\n",
      "MEAN POLICY LOSS tensor(4678.6318, grad_fn=<AddBackward0>)\n",
      "REWARD -261.0\n",
      "EPSIODE# 35\n",
      "MEAN POLICY LOSS tensor(379.0588, grad_fn=<AddBackward0>)\n",
      "REWARD -227.0\n",
      "EPSIODE# 36\n",
      "MEAN POLICY LOSS tensor(93.1782, grad_fn=<AddBackward0>)\n",
      "REWARD -241.0\n",
      "EPSIODE# 37\n",
      "MEAN POLICY LOSS tensor(1560.7477, grad_fn=<AddBackward0>)\n",
      "REWARD -403.0\n",
      "EPSIODE# 38\n",
      "MEAN POLICY LOSS tensor(2676.0640, grad_fn=<AddBackward0>)\n",
      "REWARD -344.0\n",
      "EPSIODE# 39\n",
      "MEAN POLICY LOSS tensor(6939.6196, grad_fn=<AddBackward0>)\n",
      "REWARD -206.0\n",
      "EPSIODE# 40\n",
      "MEAN POLICY LOSS tensor(282.5606, grad_fn=<AddBackward0>)\n",
      "REWARD -260.0\n",
      "EPSIODE# 41\n",
      "MEAN POLICY LOSS tensor(150.0287, grad_fn=<AddBackward0>)\n",
      "REWARD -237.0\n",
      "EPSIODE# 42\n",
      "MEAN POLICY LOSS tensor(8354.6973, grad_fn=<AddBackward0>)\n",
      "REWARD -193.0\n",
      "EPSIODE# 43\n",
      "MEAN POLICY LOSS tensor(17.5568, grad_fn=<AddBackward0>)\n",
      "REWARD -264.0\n",
      "EPSIODE# 44\n",
      "MEAN POLICY LOSS tensor(14779.5908, grad_fn=<AddBackward0>)\n",
      "REWARD -290.0\n",
      "EPSIODE# 45\n",
      "MEAN POLICY LOSS tensor(1168.7286, grad_fn=<AddBackward0>)\n",
      "REWARD -354.0\n",
      "EPSIODE# 46\n",
      "MEAN POLICY LOSS tensor(554.0082, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 47\n",
      "MEAN POLICY LOSS tensor(8104.5688, grad_fn=<AddBackward0>)\n",
      "REWARD -409.0\n",
      "EPSIODE# 48\n",
      "MEAN POLICY LOSS tensor(7574.2988, grad_fn=<AddBackward0>)\n",
      "REWARD -411.0\n",
      "EPSIODE# 49\n",
      "MEAN POLICY LOSS tensor(79.4869, grad_fn=<AddBackward0>)\n",
      "REWARD -207.0\n",
      "EPSIODE# 50\n",
      "MEAN POLICY LOSS tensor(5694.1626, grad_fn=<AddBackward0>)\n",
      "REWARD -319.0\n",
      "EPSIODE# 51\n",
      "MEAN POLICY LOSS tensor(59.9061, grad_fn=<AddBackward0>)\n",
      "REWARD -248.0\n",
      "EPSIODE# 52\n",
      "MEAN POLICY LOSS tensor(4121.7305, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 53\n",
      "MEAN POLICY LOSS tensor(3905.6724, grad_fn=<AddBackward0>)\n",
      "REWARD -239.0\n",
      "EPSIODE# 54\n",
      "MEAN POLICY LOSS tensor(12399.8291, grad_fn=<AddBackward0>)\n",
      "REWARD -291.0\n",
      "EPSIODE# 55\n",
      "MEAN POLICY LOSS tensor(123.2039, grad_fn=<AddBackward0>)\n",
      "REWARD -205.0\n",
      "EPSIODE# 56\n",
      "MEAN POLICY LOSS tensor(42.4938, grad_fn=<AddBackward0>)\n",
      "REWARD -282.0\n",
      "EPSIODE# 57\n",
      "MEAN POLICY LOSS tensor(123.7396, grad_fn=<AddBackward0>)\n",
      "REWARD -318.0\n",
      "EPSIODE# 58\n",
      "MEAN POLICY LOSS tensor(66.9829, grad_fn=<AddBackward0>)\n",
      "REWARD -218.0\n",
      "EPSIODE# 59\n",
      "MEAN POLICY LOSS tensor(32.3886, grad_fn=<AddBackward0>)\n",
      "REWARD -281.0\n",
      "EPSIODE# 60\n",
      "MEAN POLICY LOSS tensor(326.8243, grad_fn=<AddBackward0>)\n",
      "REWARD -238.0\n",
      "EPSIODE# 61\n",
      "MEAN POLICY LOSS tensor(12844.0771, grad_fn=<AddBackward0>)\n",
      "REWARD -241.0\n",
      "EPSIODE# 62\n",
      "MEAN POLICY LOSS tensor(6431.3501, grad_fn=<AddBackward0>)\n",
      "REWARD -270.0\n",
      "EPSIODE# 63\n",
      "MEAN POLICY LOSS tensor(12.7773, grad_fn=<AddBackward0>)\n",
      "REWARD -198.0\n",
      "EPSIODE# 64\n",
      "MEAN POLICY LOSS tensor(183.7424, grad_fn=<AddBackward0>)\n",
      "REWARD -215.0\n",
      "EPSIODE# 65\n",
      "MEAN POLICY LOSS tensor(735.5381, grad_fn=<AddBackward0>)\n",
      "REWARD -236.0\n",
      "EPSIODE# 66\n",
      "MEAN POLICY LOSS tensor(1755.7401, grad_fn=<AddBackward0>)\n",
      "REWARD -296.0\n",
      "EPSIODE# 67\n",
      "MEAN POLICY LOSS tensor(328.5936, grad_fn=<AddBackward0>)\n",
      "REWARD -197.0\n",
      "EPSIODE# 68\n",
      "MEAN POLICY LOSS tensor(298.1876, grad_fn=<AddBackward0>)\n",
      "REWARD -213.0\n",
      "EPSIODE# 69\n",
      "MEAN POLICY LOSS tensor(1339.0697, grad_fn=<AddBackward0>)\n",
      "REWARD -349.0\n",
      "EPSIODE# 70\n",
      "MEAN POLICY LOSS tensor(18.9288, grad_fn=<AddBackward0>)\n",
      "REWARD -221.0\n",
      "EPSIODE# 71\n",
      "MEAN POLICY LOSS tensor(7272.7837, grad_fn=<AddBackward0>)\n",
      "REWARD -203.0\n",
      "EPSIODE# 72\n",
      "MEAN POLICY LOSS tensor(79.2861, grad_fn=<AddBackward0>)\n",
      "REWARD -249.0\n",
      "EPSIODE# 73\n",
      "MEAN POLICY LOSS tensor(1345.4135, grad_fn=<AddBackward0>)\n",
      "REWARD -230.0\n",
      "EPSIODE# 74\n",
      "MEAN POLICY LOSS tensor(130.1174, grad_fn=<AddBackward0>)\n",
      "REWARD -187.0\n",
      "EPSIODE# 75\n",
      "MEAN POLICY LOSS tensor(230.6924, grad_fn=<AddBackward0>)\n",
      "REWARD -216.0\n",
      "EPSIODE# 76\n",
      "MEAN POLICY LOSS tensor(282.9250, grad_fn=<AddBackward0>)\n",
      "REWARD -157.0\n",
      "EPSIODE# 77\n",
      "MEAN POLICY LOSS tensor(1438.3573, grad_fn=<AddBackward0>)\n",
      "REWARD -238.0\n",
      "EPSIODE# 78\n",
      "MEAN POLICY LOSS tensor(459.4245, grad_fn=<AddBackward0>)\n",
      "REWARD -192.0\n",
      "EPSIODE# 79\n",
      "MEAN POLICY LOSS tensor(4052.6062, grad_fn=<AddBackward0>)\n",
      "REWARD -336.0\n",
      "EPSIODE# 80\n",
      "MEAN POLICY LOSS tensor(581.3779, grad_fn=<AddBackward0>)\n",
      "REWARD -232.0\n",
      "EPSIODE# 81\n",
      "MEAN POLICY LOSS tensor(733.2122, grad_fn=<AddBackward0>)\n",
      "REWARD -197.0\n",
      "EPSIODE# 82\n",
      "MEAN POLICY LOSS tensor(403.1841, grad_fn=<AddBackward0>)\n",
      "REWARD -201.0\n",
      "EPSIODE# 83\n",
      "MEAN POLICY LOSS tensor(24469.0195, grad_fn=<AddBackward0>)\n",
      "REWARD -249.0\n",
      "EPSIODE# 84\n",
      "MEAN POLICY LOSS tensor(307.1102, grad_fn=<AddBackward0>)\n",
      "REWARD -211.0\n",
      "EPSIODE# 85\n",
      "MEAN POLICY LOSS tensor(263.1235, grad_fn=<AddBackward0>)\n",
      "REWARD -197.0\n",
      "EPSIODE# 86\n",
      "MEAN POLICY LOSS tensor(8.8876, grad_fn=<AddBackward0>)\n",
      "REWARD -217.0\n",
      "EPSIODE# 87\n",
      "MEAN POLICY LOSS tensor(33.8469, grad_fn=<AddBackward0>)\n",
      "REWARD -243.0\n",
      "EPSIODE# 88\n",
      "MEAN POLICY LOSS tensor(2.8997, grad_fn=<AddBackward0>)\n",
      "REWARD -247.0\n",
      "EPSIODE# 89\n",
      "MEAN POLICY LOSS tensor(897.1483, grad_fn=<AddBackward0>)\n",
      "REWARD -222.0\n",
      "EPSIODE# 90\n",
      "MEAN POLICY LOSS tensor(8249.1436, grad_fn=<AddBackward0>)\n",
      "REWARD -215.0\n",
      "EPSIODE# 91\n",
      "MEAN POLICY LOSS tensor(3290.1067, grad_fn=<AddBackward0>)\n",
      "REWARD -399.0\n",
      "EPSIODE# 92\n",
      "MEAN POLICY LOSS tensor(11.2214, grad_fn=<AddBackward0>)\n",
      "REWARD -291.0\n",
      "EPSIODE# 93\n",
      "MEAN POLICY LOSS tensor(6303.5142, grad_fn=<AddBackward0>)\n",
      "REWARD -263.0\n",
      "EPSIODE# 94\n",
      "MEAN POLICY LOSS tensor(11.8661, grad_fn=<AddBackward0>)\n",
      "REWARD -341.0\n",
      "EPSIODE# 95\n",
      "MEAN POLICY LOSS tensor(4162.3765, grad_fn=<AddBackward0>)\n",
      "REWARD -389.0\n",
      "EPSIODE# 96\n",
      "MEAN POLICY LOSS tensor(13.4877, grad_fn=<AddBackward0>)\n",
      "REWARD -231.0\n",
      "EPSIODE# 97\n",
      "MEAN POLICY LOSS tensor(6860.9341, grad_fn=<AddBackward0>)\n",
      "REWARD -411.0\n",
      "EPSIODE# 98\n",
      "MEAN POLICY LOSS tensor(7482.4678, grad_fn=<AddBackward0>)\n",
      "REWARD -321.0\n",
      "EPSIODE# 99\n",
      "MEAN POLICY LOSS tensor(0.1329, grad_fn=<AddBackward0>)\n",
      "REWARD -426.0\n",
      "EPSIODE# 100\n",
      "MEAN POLICY LOSS tensor(10492.7539, grad_fn=<AddBackward0>)\n",
      "REWARD -312.0\n",
      "EPSIODE# 101\n",
      "MEAN POLICY LOSS tensor(2.7980, grad_fn=<AddBackward0>)\n",
      "REWARD -261.0\n",
      "EPSIODE# 102\n",
      "MEAN POLICY LOSS tensor(19.9436, grad_fn=<AddBackward0>)\n",
      "REWARD -317.0\n",
      "EPSIODE# 103\n",
      "MEAN POLICY LOSS tensor(104.7348, grad_fn=<AddBackward0>)\n",
      "REWARD -475.0\n",
      "EPSIODE# 104\n",
      "MEAN POLICY LOSS tensor(7643.1006, grad_fn=<AddBackward0>)\n",
      "REWARD -445.0\n",
      "EPSIODE# 105\n",
      "MEAN POLICY LOSS tensor(5027.3330, grad_fn=<AddBackward0>)\n",
      "REWARD -474.0\n",
      "EPSIODE# 106\n",
      "MEAN POLICY LOSS tensor(33.7842, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 107\n",
      "MEAN POLICY LOSS tensor(1462.7496, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 108\n",
      "MEAN POLICY LOSS tensor(2886.6824, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 109\n",
      "MEAN POLICY LOSS tensor(159.4326, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 110\n",
      "MEAN POLICY LOSS tensor(417.6529, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 111\n",
      "MEAN POLICY LOSS tensor(432.1914, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 112\n",
      "MEAN POLICY LOSS tensor(1537.0745, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 113\n",
      "MEAN POLICY LOSS tensor(25.6430, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 114\n",
      "MEAN POLICY LOSS tensor(630.8121, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 115\n",
      "MEAN POLICY LOSS tensor(725.0571, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 116\n",
      "MEAN POLICY LOSS tensor(1089.2581, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 117\n",
      "MEAN POLICY LOSS tensor(372.9477, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 118\n",
      "MEAN POLICY LOSS tensor(646.2172, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 119\n",
      "MEAN POLICY LOSS tensor(128.2292, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 120\n",
      "MEAN POLICY LOSS tensor(682.7491, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 121\n",
      "MEAN POLICY LOSS tensor(589.6930, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 122\n",
      "MEAN POLICY LOSS tensor(1057.8175, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 123\n",
      "MEAN POLICY LOSS tensor(20.3184, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 124\n",
      "MEAN POLICY LOSS tensor(139.6359, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 125\n",
      "MEAN POLICY LOSS tensor(361.8478, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 126\n",
      "MEAN POLICY LOSS tensor(465.7854, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 127\n",
      "MEAN POLICY LOSS tensor(76.7023, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 128\n",
      "MEAN POLICY LOSS tensor(314.7804, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 129\n",
      "MEAN POLICY LOSS tensor(445.0406, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 130\n",
      "MEAN POLICY LOSS tensor(329.4092, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 131\n",
      "MEAN POLICY LOSS tensor(333.7361, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 132\n",
      "MEAN POLICY LOSS tensor(217.1933, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 133\n",
      "MEAN POLICY LOSS tensor(190.6143, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 134\n",
      "MEAN POLICY LOSS tensor(176.8915, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 135\n",
      "MEAN POLICY LOSS tensor(181.2598, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 136\n",
      "MEAN POLICY LOSS tensor(56.1377, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 137\n",
      "MEAN POLICY LOSS tensor(160.1651, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 138\n",
      "MEAN POLICY LOSS tensor(115.5220, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 139\n",
      "MEAN POLICY LOSS tensor(64.7679, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 140\n",
      "MEAN POLICY LOSS tensor(90.0919, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 141\n",
      "MEAN POLICY LOSS tensor(64.6142, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 142\n",
      "MEAN POLICY LOSS tensor(55.9702, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 143\n",
      "MEAN POLICY LOSS tensor(70.7170, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 144\n",
      "MEAN POLICY LOSS tensor(53.0221, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 145\n",
      "MEAN POLICY LOSS tensor(59.4905, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 146\n",
      "MEAN POLICY LOSS tensor(56.2150, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 147\n",
      "MEAN POLICY LOSS tensor(55.4431, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 148\n",
      "MEAN POLICY LOSS tensor(46.1209, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 149\n",
      "MEAN POLICY LOSS tensor(28.1785, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 150\n",
      "MEAN POLICY LOSS tensor(38.1871, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 151\n",
      "MEAN POLICY LOSS tensor(36.4071, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 152\n",
      "MEAN POLICY LOSS tensor(35.1138, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 153\n",
      "MEAN POLICY LOSS tensor(32.2568, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 154\n",
      "MEAN POLICY LOSS tensor(33.7726, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 155\n",
      "MEAN POLICY LOSS tensor(32.1669, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 156\n",
      "MEAN POLICY LOSS tensor(29.3797, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 157\n",
      "MEAN POLICY LOSS tensor(33.6193, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 158\n",
      "MEAN POLICY LOSS tensor(24.5639, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 159\n",
      "MEAN POLICY LOSS tensor(21.1669, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 160\n",
      "MEAN POLICY LOSS tensor(25.7995, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 161\n",
      "MEAN POLICY LOSS tensor(23.4519, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 162\n",
      "MEAN POLICY LOSS tensor(9.9420, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 163\n",
      "MEAN POLICY LOSS tensor(20.2564, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 164\n",
      "MEAN POLICY LOSS tensor(19.4851, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 165\n",
      "MEAN POLICY LOSS tensor(19.7991, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 166\n",
      "MEAN POLICY LOSS tensor(13.9310, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 167\n",
      "MEAN POLICY LOSS tensor(18.6749, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 168\n",
      "MEAN POLICY LOSS tensor(18.7787, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 169\n",
      "MEAN POLICY LOSS tensor(15.5377, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 170\n",
      "MEAN POLICY LOSS tensor(18.4252, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 171\n",
      "MEAN POLICY LOSS tensor(12.6813, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 172\n",
      "MEAN POLICY LOSS tensor(15.0263, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 173\n",
      "MEAN POLICY LOSS tensor(14.8531, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 174\n",
      "MEAN POLICY LOSS tensor(13.9866, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 175\n",
      "MEAN POLICY LOSS tensor(13.8482, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 176\n",
      "MEAN POLICY LOSS tensor(11.3437, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 177\n",
      "MEAN POLICY LOSS tensor(13.6380, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 178\n",
      "MEAN POLICY LOSS tensor(12.2169, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 179\n",
      "MEAN POLICY LOSS tensor(12.4137, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 180\n",
      "MEAN POLICY LOSS tensor(11.5817, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 181\n",
      "MEAN POLICY LOSS tensor(12.5162, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 182\n",
      "MEAN POLICY LOSS tensor(12.3813, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 183\n",
      "MEAN POLICY LOSS tensor(11.1173, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 184\n",
      "MEAN POLICY LOSS tensor(10.1435, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 185\n",
      "MEAN POLICY LOSS tensor(12.8017, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 186\n",
      "MEAN POLICY LOSS tensor(10.0230, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 187\n",
      "MEAN POLICY LOSS tensor(11.8798, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 188\n",
      "MEAN POLICY LOSS tensor(12.1236, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 189\n",
      "MEAN POLICY LOSS tensor(10.5395, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 190\n",
      "MEAN POLICY LOSS tensor(9.4406, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 191\n",
      "MEAN POLICY LOSS tensor(11.1705, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 192\n",
      "MEAN POLICY LOSS tensor(10.0091, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 193\n",
      "MEAN POLICY LOSS tensor(10.4312, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 194\n",
      "MEAN POLICY LOSS tensor(9.5194, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 195\n",
      "MEAN POLICY LOSS tensor(9.0418, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 196\n",
      "MEAN POLICY LOSS tensor(10.1249, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 197\n",
      "MEAN POLICY LOSS tensor(9.8650, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 198\n",
      "MEAN POLICY LOSS tensor(9.3353, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 199\n",
      "MEAN POLICY LOSS tensor(8.7229, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 200\n",
      "MEAN POLICY LOSS tensor(8.6528, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 201\n",
      "MEAN POLICY LOSS tensor(7.9744, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 202\n",
      "MEAN POLICY LOSS tensor(8.8340, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 203\n",
      "MEAN POLICY LOSS tensor(10.7739, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 204\n",
      "MEAN POLICY LOSS tensor(6.7561, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 205\n",
      "MEAN POLICY LOSS tensor(6.4743, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 206\n",
      "MEAN POLICY LOSS tensor(9.7330, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 207\n",
      "MEAN POLICY LOSS tensor(8.3234, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 208\n",
      "MEAN POLICY LOSS tensor(6.4187, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 209\n",
      "MEAN POLICY LOSS tensor(8.1834, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 210\n",
      "MEAN POLICY LOSS tensor(7.8945, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 211\n",
      "MEAN POLICY LOSS tensor(8.0288, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 212\n",
      "MEAN POLICY LOSS tensor(6.2312, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 213\n",
      "MEAN POLICY LOSS tensor(7.7603, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 214\n",
      "MEAN POLICY LOSS tensor(7.4506, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 215\n",
      "MEAN POLICY LOSS tensor(7.5745, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 216\n",
      "MEAN POLICY LOSS tensor(7.3418, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 217\n",
      "MEAN POLICY LOSS tensor(6.7214, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 218\n",
      "MEAN POLICY LOSS tensor(6.7631, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 219\n",
      "MEAN POLICY LOSS tensor(7.7881, grad_fn=<AddBackward0>)\n",
      "REWARD -500.0\n",
      "EPSIODE# 220\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m SEEDS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stuff \u001b[38;5;129;01min\u001b[39;00m T:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mrun_experiment2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstuff\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstuff\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstuff\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m     os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmv plot.png plots/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env_name, stuff[\u001b[38;5;241m0\u001b[39m], lr, stuff[\u001b[38;5;241m1\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[69], line 10\u001b[0m, in \u001b[0;36mrun_experiment2\u001b[0;34m(lr, T, decay, decay_rate, gamma, episodes)\u001b[0m\n\u001b[1;32m      8\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# a2c_learning_rewards[i] = A2C(episodes=episodes, gamma=gamma, policy_lr=lr, value_lr=lr, T=T, decay=decay, decay_rate=decay_rate)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     reinforce_rewards[i] \u001b[38;5;241m=\u001b[39m \u001b[43mreinforce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecay_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# a2c_learning_rewards_mean = a2c_learning_rewards.mean(axis=0)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# a2c_learning_rewards_std = a2c_learning_rewards.std(axis=0)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m reinforce_rewards_mean \u001b[38;5;241m=\u001b[39m reinforce_rewards\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[64], line 16\u001b[0m, in \u001b[0;36mreinforce\u001b[0;34m(episodes, lr, gamma, T, decay, decay_rate)\u001b[0m\n\u001b[1;32m     14\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     action, prob \u001b[38;5;241m=\u001b[39m \u001b[43mBoltzmanPolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     next_state, reward, done, truncated, _\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     18\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[55], line 3\u001b[0m, in \u001b[0;36mBoltzmanPolicy\u001b[0;34m(preds, temp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mBoltzmanPolicy\u001b[39m(preds, temp):\n\u001b[1;32m      2\u001b[0m     probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(preds \u001b[38;5;241m/\u001b[39m temp, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a, probs[\u001b[38;5;241m0\u001b[39m][a]\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/distributions/categorical.py:134\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    132\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m    133\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m--> 134\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "T = [[1, False, 0.02]]\n",
    "#REINFORCE\n",
    "lr = [1e-3]\n",
    "SEEDS = 10\n",
    "for stuff in T:\n",
    "    run_experiment2(lr=lr[0], T=stuff[0], decay=stuff[1], decay_rate=stuff[2], gamma=0.99, episodes=1000)\n",
    "    time.sleep(1)\n",
    "    os.system('mv plot.png plots/{}_{}_{}_{}.png'.format(env_name, stuff[0], lr, stuff[1]))\n",
    "    time.sleep(1)\n",
    "    os.system('rm plot.png')\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "1. Value-based methods with deep neural network [50 points]\n",
                "Implement Q-learning and Exptected SARSA for both Acrobot-v1 1 and ALE/Assault-ram-v52\n",
                "environments from the Gym suite using the following guidelines:\n",
                "• Use a Neural Network approximation for Q, that is, if x is a vector representing the state\n",
                "and a is the action vector, use Q value(x) = M LP (x; θ), where θ are the parameters of the\n",
                "Value function you need to learn, Q ∈ Rm where m denotes the number of discrete actions.\n",
                "• Model configuration: Initialize the parameters for the value function uniformly between\n",
                "−0.001 and 0.001, we recommend using either a 2 or 3-layer Neural Network for the Value\n",
                "function, with a hidden dimension of 256.\n",
                "• Use an ϵ- greedy policy with three choices of ϵ and step-size parameters 1/4, 1/8, 1/16. and\n",
                "run 50 learning trials with different initializations for the Value function, each having 1000\n",
                "episodes, for each configuration. That means 3(ϵ’s) * 3 (step-sizes) * 50 runs * 1000 episodes.\n",
                "• Repeat the Previous step using a replay buffer (with transitions randomly sampled) and do\n",
                "gradient updates using a mini-batch of transitions. The capacity of the replay buffer is 1M.\n",
                "• Plot training curves with the mean across seeds as lines and the standard deviation as a shaded\n",
                "region. (Performance on the Y-axis, and the episode on the X-axis). Generate 18 graphs\n",
                "covering all configurations per environment. Present separate plots for each environment,\n",
                "with distinct graphs for settings with and without a replay buffer. Use green for Q-Learning\n",
                "and red for Expected SARSA, differentiating hyperparameters with different line styles (e.g.,\n",
                "solid, dashed).\n",
                "• Implement all the methods using any automatic differentiation package, such as Py-\n",
                "torch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Implement Q-learning and Exptected SARSA for both Acrobot-v1 1 and ALE/Assault-ram-v52 environments from the Gym suite using the following guidelines:\n",
                "\n",
                "# • Use a Neural Network approximation for Q, that is, if x is a vector representing the state and a is the action vector, use Q value(x) = M LP (x; θ), where θ are the parameters of the Value function you need to learn, Q ∈ Rm where m denotes the number of discrete actions.\n",
                "\n",
                "# • Model configuration: Initialize the parameters for the value function uniformly between −0.001 and 0.001, we recommend using either a 2 or 3-layer Neural Network for the Value function, with a hidden dimension of 256.\n",
                "\n",
                "# • Use an ϵ- greedy policy with three choices of ϵ and step-size parameters 1/4, 1/8, 1/16. and run 50 learning trials with different initializations for the Value function, each having 1000 episodes, for each configuration. That means 3(ϵ’s) * 3 (step-sizes) * 50 runs * 1000 episodes.\n",
                "\n",
                "# • Repeat the Previous step using a replay buffer (with transitions randomly sampled) and do gradient updates using a mini-batch of transitions. The capacity of the replay buffer is 1M.\n",
                "\n",
                "# • Plot training curves with the mean across seeds as lines and the standard deviation as a shaded region. (Performance on the Y-axis, and the episode on the X-axis). Generate 18 graphs covering all configurations per environment. Present separate plots for each environment, with distinct graphs for settings with and without a replay buffer. Use green for Q-Learning and red for Expected SARSA, differentiating hyperparameters with different line styles (e.g., solid, dashed).\n",
                "\n",
                "# • Implement all the methods using any automatic differentiation package, such as Py-torch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 100,
            "metadata": {},
            "outputs": [],
            "source": [
                "env_name = 'Acrobot-v1'\n",
                "# env_name = 'Assault-ram-v0'\n",
                "\n",
                "import gym\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import random\n",
                "from collections import deque\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import time\n",
                "import pickle\n",
                "\n",
                "# Hyperparameters\n",
                "EPISODES = 1000\n",
                "MAX_STEPS = 1000\n",
                "GAMMA = 0.99\n",
                "HIDDEN_DIM = 256\n",
                "SEEDS = 10\n",
                "\n",
                "# Environment\n",
                "env = gym.make(env_name)\n",
                "env._max_episode_steps = MAX_STEPS\n",
                "\n",
                "state_dim = env.observation_space.shape[0]\n",
                "action_dim = env.action_space.n\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "# device = \"mps\"\n",
                "\n",
                "# Neural Network\n",
                "class QNetwork(nn.Module):\n",
                "    def __init__(self, INPUT_DIM, OUTPUT_DIM):\n",
                "        super(QNetwork, self).__init__()\n",
                "        self.fc1 = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
                "        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
                "        self.fc3 = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
                "        self.fc1.weight.data.uniform_(-0.01, 0.01)\n",
                "        self.fc1.bias.data.uniform_(-0.01, 0.01)\n",
                "        self.fc2.weight.data.uniform_(-0.01, 0.01)\n",
                "        self.fc2.bias.data.uniform_(-0.01, 0.01)\n",
                "        self.fc3.weight.data.uniform_(-0.01, 0.01)\n",
                "        self.fc3.bias.data.uniform_(-0.01, 0.01)\n",
                "\n",
                "        # Initialize parameters\n",
                "        for m in self.modules():\n",
                "            if isinstance(m, nn.Linear):\n",
                "                print(\"initializing layer\", m)\n",
                "                nn.init.uniform_(m.weight, -0.01, 0.01)\n",
                "                nn.init.uniform_(m.bias, -0.01, 0.01)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = torch.relu(self.fc1(x))\n",
                "        x = torch.relu(self.fc2(x))\n",
                "        x = self.fc3(x)\n",
                "        return x\n",
                "\n",
                "# Q-Learning\n",
                "def q_learning(lr, batch_size, replay_buffer_size, epsilon):\n",
                "    q_network = QNetwork(state_dim, action_dim)\n",
                "    # q_network_target = QNetwork()\n",
                "    # q_network_target.load_state_dict(q_network.state_dict())\n",
                "    q_network.to(device)\n",
                "    optimizer = optim.Adam(q_network.parameters(), lr=lr, eps=0.0003125)\n",
                "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
                "    rewards = []\n",
                "    for episode in range(EPISODES):\n",
                "        state, info = env.reset()\n",
                "        done = False\n",
                "        total_reward = 0\n",
                "        while not done:\n",
                "            with torch.no_grad():\n",
                "                if random.random() < epsilon:\n",
                "                    action = env.action_space.sample()\n",
                "                else:\n",
                "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
                "                    action = q_network(state_tensor).argmax().item()\n",
                "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
                "                done = terminated or truncated\n",
                "                total_reward += reward\n",
                "                replay_buffer.append((state, action, reward, next_state, done))\n",
                "            if len(replay_buffer) >= batch_size:\n",
                "                batch = random.sample(replay_buffer, batch_size)\n",
                "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
                "                state_batch = torch.tensor(np.array(state_batch), dtype=torch.float32).to(device)\n",
                "                action_batch = torch.tensor(np.array(action_batch), dtype=torch.long).to(device)\n",
                "                reward_batch = torch.tensor(np.array(reward_batch), dtype=torch.float32).to(device)\n",
                "                next_state_batch = torch.tensor(np.array(next_state_batch), dtype=torch.float32).to(device)\n",
                "                done_batch = torch.tensor(np.array(done_batch), dtype=torch.float32).to(device)\n",
                "                q_values = q_network(state_batch)\n",
                "                # next_q_values = q_network_target(next_state_batch)\n",
                "                next_q_values = q_network(next_state_batch)\n",
                "                target_q_values = q_values.clone()\n",
                "                # for i in range(batch_size):\n",
                "                #     target_q_values[i][action_batch[i]] = reward_batch[i] + GAMMA * next_q_values[i].max() * (1 - done_batch[i])\n",
                "                target_q_values[range(batch_size), action_batch] = reward_batch + GAMMA * next_q_values.max(dim=1).values * (1 - done_batch)\n",
                "                loss = nn.MSELoss()(q_values, target_q_values)\n",
                "                optimizer.zero_grad()\n",
                "                loss.backward()\n",
                "                optimizer.step()\n",
                "            state = next_state\n",
                "        rewards.append(total_reward)\n",
                "        if episode % 100 == 0:\n",
                "            # q_network_target.load_state_dict(q_network.state_dict())\n",
                "            print(episode, total_reward, loss.item())\n",
                "\n",
                "    return rewards\n",
                "\n",
                "# Expected SARSA\n",
                "def expected_sarsa(lr, batch_size, replay_buffer_size, epsilon):\n",
                "    q_network = QNetwork()\n",
                "    # q_network_target = QNetwork()\n",
                "    # q_network_target.load_state_dict(q_network.state_dict())\n",
                "    optimizer = optim.Adam(q_network.parameters(), lr=lr, eps=0.0003125)\n",
                "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
                "    rewards = []\n",
                "    for episode in range(EPISODES):\n",
                "        state, info = env.reset()\n",
                "        done = False\n",
                "        total_reward = 0\n",
                "        while not done:\n",
                "            with torch.no_grad():\n",
                "                if random.random() < epsilon:\n",
                "                    action = env.action_space.sample()\n",
                "                else:\n",
                "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
                "                    action = q_network(state_tensor).argmax().item()\n",
                "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
                "                done = terminated or truncated\n",
                "                total_reward += reward\n",
                "                replay_buffer.append((state, action, reward, next_state, done))\n",
                "            if len(replay_buffer) >= batch_size:\n",
                "                batch = random.sample(replay_buffer, batch_size)\n",
                "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
                "                state_batch = torch.tensor(np.array(state_batch), dtype=torch.float32).to(device)\n",
                "                action_batch = torch.tensor(np.array(action_batch), dtype=torch.long).to(device)\n",
                "                reward_batch = torch.tensor(np.array(reward_batch), dtype=torch.float32).to(device)\n",
                "                next_state_batch = torch.tensor(np.array(next_state_batch), dtype=torch.float32).to(device)\n",
                "                done_batch = torch.tensor(np.array(done_batch), dtype=torch.float32).to(device)\n",
                "                q_values = q_network(state_batch)\n",
                "                # next_q_values = q_network_target(next_state_batch)\n",
                "                next_q_values = q_network(next_state_batch)\n",
                "                target_q_values = q_values.clone()\n",
                "                # expected sarsa\n",
                "                probs = torch.ones(batch_size, action_dim) * epsilon / action_dim\n",
                "                probs[range(batch_size), next_q_values.argmax(dim=1)] += 1 - epsilon\n",
                "                target_q_values[range(batch_size), action_batch] = reward_batch + GAMMA * (probs * next_q_values).sum(dim=1) * (1 - done_batch)\n",
                "\n",
                "                loss = nn.MSELoss()(q_values, target_q_values)\n",
                "                optimizer.zero_grad()\n",
                "                loss.backward()\n",
                "                optimizer.step()\n",
                "            state = next_state\n",
                "        rewards.append(total_reward)\n",
                "        if episode % 100 == 0:\n",
                "        #     q_network_target.load_state_dict(q_network.state_dict())\n",
                "            print(episode, total_reward, loss.item())\n",
                "    return rewards\n",
                "\n",
                "\n",
                "def run_experiment(lr, batch_size, replay_buffer_size, epsilon, seeds):\n",
                "    random_seeds = [0, 1, 2, 3, 4, 5, 6, 8, 11, 12]\n",
                "    print(seeds)\n",
                "    q_learning_rewards = np.zeros((seeds, EPISODES))\n",
                "    expected_sarsa_rewards = np.zeros((seeds, EPISODES))\n",
                "    if os.path.exists(f'q_learning_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl'):\n",
                "        old_q_learning_rewards = pickle.load(open(f'q_learning_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl', 'rb'))\n",
                "        q_learning_rewards[:old_q_learning_rewards.shape[0]] = old_q_learning_rewards\n",
                "    if os.path.exists(f'expected_sarsa_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl'):\n",
                "        old_expected_sarsa_rewards = pickle.load(open(f'expected_sarsa_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl', 'rb'))\n",
                "        expected_sarsa_rewards[:old_expected_sarsa_rewards.shape[0]] = old_expected_sarsa_rewards\n",
                "    # for i, seed in enumerate(random_seeds):\n",
                "    for i in range(seeds):\n",
                "        print(f'Run {i+1}/{seeds}')\n",
                "        if q_learning_rewards[i].sum() != 0 and expected_sarsa_rewards[i].sum() != 0:\n",
                "            continue\n",
                "        print('Q-Learning')\n",
                "        q_learning_rewards[i] = q_learning(lr, batch_size, replay_buffer_size, epsilon)\n",
                "        print('Expected SARSA')\n",
                "        expected_sarsa_rewards[i] = expected_sarsa(lr, batch_size, replay_buffer_size, epsilon)\n",
                "        q_learning_rewards_mean = q_learning_rewards[:i+1].mean(axis=0)\n",
                "        q_learning_rewards_std = q_learning_rewards[:i+1].std(axis=0)\n",
                "        expected_sarsa_rewards_mean = expected_sarsa_rewards[:i+1].mean(axis=0)\n",
                "        expected_sarsa_rewards_std = expected_sarsa_rewards[:i+1].std(axis=0)\n",
                "        plt.plot(q_learning_rewards_mean, label='Q-Learning', color='green')\n",
                "        plt.fill_between(range(EPISODES), q_learning_rewards_mean - q_learning_rewards_std, q_learning_rewards_mean + q_learning_rewards_std, color='green', alpha=0.2)\n",
                "        plt.plot(expected_sarsa_rewards_mean, label='Expected SARSA', color='red')\n",
                "        plt.fill_between(range(EPISODES), expected_sarsa_rewards_mean - expected_sarsa_rewards_std, expected_sarsa_rewards_mean + expected_sarsa_rewards_std, color='red', alpha=0.2)\n",
                "        plt.title(f'lr={lr}, batch_size={batch_size}, replay_buffer_size={replay_buffer_size}, epsilon={epsilon}')\n",
                "        plt.xlabel('Episode')\n",
                "        plt.ylabel('Reward')\n",
                "        plt.legend()\n",
                "        plt.savefig(f'plot_{i}_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.png')\n",
                "        plt.show()\n",
                "        pickle.dump(q_learning_rewards, open(f'q_learning_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl', 'wb'))\n",
                "        pickle.dump(expected_sarsa_rewards, open(f'expected_sarsa_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl', 'wb'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SEEDS = 1\n",
                "# EPISODES = 1000\n",
                "# run_experiment(0.001, 128, 1000000, 0.25)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.25_0.01_128.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.25_0.01_1.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.25_0.001_128.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.25_0.001_1.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.25_0.0001_128.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.25_0.0001_1.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.1_0.01_128.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.1_0.01_1.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.1_0.001_128.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.1_0.001_1.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.1_0.0001_128.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.1_0.0001_1.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.01_0.01_128.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.01_0.01_1.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.01_0.001_128.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.01_0.001_1.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.01_0.0001_128.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "Run 1/1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "mv: rename plot.png to plots/Acrobot-v1_0.01_0.0001_1.png: No such file or directory\n",
                        "rm: plot.png: No such file or directory\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2\n",
                        "Run 1/2\n",
                        "Run 2/2\n"
                    ]
                },
                {
                    "ename": "IndexError",
                    "evalue": "index 1 is out of bounds for axis 0 with size 1",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m lrs:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m replay_buffer \u001b[38;5;129;01min\u001b[39;00m replay_buffers:\n\u001b[0;32m----> 9\u001b[0m         \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m         os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmv plot.png plots/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env_name, epsilon, lr, replay_buffer[\u001b[38;5;241m0\u001b[39m]))\n",
                        "Cell \u001b[0;32mIn[2], line 169\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(lr, batch_size, replay_buffer_size, epsilon, seeds)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seeds):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseeds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mq_learning_rewards\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m expected_sarsa_rewards[i]\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ-Learning\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
                        "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
                    ]
                }
            ],
            "source": [
                "epsilons = [0.25, 0.1, 0.01]\n",
                "# lrs = [1/4, 1/8, 1/16]\n",
                "lrs = [0.01, 0.001, 0.0001]\n",
                "replay_buffers = [(128, 1000000), (1, 1)]\n",
                "for i in range(1, 10):\n",
                "    for epsilon in epsilons:\n",
                "        for lr in lrs:\n",
                "            for replay_buffer in replay_buffers:\n",
                "                run_experiment(lr, replay_buffer[0], replay_buffer[1], epsilon, i)\n",
                "                time.sleep(1)\n",
                "                os.system('mv plot.png plots/{}_{}_{}_{}.png'.format(env_name, epsilon, lr, replay_buffer[0]))\n",
                "                time.sleep(1)\n",
                "                os.system('rm plot.png')\n",
                "                time.sleep(1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 135,
            "metadata": {},
            "outputs": [],
            "source": [
                "def BoltzmanPolicy(preds,T):\n",
                "    return np.random.choice(len(preds), p=((np.exp(preds)/T) / np.sum(np.exp(preds)/T)))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
