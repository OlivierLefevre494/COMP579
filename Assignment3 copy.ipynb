{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Value-based methods with deep neural network [50 points]\n",
    "Implement Q-learning and Exptected SARSA for both Acrobot-v1 1 and ALE/Assault-ram-v52\n",
    "environments from the Gym suite using the following guidelines:\n",
    "• Use a Neural Network approximation for Q, that is, if x is a vector representing the state\n",
    "and a is the action vector, use Q value(x) = M LP (x; θ), where θ are the parameters of the\n",
    "Value function you need to learn, Q ∈ Rm where m denotes the number of discrete actions.\n",
    "• Model configuration: Initialize the parameters for the value function uniformly between\n",
    "−0.001 and 0.001, we recommend using either a 2 or 3-layer Neural Network for the Value\n",
    "function, with a hidden dimension of 256.\n",
    "• Use an ϵ- greedy policy with three choices of ϵ and step-size parameters 1/4, 1/8, 1/16. and\n",
    "run 50 learning trials with different initializations for the Value function, each having 1000\n",
    "episodes, for each configuration. That means 3(ϵ’s) * 3 (step-sizes) * 50 runs * 1000 episodes.\n",
    "• Repeat the Previous step using a replay buffer (with transitions randomly sampled) and do\n",
    "gradient updates using a mini-batch of transitions. The capacity of the replay buffer is 1M.\n",
    "• Plot training curves with the mean across seeds as lines and the standard deviation as a shaded\n",
    "region. (Performance on the Y-axis, and the episode on the X-axis). Generate 18 graphs\n",
    "covering all configurations per environment. Present separate plots for each environment,\n",
    "with distinct graphs for settings with and without a replay buffer. Use green for Q-Learning\n",
    "and red for Expected SARSA, differentiating hyperparameters with different line styles (e.g.,\n",
    "solid, dashed).\n",
    "• Implement all the methods using any automatic differentiation package, such as Py-\n",
    "torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Q-learning and Exptected SARSA for both Acrobot-v1 1 and ALE/Assault-ram-v52 environments from the Gym suite using the following guidelines:\n",
    "\n",
    "# • Use a Neural Network approximation for Q, that is, if x is a vector representing the state and a is the action vector, use Q value(x) = M LP (x; θ), where θ are the parameters of the Value function you need to learn, Q ∈ Rm where m denotes the number of discrete actions.\n",
    "\n",
    "# • Model configuration: Initialize the parameters for the value function uniformly between −0.001 and 0.001, we recommend using either a 2 or 3-layer Neural Network for the Value function, with a hidden dimension of 256.\n",
    "\n",
    "# • Use an ϵ- greedy policy with three choices of ϵ and step-size parameters 1/4, 1/8, 1/16. and run 50 learning trials with different initializations for the Value function, each having 1000 episodes, for each configuration. That means 3(ϵ’s) * 3 (step-sizes) * 50 runs * 1000 episodes.\n",
    "\n",
    "# • Repeat the Previous step using a replay buffer (with transitions randomly sampled) and do gradient updates using a mini-batch of transitions. The capacity of the replay buffer is 1M.\n",
    "\n",
    "# • Plot training curves with the mean across seeds as lines and the standard deviation as a shaded region. (Performance on the Y-axis, and the episode on the X-axis). Generate 18 graphs covering all configurations per environment. Present separate plots for each environment, with distinct graphs for settings with and without a replay buffer. Use green for Q-Learning and red for Expected SARSA, differentiating hyperparameters with different line styles (e.g., solid, dashed).\n",
    "\n",
    "# • Implement all the methods using any automatic differentiation package, such as Py-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = 'Acrobot-v1'\n",
    "env_name = 'ALE/Assault-ram-v5'\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import math as math\n",
    "import ale_py\n",
    "\n",
    "# Hyperparameters\n",
    "EPISODES = 1000\n",
    "MAX_STEPS = 1000\n",
    "GAMMA = 0.99\n",
    "HIDDEN_DIM = 256\n",
    "SEEDS = 50\n",
    "\n",
    "# Environment\n",
    "env = gym.make(env_name)\n",
    "env._max_episode_steps = MAX_STEPS\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Neural Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, INPUT_DIM, OUTPUT_DIM):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc3 = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "        # torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        # torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        # torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        # self.fc1.bias.data.fill_(0.01)\n",
    "        # self.fc2.bias.data.fill_(0.01)\n",
    "        # self.fc3.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Q-Learning\n",
    "def q_learning(lr, batch_size, replay_buffer_size, epsilon):\n",
    "    q_network = QNetwork(state_dim, action_dim)\n",
    "    # q_network_target = QNetwork()\n",
    "    # q_network_target.load_state_dict(q_network.state_dict())\n",
    "    q_network.to(device)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "    rewards = []\n",
    "    for episode in range(EPISODES):\n",
    "        print(episode)\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    action = q_network(state_tensor).argmax().item()\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "                state_batch = torch.tensor(np.array(state_batch), dtype=torch.float32).to(device)\n",
    "                action_batch = torch.tensor(np.array(action_batch), dtype=torch.long).to(device)\n",
    "                reward_batch = torch.tensor(np.array(reward_batch), dtype=torch.float32).to(device)\n",
    "                next_state_batch = torch.tensor(np.array(next_state_batch), dtype=torch.float32).to(device)\n",
    "                done_batch = torch.tensor(np.array(done_batch), dtype=torch.float32).to(device)\n",
    "                q_values = q_network(state_batch)\n",
    "                # next_q_values = q_network_target(next_state_batch)\n",
    "                next_q_values = q_network(next_state_batch)\n",
    "                target_q_values = q_values.clone()\n",
    "                # for i in range(batch_size):\n",
    "                #     target_q_values[i][action_batch[i]] = reward_batch[i] + GAMMA * next_q_values[i].max() * (1 - done_batch[i])\n",
    "                target_q_values[range(batch_size), action_batch] = reward_batch + GAMMA * next_q_values.max(dim=1).values * (1 - done_batch)\n",
    "                loss = nn.MSELoss()(q_values, target_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            state = next_state\n",
    "        rewards.append(total_reward)\n",
    "        # if episode % 100 == 0:\n",
    "        #     q_network_target.load_state_dict(q_network.state_dict())\n",
    "    return rewards\n",
    "\n",
    "# Expected SARSA\n",
    "def expected_sarsa(lr, batch_size, replay_buffer_size, epsilon):\n",
    "    q_network = QNetwork()\n",
    "    # q_network_target = QNetwork()\n",
    "    # q_network_target.load_state_dict(q_network.state_dict())\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "    rewards = []\n",
    "    for episode in range(EPISODES):\n",
    "        print(episode)\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                    action = q_network(state_tensor).argmax().item()\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "                state_batch = torch.tensor(np.array(state_batch), dtype=torch.float32).to(device)\n",
    "                action_batch = torch.tensor(np.array(action_batch), dtype=torch.long).to(device)\n",
    "                reward_batch = torch.tensor(np.array(reward_batch), dtype=torch.float32).to(device)\n",
    "                next_state_batch = torch.tensor(np.array(next_state_batch), dtype=torch.float32).to(device)\n",
    "                done_batch = torch.tensor(np.array(done_batch), dtype=torch.float32).to(device)\n",
    "                q_values = q_network(state_batch)\n",
    "                # next_q_values = q_network_target(next_state_batch)\n",
    "                next_q_values = q_network(next_state_batch)\n",
    "                target_q_values = q_values.clone()\n",
    "                # for i in range(batch_size):\n",
    "                #     target_q_values[i][action_batch[i]] = reward_batch[i] + GAMMA * next_q_values[i].mean() * (1 - done_batch[i])\n",
    "                target_q_values[range(batch_size), action_batch] = reward_batch + GAMMA * next_q_values.mean(dim=1) * (1 - done_batch)\n",
    "                loss = nn.MSELoss()(q_values, target_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            state = next_state\n",
    "        rewards.append(total_reward)\n",
    "        # if episode % 100 == 0:\n",
    "        #     q_network_target.load_state_dict(q_network.state_dict())\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def run_experiment(lr, batch_size, replay_buffer_size, epsilon):\n",
    "    random_seeds = np.random.randint(0, 1000, size=SEEDS)\n",
    "    q_learning_rewards = np.zeros((SEEDS, EPISODES))\n",
    "    expected_sarsa_rewards = np.zeros((SEEDS, EPISODES))\n",
    "    for i, seed in enumerate(random_seeds):\n",
    "        print(f'Run {i+1}/{SEEDS}')\n",
    "        torch.random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        q_learning_rewards[i] = q_learning(lr, batch_size, replay_buffer_size, epsilon)\n",
    "        expected_sarsa_rewards[i] = expected_sarsa(lr, batch_size, replay_buffer_size, epsilon)\n",
    "        pickle.dump(q_learning_rewards, open(f'RUN#{i}_ACROBOT_q_learning_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl', 'wb'))\n",
    "        pickle.dump(expected_sarsa_rewards, open(f'RUN#{i}_ACROBOT_expected_sarsa_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl', 'wb'))\n",
    "    q_learning_rewards_mean = q_learning_rewards.mean(axis=0)\n",
    "    q_learning_rewards_std = q_learning_rewards.std(axis=0)\n",
    "    expected_sarsa_rewards_mean = expected_sarsa_rewards.mean(axis=0)\n",
    "    expected_sarsa_rewards_std = expected_sarsa_rewards.std(axis=0)\n",
    "    plt.plot(q_learning_rewards_mean, label='Q-Learning', color='green')\n",
    "    plt.fill_between(range(EPISODES), q_learning_rewards_mean - q_learning_rewards_std, q_learning_rewards_mean + q_learning_rewards_std, color='green', alpha=0.2)\n",
    "    plt.plot(expected_sarsa_rewards_mean, label='Expected SARSA', color='red')\n",
    "    plt.fill_between(range(EPISODES), expected_sarsa_rewards_mean - expected_sarsa_rewards_std, expected_sarsa_rewards_mean + expected_sarsa_rewards_std, color='red', alpha=0.2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEEDS = 1\n",
    "# EPISODES = 100\n",
    "# run_experiment(0.001, 64, 1000000, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/50\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m lrs:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m replay_buffer \u001b[38;5;129;01min\u001b[39;00m replay_buffers:\n\u001b[0;32m----> 7\u001b[0m         \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m         os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmv plot.png plots/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env_name, epsilon, lr, replay_buffer[\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[1], line 160\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(lr, batch_size, replay_buffer_size, epsilon)\u001b[0m\n\u001b[1;32m    158\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m    159\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m--> 160\u001b[0m     q_learning_rewards[i] \u001b[38;5;241m=\u001b[39m \u001b[43mq_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     expected_sarsa_rewards[i] \u001b[38;5;241m=\u001b[39m expected_sarsa(lr, batch_size, replay_buffer_size, epsilon)\n\u001b[1;32m    162\u001b[0m q_learning_rewards_mean \u001b[38;5;241m=\u001b[39m q_learning_rewards\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m, in \u001b[0;36mq_learning\u001b[0;34m(lr, batch_size, replay_buffer_size, epsilon)\u001b[0m\n\u001b[1;32m     92\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(q_values, target_q_values)\n\u001b[1;32m     93\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     96\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "epsilons = [0.25, 0.125, 0.0625]\n",
    "lrs = [1/4, 1/8, 1/16]\n",
    "replay_buffers = [(1, 1), (32, 1000000)]\n",
    "for epsilon in epsilons:\n",
    "    for lr in lrs:\n",
    "        for replay_buffer in replay_buffers:\n",
    "            run_experiment(lr, replay_buffer[0], replay_buffer[1], epsilon)\n",
    "            time.sleep(1)\n",
    "            os.system('mv plot.png plots/{}_{}_{}_{}.png'.format(env_name, epsilon, lr, replay_buffer[0]))\n",
    "            time.sleep(1)\n",
    "            os.system('rm plot.png')\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoltzmanPolicy(preds, temp):\n",
    "    probs = torch.softmax(preds / temp, dim=1)\n",
    "    a = torch.distributions.Categorical(probs).sample().item()\n",
    "    return a, probs[0][a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(episodes, lr, gamma, T, decay, decay_rate, end_temp):\n",
    "    rewards = []\n",
    "    q_network = QNetwork(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    temp = T\n",
    "    end_temp = end_temp\n",
    "    for i in range(episodes):\n",
    "        if decay:\n",
    "            temp = T - ((T - end_temp) * i / episodes)\n",
    "        total_reward = 0\n",
    "        if i % 100 == 0:\n",
    "            print(\"EPISODE#\", i)\n",
    "        ep = []\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done and step < 1000:\n",
    "            action, prob = BoltzmanPolicy(q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)), temp)\n",
    "            next_state, reward, done, truncated, _= env.step(action)\n",
    "            total_reward += reward\n",
    "            ep.append((state, action, reward, prob))\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        grad = 0\n",
    "        for t in range(len(ep)):\n",
    "            G = sum([gamma**(i-t-1) * ep[i][2] for i in range(t, len(ep))])\n",
    "            # print(G)\n",
    "            grad += (gamma**t) * G * torch.log(max(prob,torch.tensor(1e-8)))\n",
    "        optimizer.zero_grad()\n",
    "        grad.backward()\n",
    "        optimizer.step()\n",
    "        rewards.append(total_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2C(episodes, gamma, policy_lr,value_lr, T, decay, decay_rate, end_temp):\n",
    "    policy_network = QNetwork(state_dim, action_dim)\n",
    "    value_network = QNetwork(state_dim, 1)\n",
    "    optimizer_actor = optim.Adam(policy_network.parameters(), lr=0.0001)\n",
    "    optimizer_value = optim.Adam(value_network.parameters(), lr=0.001)\n",
    "    rewards = []\n",
    "    temp = T\n",
    "    end_temp = end_temp\n",
    "    for i in range(episodes):\n",
    "        if decay:\n",
    "            temp = T - ((T - end_temp) * i / episodes)\n",
    "        total_reward = 0\n",
    "        if i % 100 == 0:\n",
    "            print(\"EPISODE#\", i)\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "        while not done  and step < 1000:\n",
    "            with torch.no_grad():\n",
    "                action, prob = BoltzmanPolicy(policy_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)), temp)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                advantage = reward - value_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)).detach()\n",
    "            else:\n",
    "                advantage = reward + gamma * value_network(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)).detach() - value_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)).detach()\n",
    "            preds = policy_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))[0]\n",
    "            policy_loss -= torch.log(torch.exp(preds[action]/T)/torch.sum(torch.exp(preds/T))) * advantage\n",
    "            value_loss +=  0.5*(reward + gamma * value_network(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)).detach() - value_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)))**2\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        mean_policy_loss = policy_loss / step\n",
    "        mean_value_loss = value_loss / step\n",
    "        optimizer_actor.zero_grad()\n",
    "        optimizer_value.zero_grad()\n",
    "        mean_policy_loss.backward()\n",
    "        mean_value_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "        optimizer_value.step()\n",
    "        rewards.append(total_reward)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment2(lr, T, decay, decay_rate, gamma, episodes, seeds):\n",
    "    random_seeds = np.random.randint(0, 50, size=seeds)\n",
    "    a2c_learning_rewards = np.zeros((seeds, EPISODES))\n",
    "    reinforce_rewards = np.zeros((seeds, EPISODES))\n",
    "    seeds = seeds\n",
    "    for i in range(seeds):\n",
    "        print(f'Run {i+1}/{seeds}')\n",
    "        torch.manual_seed(i)\n",
    "        np.random.seed(i)\n",
    "        a2c_learning_rewards[i] = A2C(episodes=episodes, gamma=gamma, policy_lr=lr, value_lr=lr, T=T, decay=decay, decay_rate=decay_rate, end_temp=decay_rate)\n",
    "        reinforce_rewards[i] = reinforce(episodes=episodes, lr=lr, gamma=gamma, T=T, decay=decay, decay_rate=decay_rate, end_temp=decay_rate)\n",
    "        pickle.dump(a2c_learning_rewards[i], open(f'RUN#{i}_ASST_a2c_learning_rewards_{lr}_{T}_{decay}_{decay_rate}.pkl', 'wb'))\n",
    "        pickle.dump(reinforce_rewards[i], open(f'RUN#{i}_ASSOT_reinforce_rewards_{lr}_{T}_{decay}_{decay_rate}.pkl', 'wb'))\n",
    "    a2c_learning_rewards_mean = a2c_learning_rewards.mean(axis=0)\n",
    "    a2c_learning_rewards_std = a2c_learning_rewards.std(axis=0)/math.sqrt(seeds)\n",
    "    reinforce_rewards_mean = reinforce_rewards.mean(axis=0)\n",
    "    reinforce_rewards_std = reinforce_rewards.std(axis=0)/math.sqrt(seeds)\n",
    "    plt.plot(a2c_learning_rewards_mean, label='A2C', color='green')\n",
    "    plt.fill_between(range(EPISODES), a2c_learning_rewards_mean - a2c_learning_rewards_std, a2c_learning_rewards_mean + a2c_learning_rewards_std, color='green', alpha=0.2)\n",
    "    plt.plot(reinforce_rewards_mean, label='Reinforce', color='red')\n",
    "    plt.fill_between(range(EPISODES), reinforce_rewards_mean - reinforce_rewards_std, reinforce_rewards_mean + reinforce_rewards_std, color='red', alpha=0.2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.savefig('plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/3\n",
      "EPISODE# 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "T = [[2, False, 1]]\n",
    "import pickle\n",
    "#REINFORCE\n",
    "lr = [1e-4]\n",
    "SEEDS = 3\n",
    "for stuff in T:\n",
    "    run_experiment2(lr=lr[0], T=stuff[0], decay=stuff[1], decay_rate=stuff[2], gamma=0.99, episodes=1000, seeds=SEEDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
