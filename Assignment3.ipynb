{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Value-based methods with deep neural network [50 points]\n",
    "Implement Q-learning and Exptected SARSA for both Acrobot-v1 1 and ALE/Assault-ram-v52\n",
    "environments from the Gym suite using the following guidelines:\n",
    "• Use a Neural Network approximation for Q, that is, if x is a vector representing the state\n",
    "and a is the action vector, use Q value(x) = M LP (x; θ), where θ are the parameters of the\n",
    "Value function you need to learn, Q ∈ Rm where m denotes the number of discrete actions.\n",
    "• Model configuration: Initialize the parameters for the value function uniformly between\n",
    "−0.001 and 0.001, we recommend using either a 2 or 3-layer Neural Network for the Value\n",
    "function, with a hidden dimension of 256.\n",
    "• Use an ϵ- greedy policy with three choices of ϵ and step-size parameters 1/4, 1/8, 1/16. and\n",
    "run 50 learning trials with different initializations for the Value function, each having 1000\n",
    "episodes, for each configuration. That means 3(ϵ’s) * 3 (step-sizes) * 50 runs * 1000 episodes.\n",
    "• Repeat the Previous step using a replay buffer (with transitions randomly sampled) and do\n",
    "gradient updates using a mini-batch of transitions. The capacity of the replay buffer is 1M.\n",
    "• Plot training curves with the mean across seeds as lines and the standard deviation as a shaded\n",
    "region. (Performance on the Y-axis, and the episode on the X-axis). Generate 18 graphs\n",
    "covering all configurations per environment. Present separate plots for each environment,\n",
    "with distinct graphs for settings with and without a replay buffer. Use green for Q-Learning\n",
    "and red for Expected SARSA, differentiating hyperparameters with different line styles (e.g.,\n",
    "solid, dashed).\n",
    "• Implement all the methods using any automatic differentiation package, such as Py-\n",
    "torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Q-learning and Exptected SARSA for both Acrobot-v1 1 and ALE/Assault-ram-v52 environments from the Gym suite using the following guidelines:\n",
    "\n",
    "# • Use a Neural Network approximation for Q, that is, if x is a vector representing the state and a is the action vector, use Q value(x) = M LP (x; θ), where θ are the parameters of the Value function you need to learn, Q ∈ Rm where m denotes the number of discrete actions.\n",
    "\n",
    "# • Model configuration: Initialize the parameters for the value function uniformly between −0.001 and 0.001, we recommend using either a 2 or 3-layer Neural Network for the Value function, with a hidden dimension of 256.\n",
    "\n",
    "# • Use an ϵ- greedy policy with three choices of ϵ and step-size parameters 1/4, 1/8, 1/16. and run 50 learning trials with different initializations for the Value function, each having 1000 episodes, for each configuration. That means 3(ϵ’s) * 3 (step-sizes) * 50 runs * 1000 episodes.\n",
    "\n",
    "# • Repeat the Previous step using a replay buffer (with transitions randomly sampled) and do gradient updates using a mini-batch of transitions. The capacity of the replay buffer is 1M.\n",
    "\n",
    "# • Plot training curves with the mean across seeds as lines and the standard deviation as a shaded region. (Performance on the Y-axis, and the episode on the X-axis). Generate 18 graphs covering all configurations per environment. Present separate plots for each environment, with distinct graphs for settings with and without a replay buffer. Use green for Q-Learning and red for Expected SARSA, differentiating hyperparameters with different line styles (e.g., solid, dashed).\n",
    "\n",
    "# • Implement all the methods using any automatic differentiation package, such as Py-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Acrobot-v1'\n",
    "# env_name = 'Assault-ram-v0'\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Hyperparameters\n",
    "EPISODES = 1000\n",
    "MAX_STEPS = 1000\n",
    "GAMMA = 0.99\n",
    "HIDDEN_DIM = 256\n",
    "SEEDS = 10\n",
    "\n",
    "# Environment\n",
    "env = gym.make(env_name)\n",
    "env._max_episode_steps = MAX_STEPS\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"mps\"\n",
    "\n",
    "# Neural Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc3 = nn.Linear(HIDDEN_DIM, action_dim)\n",
    "\n",
    "        # Initialize parameters\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                print(\"initializing layer\", m)\n",
    "                nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                nn.init.uniform_(m.bias, -0.01, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Q-Learning\n",
    "def q_learning(lr, batch_size, replay_buffer_size, epsilon):\n",
    "    q_network = QNetwork()\n",
    "    # q_network_target = QNetwork()\n",
    "    # q_network_target.load_state_dict(q_network.state_dict())\n",
    "    q_network.to(device)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr, eps=0.0003125)\n",
    "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "    rewards = []\n",
    "    for episode in range(EPISODES):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    action = q_network(state_tensor).argmax().item()\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "                state_batch = torch.tensor(np.array(state_batch), dtype=torch.float32).to(device)\n",
    "                action_batch = torch.tensor(np.array(action_batch), dtype=torch.long).to(device)\n",
    "                reward_batch = torch.tensor(np.array(reward_batch), dtype=torch.float32).to(device)\n",
    "                next_state_batch = torch.tensor(np.array(next_state_batch), dtype=torch.float32).to(device)\n",
    "                done_batch = torch.tensor(np.array(done_batch), dtype=torch.float32).to(device)\n",
    "                q_values = q_network(state_batch)\n",
    "                # next_q_values = q_network_target(next_state_batch)\n",
    "                next_q_values = q_network(next_state_batch)\n",
    "                target_q_values = q_values.clone()\n",
    "                # for i in range(batch_size):\n",
    "                #     target_q_values[i][action_batch[i]] = reward_batch[i] + GAMMA * next_q_values[i].max() * (1 - done_batch[i])\n",
    "                target_q_values[range(batch_size), action_batch] = reward_batch + GAMMA * next_q_values.max(dim=1).values * (1 - done_batch)\n",
    "                loss = nn.MSELoss()(q_values, target_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            state = next_state\n",
    "        rewards.append(total_reward)\n",
    "        if episode % 100 == 0:\n",
    "            # q_network_target.load_state_dict(q_network.state_dict())\n",
    "            print(episode, total_reward, loss.item())\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# Expected SARSA\n",
    "def expected_sarsa(lr, batch_size, replay_buffer_size, epsilon):\n",
    "    q_network = QNetwork()\n",
    "    # q_network_target = QNetwork()\n",
    "    # q_network_target.load_state_dict(q_network.state_dict())\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr, eps=0.0003125)\n",
    "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "    rewards = []\n",
    "    for episode in range(EPISODES):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                    action = q_network(state_tensor).argmax().item()\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "                state_batch = torch.tensor(np.array(state_batch), dtype=torch.float32).to(device)\n",
    "                action_batch = torch.tensor(np.array(action_batch), dtype=torch.long).to(device)\n",
    "                reward_batch = torch.tensor(np.array(reward_batch), dtype=torch.float32).to(device)\n",
    "                next_state_batch = torch.tensor(np.array(next_state_batch), dtype=torch.float32).to(device)\n",
    "                done_batch = torch.tensor(np.array(done_batch), dtype=torch.float32).to(device)\n",
    "                q_values = q_network(state_batch)\n",
    "                # next_q_values = q_network_target(next_state_batch)\n",
    "                next_q_values = q_network(next_state_batch)\n",
    "                target_q_values = q_values.clone()\n",
    "                # expected sarsa\n",
    "                probs = torch.ones(batch_size, action_dim) * epsilon / action_dim\n",
    "                probs[range(batch_size), next_q_values.argmax(dim=1)] += 1 - epsilon\n",
    "                target_q_values[range(batch_size), action_batch] = reward_batch + GAMMA * (probs * next_q_values).sum(dim=1) * (1 - done_batch)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, target_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            state = next_state\n",
    "        rewards.append(total_reward)\n",
    "        if episode % 100 == 0:\n",
    "        #     q_network_target.load_state_dict(q_network.state_dict())\n",
    "            print(episode, total_reward, loss.item())\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def run_experiment(lr, batch_size, replay_buffer_size, epsilon, seeds):\n",
    "    random_seeds = [0, 1, 2, 3, 4, 5, 6, 8, 11, 12]\n",
    "    print(seeds)\n",
    "    q_learning_rewards = np.zeros((seeds, EPISODES))\n",
    "    expected_sarsa_rewards = np.zeros((seeds, EPISODES))\n",
    "    if os.path.exists(f'q_learning_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl'):\n",
    "        old_q_learning_rewards = pickle.load(open(f'q_learning_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl', 'rb'))\n",
    "        q_learning_rewards[:min(old_q_learning_rewards.shape[0], seeds)] = old_q_learning_rewards[:min(old_q_learning_rewards.shape[0], seeds)]\n",
    "    if os.path.exists(f'expected_sarsa_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl'):\n",
    "        old_expected_sarsa_rewards = pickle.load(open(f'expected_sarsa_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl', 'rb'))\n",
    "        expected_sarsa_rewards[:min(old_expected_sarsa_rewards.shape[0], seeds)] = old_expected_sarsa_rewards[:min(old_expected_sarsa_rewards.shape[0], seeds)]\n",
    "    # for i, seed in enumerate(random_seeds):\n",
    "    for i in range(seeds):\n",
    "        print(f'Run {i+1}/{seeds}')\n",
    "        if q_learning_rewards[i].sum() != 0 and expected_sarsa_rewards[i].sum() != 0:\n",
    "            continue\n",
    "        print('Q-Learning')\n",
    "        q_learning_rewards[i] = q_learning(lr, batch_size, replay_buffer_size, epsilon)\n",
    "        print('Expected SARSA')\n",
    "        expected_sarsa_rewards[i] = expected_sarsa(lr, batch_size, replay_buffer_size, epsilon)\n",
    "        q_learning_rewards_mean = q_learning_rewards[:i+1].mean(axis=0)\n",
    "        q_learning_rewards_std = q_learning_rewards[:i+1].std(axis=0)\n",
    "        expected_sarsa_rewards_mean = expected_sarsa_rewards[:i+1].mean(axis=0)\n",
    "        expected_sarsa_rewards_std = expected_sarsa_rewards[:i+1].std(axis=0)\n",
    "        plt.plot(q_learning_rewards_mean, label='Q-Learning', color='green')\n",
    "        plt.fill_between(range(EPISODES), q_learning_rewards_mean - q_learning_rewards_std, q_learning_rewards_mean + q_learning_rewards_std, color='green', alpha=0.2)\n",
    "        plt.plot(expected_sarsa_rewards_mean, label='Expected SARSA', color='red')\n",
    "        plt.fill_between(range(EPISODES), expected_sarsa_rewards_mean - expected_sarsa_rewards_std, expected_sarsa_rewards_mean + expected_sarsa_rewards_std, color='red', alpha=0.2)\n",
    "        plt.title(f'lr={lr}, batch_size={batch_size}, replay_buffer_size={replay_buffer_size}, epsilon={epsilon}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'plot_{i}_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.png')\n",
    "        plt.show()\n",
    "        pickle.dump(q_learning_rewards, open(f'q_learning_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl', 'wb'))\n",
    "        pickle.dump(expected_sarsa_rewards, open(f'expected_sarsa_rewards_{lr}_{batch_size}_{replay_buffer_size}_{epsilon}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEEDS = 1\n",
    "# EPISODES = 1000\n",
    "# run_experiment(0.001, 128, 1000000, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "1\n",
      "Run 1/1\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "2\n",
      "Run 1/2\n",
      "Run 2/2\n",
      "3\n",
      "Run 1/3\n",
      "Run 2/3\n",
      "Run 3/3\n",
      "3\n",
      "Run 1/3\n",
      "Run 2/3\n",
      "Run 3/3\n",
      "Q-Learning\n",
      "initializing layer Linear(in_features=6, out_features=256, bias=True)\n",
      "initializing layer Linear(in_features=256, out_features=256, bias=True)\n",
      "initializing layer Linear(in_features=256, out_features=3, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -724.0 0.03524211421608925\n",
      "100 -930.0 17.181575775146484\n",
      "200 -1000.0 23.79694938659668\n"
     ]
    }
   ],
   "source": [
    "epsilons = [0.25, 0.1, 0.01]\n",
    "# lrs = [1/4, 1/8, 1/16]\n",
    "lrs = [0.01, 0.001, 0.0001]\n",
    "replay_buffers = [(128, 1000000), (1, 1)]\n",
    "for i in range(1, 10):\n",
    "    for epsilon in epsilons:\n",
    "        for lr in lrs:\n",
    "            for replay_buffer in replay_buffers:\n",
    "                run_experiment(lr, replay_buffer[0], replay_buffer[1], epsilon, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(stepsize, episodes, lr):\n",
    "    q_network = QNetwork()\n",
    "    for i in range(episodes):\n",
    "        ep = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.random.choice(env.action_space, p=[i.item() for i in q_network(state)])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            ep.append((state, action, reward))\n",
    "            state = next_state\n",
    "        grad = 0\n",
    "        for t in range(len(ep)):\n",
    "            G = sum([r for (_, _, r) in ep[t:]]) # sum of rewards from time t\n",
    "        return q_network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
