{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoltzmanPolicy(preds, temp):\n",
    "    probs = torch.softmax(preds / temp, dim=1)\n",
    "    a = torch.distributions.Categorical(probs).sample().item()\n",
    "    return a, probs[0][a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(episodes, lr, gamma, T, decay, decay_rate, end_temp):\n",
    "    rewards = []\n",
    "    q_network = QNetwork(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    temp = T\n",
    "    end_temp = end_temp\n",
    "    for i in range(episodes):\n",
    "        if decay:\n",
    "            temp = T - ((T - end_temp) * i / episodes)\n",
    "        total_reward = 0\n",
    "        if i % 100 == 0:\n",
    "            print(\"EPISODE#\", i)\n",
    "        ep = []\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done and step < 1000:\n",
    "            action, prob = BoltzmanPolicy(q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)), temp)\n",
    "            next_state, reward, done, truncated, _= env.step(action)\n",
    "            total_reward += reward\n",
    "            ep.append((state, action, reward, prob))\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        grad = 0\n",
    "        for t in range(len(ep)):\n",
    "            G = sum([gamma**(i-t-1) * ep[i][2] for i in range(t, len(ep))])\n",
    "            # print(G)\n",
    "            grad += (gamma**t) * G * torch.log(max(prob,torch.tensor(1e-8)))\n",
    "        optimizer.zero_grad()\n",
    "        grad.backward()\n",
    "        optimizer.step()\n",
    "        rewards.append(total_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2C(episodes, gamma, policy_lr,value_lr, T, decay, decay_rate, end_temp):\n",
    "    policy_network = QNetwork(state_dim, action_dim)\n",
    "    value_network = QNetwork(state_dim, 1)\n",
    "    optimizer_actor = optim.Adam(policy_network.parameters(), lr=0.0001)\n",
    "    optimizer_value = optim.Adam(value_network.parameters(), lr=0.001)\n",
    "    rewards = []\n",
    "    temp = T\n",
    "    end_temp = end_temp\n",
    "    for i in range(episodes):\n",
    "        if decay:\n",
    "            temp = T - ((T - end_temp) * i / episodes)\n",
    "        total_reward = 0\n",
    "        if i % 100 == 0:\n",
    "            print(\"EPISODE#\", i)\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "        while not done  and step < 1000:\n",
    "            with torch.no_grad():\n",
    "                action, prob = BoltzmanPolicy(policy_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)), temp)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                advantage = reward - value_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)).detach()\n",
    "            else:\n",
    "                advantage = reward + gamma * value_network(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)).detach() - value_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)).detach()\n",
    "            preds = policy_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))[0]\n",
    "            policy_loss -= torch.log(torch.exp(preds[action]/T)/torch.sum(torch.exp(preds/T))) * advantage\n",
    "            value_loss +=  0.5*(reward + gamma * value_network(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)).detach() - value_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)))**2\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        mean_policy_loss = policy_loss / step\n",
    "        mean_value_loss = value_loss / step\n",
    "        optimizer_actor.zero_grad()\n",
    "        optimizer_value.zero_grad()\n",
    "        mean_policy_loss.backward()\n",
    "        mean_value_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "        optimizer_value.step()\n",
    "        rewards.append(total_reward)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment2(lr, T, decay, decay_rate, gamma, episodes, seeds):\n",
    "    random_seeds = np.random.randint(0, 50, size=seeds)\n",
    "    a2c_learning_rewards = np.zeros((seeds, EPISODES))\n",
    "    reinforce_rewards = np.zeros((seeds, EPISODES))\n",
    "    seeds = seeds\n",
    "    for i in range(seeds):\n",
    "        print(f'Run {i+1}/{seeds}')\n",
    "        torch.manual_seed(i)\n",
    "        np.random.seed(i)\n",
    "        a2c_learning_rewards[i] = A2C(episodes=episodes, gamma=gamma, policy_lr=lr, value_lr=lr, T=T, decay=decay, decay_rate=decay_rate, end_temp=decay_rate)\n",
    "        reinforce_rewards[i] = reinforce(episodes=episodes, lr=lr, gamma=gamma, T=T, decay=decay, decay_rate=decay_rate, end_temp=decay_rate)\n",
    "        pickle.dump(a2c_learning_rewards[i], open(f'RUN#{i}_ASST_a2c_learning_rewards_{lr}_{T}_{decay}_{decay_rate}.pkl', 'wb'))\n",
    "        pickle.dump(reinforce_rewards[i], open(f'RUN#{i}_ASSOT_reinforce_rewards_{lr}_{T}_{decay}_{decay_rate}.pkl', 'wb'))\n",
    "    a2c_learning_rewards_mean = a2c_learning_rewards.mean(axis=0)\n",
    "    a2c_learning_rewards_std = a2c_learning_rewards.std(axis=0)/math.sqrt(seeds)\n",
    "    reinforce_rewards_mean = reinforce_rewards.mean(axis=0)\n",
    "    reinforce_rewards_std = reinforce_rewards.std(axis=0)/math.sqrt(seeds)\n",
    "    plt.plot(a2c_learning_rewards_mean, label='A2C', color='green')\n",
    "    plt.fill_between(range(EPISODES), a2c_learning_rewards_mean - a2c_learning_rewards_std, a2c_learning_rewards_mean + a2c_learning_rewards_std, color='green', alpha=0.2)\n",
    "    plt.plot(reinforce_rewards_mean, label='Reinforce', color='red')\n",
    "    plt.fill_between(range(EPISODES), reinforce_rewards_mean - reinforce_rewards_std, reinforce_rewards_mean + reinforce_rewards_std, color='red', alpha=0.2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.savefig('plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = [[2, False, 1]]\n",
    "import pickle\n",
    "#REINFORCE\n",
    "lr = [1e-4]\n",
    "SEEDS = 3\n",
    "for stuff in T:\n",
    "    run_experiment2(lr=lr[0], T=stuff[0], decay=stuff[1], decay_rate=stuff[2], gamma=0.99, episodes=1000, seeds=SEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "epsilons = [0.25, 0.125, 0.0625]\n",
    "lrs = [1/4, 1/8, 1/16]\n",
    "replay_buffers = [(1, 1), (32, 1000000)]\n",
    "for epsilon in epsilons:\n",
    "    for lr in lrs:\n",
    "        for replay_buffer in replay_buffers:\n",
    "            run_experiment(lr, replay_buffer[0], replay_buffer[1], epsilon)\n",
    "            time.sleep(1)\n",
    "            os.system('mv plot.png plots/{}_{}_{}_{}.png'.format(env_name, epsilon, lr, replay_buffer[0]))\n",
    "            time.sleep(1)\n",
    "            os.system('rm plot.png')\n",
    "            time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
