{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Policy-based methods with deep neural network [30 points]\n",
    "# Implement REINFORCE and Actor-Critic method for both the Acrobot-v1 and ALE/Assault-ram-v5 environments.\n",
    "# π(ai|s) = (exp(z(s, ai)/T) / Σa∈A exp(z(s, a)/T))\n",
    "# • Implement a Boltzman’s Policy as in eq. (3) and Neural Network approximation for z. That is z(s) = M LP (s, θ), where θ are the parameters of z you need to learn, and a ∈ {1, . . . , m} is a discrete action. In the case of the Actor-Critic algorithm use a Neural Network approximation for the State-Value function ˆV (s, w), where w are the parameters of the State-Value function.\n",
    "# • Similar to Question-1, use appropriate initialization & model configuration for the policy parameters and state-value parameters.\n",
    "# • Implement a Boltzman’s Policy and run 50 learning trials with different initializations for the model, each having 1000 episodes for the following two configurations. 1. A fixed temperature T > 0 (of your choice) and 2. A decreasing temperature T . (50 runs * 1000 episodes * 2 configuration) You are free to choose your step sizes for these implementations.\n",
    "# • Plot training curves with the mean across seeds as lines and the standard deviation as a shaded region. (Performance on the Y-axis, and the episode on the X-axis). Generate 2 graphs covering all configurations per environment. Use green for REINFORCE and red for Actor-Critic, differentiating hyperparameters with different line styles (e.g., solid, dashed).\n",
    "# • Similar to value-based methods, you can implement all the methods using any automatic differentiation package, such as Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# env_name = 'Acrobot-v1'\n",
    "env_name = 'ALE/Assault-ram-v5'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "\n",
    "# Hyperparameters\n",
    "EPISODES = 1000\n",
    "MAX_STEPS = 1000\n",
    "GAMMA = 0.99\n",
    "HIDDEN_DIM = 256\n",
    "SEEDS = 10\n",
    "\n",
    "# Environment\n",
    "env = gym.make(env_name)\n",
    "env._max_episode_steps = MAX_STEPS\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Neural Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, action_dim)\n",
    "\n",
    "        # Initialize parameters\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                print(\"initializing layer\", m)\n",
    "                nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                nn.init.uniform_(m.bias, -0.01, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, 1)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                print(\"initializing layer\", m)\n",
    "                nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                nn.init.uniform_(m.bias, -0.01, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0 \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def reinforce(lr, temperature, final_temperature, seeds):\n",
    "    policy_network = PolicyNetwork().to(device)\n",
    "    optimizer_policy = optim.Adam(policy_network.parameters(), lr=lr)\n",
    "    rewards = []\n",
    "    initial_temperature = temperature\n",
    "    for episode in range(EPISODES):\n",
    "        rewards_episode = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        temperature = initial_temperature - (initial_temperature - final_temperature) * episode / EPISODES\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            logits = policy_network(state_tensor)\n",
    "            probs = F.softmax(logits / temperature, dim=1)\n",
    "            m = Categorical(probs)\n",
    "            action = m.sample().item()\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            rewards_episode.append(reward)\n",
    "            log_probs.append(m.log_prob(torch.tensor(action).to(device)))\n",
    "            state = next_state\n",
    "        rewards.append(sum(rewards_episode))\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in rewards_episode[::-1]:\n",
    "            R = r + GAMMA * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        policy_loss = -log_probs * returns\n",
    "        optimizer_policy.zero_grad()\n",
    "        policy_loss.sum().backward()\n",
    "        optimizer_policy.step()\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(episode, sum(rewards_episode), policy_loss.sum().item())\n",
    "    return rewards\n",
    "\n",
    "def actor_critic(lr, temperature, final_temperature, seeds):\n",
    "    policy_network = PolicyNetwork().to(device)\n",
    "    value_network = ValueNetwork().to(device)\n",
    "    optimizer_policy = optim.Adam(policy_network.parameters(), lr=lr)\n",
    "    optimizer_value = optim.Adam(value_network.parameters(), lr=lr)\n",
    "    rewards = []\n",
    "    initial_temperature = temperature\n",
    "    for episode in range(EPISODES):\n",
    "        rewards_episode = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        temperature = initial_temperature - (initial_temperature - final_temperature) * episode / EPISODES\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            logits = policy_network(state_tensor)\n",
    "            probs = F.softmax(logits / temperature, dim=1)\n",
    "            m = Categorical(probs)\n",
    "            action = m.sample().item()\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            rewards_episode.append(reward)\n",
    "            log_probs.append(m.log_prob(torch.tensor(action).to(device)))\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            value = value_network(state_tensor)\n",
    "            values.append(value)\n",
    "            state = next_state\n",
    "        rewards.append(sum(rewards_episode))\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in rewards_episode[::-1]:\n",
    "            R = r + GAMMA * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        values = torch.stack(values).squeeze()\n",
    "        advantages = returns - values\n",
    "        policy_loss = -log_probs * advantages\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        optimizer_policy.zero_grad()\n",
    "        optimizer_value.zero_grad()\n",
    "        policy_loss.sum().backward(retain_graph=True)\n",
    "        value_loss.backward()\n",
    "        optimizer_policy.step()\n",
    "        optimizer_value.step()\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(episode, sum(rewards_episode), policy_loss.sum().item(), value_loss.item())\n",
    "    return rewards\n",
    "\n",
    "def run_experiment(lr, temperature, final_temperature, seeds):\n",
    "    if temperature < final_temperature:\n",
    "        return\n",
    "    random_seeds = [0, 1, 2, 3, 4, 5, 6, 8, 11, 12]\n",
    "    print(seeds)\n",
    "    reinforce_rewards = np.zeros((seeds, EPISODES))\n",
    "    actor_critic_rewards = np.zeros((seeds, EPISODES))\n",
    "    if os.path.exists(f'reinforce_rewards_{lr}_{temperature}_{final_temperature}.pkl'):\n",
    "        old_reinforce_rewards = pickle.load(open(f'reinforce_rewards_{lr}_{temperature}_{final_temperature}.pkl', 'rb'))\n",
    "        reinforce_rewards[:min(old_reinforce_rewards.shape[0], seeds)] = old_reinforce_rewards[:min(old_reinforce_rewards.shape[0], seeds)]\n",
    "    if os.path.exists(f'actor_critic_rewards_{lr}_{temperature}_{final_temperature}.pkl'):\n",
    "        old_actor_critic_rewards = pickle.load(open(f'actor_critic_rewards_{lr}_{temperature}_{final_temperature}.pkl', 'rb'))\n",
    "        actor_critic_rewards[:min(old_actor_critic_rewards.shape[0], seeds)] = old_actor_critic_rewards[:min(old_actor_critic_rewards.shape[0], seeds)]\n",
    "    # for i, seed in enumerate(random_seeds):\n",
    "    for i in range(seeds):\n",
    "        print(f'Run {i+1}/{seeds}')\n",
    "        if reinforce_rewards[i].sum() == 0:\n",
    "            print('REINFORCE')\n",
    "            reinforce_rewards[i] = reinforce(lr, temperature, final_temperature, seeds)\n",
    "            pickle.dump(reinforce_rewards, open(f'reinforce_rewards_{lr}_{temperature}_{final_temperature}.pkl', 'wb'))\n",
    "        if actor_critic_rewards[i].sum() == 0:\n",
    "            print('Actor-Critic')\n",
    "            actor_critic_rewards[i] = actor_critic(lr, temperature, final_temperature, seeds)\n",
    "            pickle.dump(actor_critic_rewards, open(f'actor_critic_rewards_{lr}_{temperature}_{final_temperature}.pkl', 'wb'))\n",
    "        reinforce_rewards_mean = reinforce_rewards[:i+1].mean(axis=0)\n",
    "        reinforce_rewards_std = reinforce_rewards[:i+1].std(axis=0)\n",
    "        actor_critic_rewards_mean = actor_critic_rewards[:i+1].mean(axis=0)\n",
    "        actor_critic_rewards_std = actor_critic_rewards[:i+1].std(axis=0)\n",
    "        plt.plot(reinforce_rewards_mean, label='REINFORCE', color='green')\n",
    "        plt.fill_between(range(EPISODES), reinforce_rewards_mean - reinforce_rewards_std, reinforce_rewards_mean + reinforce_rewards_std, color='green', alpha=0.2)\n",
    "        plt.plot(actor_critic_rewards_mean, label='Actor-Critic', color='red')\n",
    "        plt.fill_between(range(EPISODES), actor_critic_rewards_mean - actor_critic_rewards_std, actor_critic_rewards_mean + actor_critic_rewards_std, color='red', alpha=0.2)\n",
    "        plt.title(f'lr={lr}, temperature={temperature}, final_temperature={final_temperature}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'plot_{i}_{lr}_{temperature}_{final_temperature}.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 1 0.1\n",
      "1\n",
      "Run 1/1\n",
      "REINFORCE\n",
      "initializing layer Linear(in_features=128, out_features=256, bias=True)\n",
      "initializing layer Linear(in_features=256, out_features=7, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanlamontange-kratz/Library/Python/3.9/lib/python/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 189.0 18909330.0\n",
      "10 0.0 0.0\n",
      "20 0.0 0.0\n",
      "30 0.0 0.0\n",
      "40 0.0 0.0\n",
      "50 0.0 0.0\n",
      "60 0.0 0.0\n",
      "70 0.0 0.0\n",
      "80 0.0 0.0\n",
      "90 0.0 0.0\n",
      "100 0.0 0.0\n",
      "110 0.0 0.0\n",
      "120 0.0 0.0\n",
      "130 0.0 0.0\n",
      "140 0.0 0.0\n",
      "150 0.0 0.0\n",
      "160 0.0 0.0\n",
      "170 0.0 0.0\n",
      "180 0.0 0.0\n",
      "190 0.0 0.0\n",
      "200 0.0 0.0\n",
      "210 0.0 0.0\n",
      "220 0.0 0.0\n",
      "230 0.0 0.0\n",
      "240 0.0 0.0\n",
      "250 0.0 0.0\n",
      "260 0.0 0.0\n",
      "270 0.0 0.0\n",
      "280 0.0 0.0\n",
      "290 0.0 0.0\n",
      "300 0.0 0.0\n",
      "310 0.0 0.0\n",
      "320 0.0 0.0\n",
      "330 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "initial_temperatures = [1]\n",
    "final_temperatures = [0.1, 1]\n",
    "lrs = [0.001] \n",
    "\n",
    "for i in range(1, 11):\n",
    "    for lr in lrs:\n",
    "        for initial_temperature in initial_temperatures:\n",
    "            for final_temperature in final_temperatures:\n",
    "                print(lr, initial_temperature, final_temperature)\n",
    "                run_experiment(lr, initial_temperature, final_temperature, i)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
