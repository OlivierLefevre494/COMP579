{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Value-based methods with deep neural network [50 points]\n",
    "Implement Q-learning and Exptected SARSA for both Acrobot-v1 1 and ALE/Assault-ram-v52\n",
    "environments from the Gym suite using the following guidelines:\n",
    "• Use a Neural Network approximation for Q, that is, if x is a vector representing the state\n",
    "and a is the action vector, use Q value(x) = M LP (x; θ), where θ are the parameters of the\n",
    "Value function you need to learn, Q ∈ Rm where m denotes the number of discrete actions.\n",
    "• Model configuration: Initialize the parameters for the value function uniformly between\n",
    "−0.001 and 0.001, we recommend using either a 2 or 3-layer Neural Network for the Value\n",
    "function, with a hidden dimension of 256.\n",
    "• Use an ϵ- greedy policy with three choices of ϵ and step-size parameters 1/4, 1/8, 1/16. and\n",
    "run 50 learning trials with different initializations for the Value function, each having 1000\n",
    "episodes, for each configuration. That means 3(ϵ’s) * 3 (step-sizes) * 50 runs * 1000 episodes.\n",
    "• Repeat the Previous step using a replay buffer (with transitions randomly sampled) and do\n",
    "gradient updates using a mini-batch of transitions. The capacity of the replay buffer is 1M.\n",
    "• Plot training curves with the mean across seeds as lines and the standard deviation as a shaded\n",
    "region. (Performance on the Y-axis, and the episode on the X-axis). Generate 18 graphs\n",
    "covering all configurations per environment. Present separate plots for each environment,\n",
    "with distinct graphs for settings with and without a replay buffer. Use green for Q-Learning\n",
    "and red for Expected SARSA, differentiating hyperparameters with different line styles (e.g.,\n",
    "solid, dashed).\n",
    "• Implement all the methods using any automatic differentiation package, such as Py-\n",
    "torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Q-learning and Exptected SARSA for both Acrobot-v1 1 and ALE/Assault-ram-v52 environments from the Gym suite using the following guidelines:\n",
    "\n",
    "# • Use a Neural Network approximation for Q, that is, if x is a vector representing the state and a is the action vector, use Q value(x) = M LP (x; θ), where θ are the parameters of the Value function you need to learn, Q ∈ Rm where m denotes the number of discrete actions.\n",
    "\n",
    "# • Model configuration: Initialize the parameters for the value function uniformly between −0.001 and 0.001, we recommend using either a 2 or 3-layer Neural Network for the Value function, with a hidden dimension of 256.\n",
    "\n",
    "# • Use an ϵ- greedy policy with three choices of ϵ and step-size parameters 1/4, 1/8, 1/16. and run 50 learning trials with different initializations for the Value function, each having 1000 episodes, for each configuration. That means 3(ϵ’s) * 3 (step-sizes) * 50 runs * 1000 episodes.\n",
    "\n",
    "# • Repeat the Previous step using a replay buffer (with transitions randomly sampled) and do gradient updates using a mini-batch of transitions. The capacity of the replay buffer is 1M.\n",
    "\n",
    "# • Plot training curves with the mean across seeds as lines and the standard deviation as a shaded region. (Performance on the Y-axis, and the episode on the X-axis). Generate 18 graphs covering all configurations per environment. Present separate plots for each environment, with distinct graphs for settings with and without a replay buffer. Use green for Q-Learning and red for Expected SARSA, differentiating hyperparameters with different line styles (e.g., solid, dashed).\n",
    "\n",
    "# • Implement all the methods using any automatic differentiation package, such as Py-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Acrobot-v1'\n",
    "# env_name = 'Assault-ram-v0'\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import math as math\n",
    "\n",
    "# Hyperparameters\n",
    "EPISODES = 1000\n",
    "MAX_STEPS = 1000\n",
    "GAMMA = 0.99\n",
    "HIDDEN_DIM = 256\n",
    "SEEDS = 50\n",
    "\n",
    "# Environment\n",
    "env = gym.make(env_name)\n",
    "env._max_episode_steps = MAX_STEPS\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Neural Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, INPUT_DIM, OUTPUT_DIM):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc3 = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "        self.fc1.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.fc1.bias.data.uniform_(-0.01, 0.01)\n",
    "        self.fc2.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.fc2.bias.data.uniform_(-0.01, 0.01)\n",
    "        self.fc3.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.fc3.bias.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Q-Learning\n",
    "def q_learning(lr, batch_size, replay_buffer_size, epsilon):\n",
    "    q_network = QNetwork(state_dim, action_dim)\n",
    "    # q_network_target = QNetwork()\n",
    "    # q_network_target.load_state_dict(q_network.state_dict())\n",
    "    q_network.to(device)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "    rewards = []\n",
    "    for episode in range(EPISODES):\n",
    "        print(episode)\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    action = q_network(state_tensor).argmax().item()\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "                state_batch = torch.tensor(np.array(state_batch), dtype=torch.float32).to(device)\n",
    "                action_batch = torch.tensor(np.array(action_batch), dtype=torch.long).to(device)\n",
    "                reward_batch = torch.tensor(np.array(reward_batch), dtype=torch.float32).to(device)\n",
    "                next_state_batch = torch.tensor(np.array(next_state_batch), dtype=torch.float32).to(device)\n",
    "                done_batch = torch.tensor(np.array(done_batch), dtype=torch.float32).to(device)\n",
    "                q_values = q_network(state_batch)\n",
    "                # next_q_values = q_network_target(next_state_batch)\n",
    "                next_q_values = q_network(next_state_batch)\n",
    "                target_q_values = q_values.clone()\n",
    "                # for i in range(batch_size):\n",
    "                #     target_q_values[i][action_batch[i]] = reward_batch[i] + GAMMA * next_q_values[i].max() * (1 - done_batch[i])\n",
    "                target_q_values[range(batch_size), action_batch] = reward_batch + GAMMA * next_q_values.max(dim=1).values * (1 - done_batch)\n",
    "                loss = nn.MSELoss()(q_values, target_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            state = next_state\n",
    "        rewards.append(total_reward)\n",
    "        # if episode % 100 == 0:\n",
    "        #     q_network_target.load_state_dict(q_network.state_dict())\n",
    "    return rewards\n",
    "\n",
    "# Expected SARSA\n",
    "def expected_sarsa(lr, batch_size, replay_buffer_size, epsilon):\n",
    "    q_network = QNetwork()\n",
    "    # q_network_target = QNetwork()\n",
    "    # q_network_target.load_state_dict(q_network.state_dict())\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "    rewards = []\n",
    "    for episode in range(EPISODES):\n",
    "        print(episode)\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                    action = q_network(state_tensor).argmax().item()\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "                state_batch = torch.tensor(np.array(state_batch), dtype=torch.float32).to(device)\n",
    "                action_batch = torch.tensor(np.array(action_batch), dtype=torch.long).to(device)\n",
    "                reward_batch = torch.tensor(np.array(reward_batch), dtype=torch.float32).to(device)\n",
    "                next_state_batch = torch.tensor(np.array(next_state_batch), dtype=torch.float32).to(device)\n",
    "                done_batch = torch.tensor(np.array(done_batch), dtype=torch.float32).to(device)\n",
    "                q_values = q_network(state_batch)\n",
    "                # next_q_values = q_network_target(next_state_batch)\n",
    "                next_q_values = q_network(next_state_batch)\n",
    "                target_q_values = q_values.clone()\n",
    "                # for i in range(batch_size):\n",
    "                #     target_q_values[i][action_batch[i]] = reward_batch[i] + GAMMA * next_q_values[i].mean() * (1 - done_batch[i])\n",
    "                target_q_values[range(batch_size), action_batch] = reward_batch + GAMMA * next_q_values.mean(dim=1) * (1 - done_batch)\n",
    "                loss = nn.MSELoss()(q_values, target_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            state = next_state\n",
    "        rewards.append(total_reward)\n",
    "        # if episode % 100 == 0:\n",
    "        #     q_network_target.load_state_dict(q_network.state_dict())\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def run_experiment(lr, batch_size, replay_buffer_size, epsilon):\n",
    "    random_seeds = np.random.randint(0, 1000, size=SEEDS)\n",
    "    q_learning_rewards = np.zeros((SEEDS, EPISODES))\n",
    "    expected_sarsa_rewards = np.zeros((SEEDS, EPISODES))\n",
    "    for i, seed in enumerate(random_seeds):\n",
    "        print(f'Run {i+1}/{SEEDS}')\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        q_learning_rewards[i] = q_learning(lr, batch_size, replay_buffer_size, epsilon)\n",
    "        expected_sarsa_rewards[i] = expected_sarsa(lr, batch_size, replay_buffer_size, epsilon)\n",
    "    q_learning_rewards_mean = q_learning_rewards.mean(axis=0)\n",
    "    q_learning_rewards_std = q_learning_rewards.std(axis=0)\n",
    "    expected_sarsa_rewards_mean = expected_sarsa_rewards.mean(axis=0)\n",
    "    expected_sarsa_rewards_std = expected_sarsa_rewards.std(axis=0)\n",
    "    plt.plot(q_learning_rewards_mean, label='Q-Learning', color='green')\n",
    "    plt.fill_between(range(EPISODES), q_learning_rewards_mean - q_learning_rewards_std, q_learning_rewards_mean + q_learning_rewards_std, color='green', alpha=0.2)\n",
    "    plt.plot(expected_sarsa_rewards_mean, label='Expected SARSA', color='red')\n",
    "    plt.fill_between(range(EPISODES), expected_sarsa_rewards_mean - expected_sarsa_rewards_std, expected_sarsa_rewards_mean + expected_sarsa_rewards_std, color='red', alpha=0.2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.savefig('plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEEDS = 1\n",
    "# EPISODES = 100\n",
    "# run_experiment(0.001, 64, 1000000, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/50\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m lrs:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m replay_buffer \u001b[38;5;129;01min\u001b[39;00m replay_buffers:\n\u001b[0;32m----> 7\u001b[0m         \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m         os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmv plot.png plots/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env_name, epsilon, lr, replay_buffer[\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[5], line 153\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(lr, batch_size, replay_buffer_size, epsilon)\u001b[0m\n\u001b[1;32m    151\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m    152\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m--> 153\u001b[0m     q_learning_rewards[i] \u001b[38;5;241m=\u001b[39m \u001b[43mq_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     expected_sarsa_rewards[i] \u001b[38;5;241m=\u001b[39m expected_sarsa(lr, batch_size, replay_buffer_size, epsilon)\n\u001b[1;32m    155\u001b[0m q_learning_rewards_mean \u001b[38;5;241m=\u001b[39m q_learning_rewards\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 84\u001b[0m, in \u001b[0;36mq_learning\u001b[0;34m(lr, batch_size, replay_buffer_size, epsilon)\u001b[0m\n\u001b[1;32m     81\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# for i in range(batch_size):\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#     target_q_values[i][action_batch[i]] = reward_batch[i] + GAMMA * next_q_values[i].max() * (1 - done_batch[i])\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m target_q_values[\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m, action_batch] \u001b[38;5;241m=\u001b[39m reward_batch \u001b[38;5;241m+\u001b[39m GAMMA \u001b[38;5;241m*\u001b[39m next_q_values\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m done_batch)\n\u001b[1;32m     85\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(q_values, target_q_values)\n\u001b[1;32m     86\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilons = [0.25, 0.125, 0.0625]\n",
    "lrs = [1/4, 1/8, 1/16]\n",
    "replay_buffers = [(1, 1), (32, 1000000)]\n",
    "for epsilon in epsilons:\n",
    "    for lr in lrs:\n",
    "        for replay_buffer in replay_buffers:\n",
    "            run_experiment(lr, replay_buffer[0], replay_buffer[1], epsilon)\n",
    "            time.sleep(1)\n",
    "            os.system('mv plot.png plots/{}_{}_{}_{}.png'.format(env_name, epsilon, lr, replay_buffer[0]))\n",
    "            time.sleep(1)\n",
    "            os.system('rm plot.png')\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoltzmanPolicy(preds,T):\n",
    "    return np.random.choice(len(preds), p=((np.exp(preds)/T) / np.sum(np.exp(preds)/T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(episodes, lr, gamma, T, decay, decay_rate):\n",
    "    rewards = []\n",
    "    q_network = QNetwork(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    temp = T\n",
    "    for i in range(episodes):\n",
    "        if decay:\n",
    "            temp = T * math.exp(-decay_rate * i)\n",
    "        total_reward = 0\n",
    "        print(\"EPSIODE#\", i)\n",
    "        ep = []\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done and step < 500:\n",
    "            action = BoltzmanPolicy([i.item() for i in q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))[0]], temp)\n",
    "            prob = q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))[action].item()\n",
    "            next_state, reward, done, truncated, _= env.step(action)\n",
    "            total_reward += reward\n",
    "            ep.append((state, action, reward, prob))\n",
    "            state = next_state\n",
    "        grad = 0\n",
    "        for t in range(len(ep)):\n",
    "            G = sum([gamma**(i-t-1) * ep[i][2] for i in range(t, len(ep))])\n",
    "            grad += (gamma**t) * G * torch.log(q_network(ep[t][0])[ep[t][1]])\n",
    "        optimizer.zero_grad()\n",
    "        grad.backward()\n",
    "        step += 1\n",
    "        optimizer.step()\n",
    "        rewards.append(total_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2C(episodes, gamma, policy_lr,value_lr, T, decay, decay_rate):\n",
    "    policy_network = QNetwork(state_dim, action_dim)\n",
    "    value_network = QNetwork(state_dim, 1)\n",
    "    optimizer_actor = optim.Adam(policy_network.parameters(), lr=policy_lr)\n",
    "    optimizer_value = optim.Adam(value_network.parameters(), lr=value_lr)\n",
    "    rewards = []\n",
    "    temp = T\n",
    "    for i in range(episodes):\n",
    "        if decay:\n",
    "            temp = T * math.exp(-decay_rate*i)\n",
    "        total_reward = 0\n",
    "        print(\"EPSIODE#\", i)\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        I = 1\n",
    "        step = 0\n",
    "        while not done  and step < 500:\n",
    "            action = BoltzmanPolicy([i.item() for i in policy_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))[0]], temp)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                td_error = reward - value_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))\n",
    "            else:\n",
    "                td_error = reward + gamma * value_network(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)).detach() - value_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))\n",
    "            policy_loss = max(torch.log(policy_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))[0])[action], -50) * td_error.detach()  * I\n",
    "            value_loss = td_error  ** 2\n",
    "            optimizer_actor.zero_grad()\n",
    "            optimizer_value.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            value_loss.backward()\n",
    "            optimizer_actor.step()\n",
    "            optimizer_value.step()\n",
    "            I = gamma * I\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        rewards.append(total_reward)\n",
    "        print(\"REWARD\", total_reward)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment2(lr, T, decay, decay_rate, gamma, episodes):\n",
    "    random_seeds = np.random.randint(0, 10, size=SEEDS)\n",
    "    a2c_learning_rewards = np.zeros((SEEDS, EPISODES))\n",
    "    reinforce_rewards = np.zeros((SEEDS, EPISODES))\n",
    "    for i, seed in enumerate(random_seeds):\n",
    "        print(f'Run {i+1}/{SEEDS}')\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        a2c_learning_rewards[i] = A2C(episodes=episodes, gamma=gamma, policy_lr=lr, value_lr=lr, T=T, decay=decay, decay_rate=decay_rate)\n",
    "        reinforce_rewards[i] = reinforce(episodes=episodes, lr=lr, gamma=gamma, T=T, decay=decay, decay_rate=decay_rate)\n",
    "    a2c_learning_rewards_mean = a2c_learning_rewards.mean(axis=0)\n",
    "    a2c_learning_rewards_std = a2c_learning_rewards.std(axis=0)\n",
    "    reinforce_rewards_mean = reinforce_rewards.mean(axis=0)\n",
    "    reinforce_rewards_std = reinforce_rewards.std(axis=0)\n",
    "    plt.plot(a2c_learning_rewards_mean, label='Q-Learning', color='green')\n",
    "    plt.fill_between(range(EPISODES), a2c_learning_rewards_mean - a2c_learning_rewards_std, a2c_learning_rewards_mean + a2c_learning_rewards_std, color='green', alpha=0.2)\n",
    "    plt.plot(reinforce_rewards_mean, label='Expected SARSA', color='red')\n",
    "    plt.fill_between(range(EPISODES), reinforce_rewards_mean - reinforce_rewards_std, reinforce_rewards_mean + reinforce_rewards_std, color='red', alpha=0.2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.savefig('plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/50\n",
      "EPSIODE# 0\n",
      "REWARD -500.0\n",
      "EPSIODE# 1\n",
      "REWARD -500.0\n",
      "EPSIODE# 2\n",
      "REWARD -500.0\n",
      "EPSIODE# 3\n",
      "REWARD -500.0\n",
      "EPSIODE# 4\n",
      "REWARD -500.0\n",
      "EPSIODE# 5\n",
      "REWARD -500.0\n",
      "EPSIODE# 6\n",
      "REWARD -500.0\n",
      "EPSIODE# 7\n"
     ]
    }
   ],
   "source": [
    "T = [[0.5, True, 0.01], [0.1, False, 0.01], [0.01, False, 0.01], [0.2, True, 0.01]]\n",
    "lr = [0.001]\n",
    "for stuff in T:\n",
    "    run_experiment2(lr=lr[0], T=stuff[0], decay=stuff[1], decay_rate=stuff[2], gamma=0.99, episodes=1000)\n",
    "    time.sleep(1)\n",
    "    os.system('mv plot.png plots/{}_{}_{}_{}.png'.format(env_name, epsilon, lr, replay_buffer[0]))\n",
    "    time.sleep(1)\n",
    "    os.system('rm plot.png')\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
